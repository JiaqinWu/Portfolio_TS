{"title":"Deep Learning for Time Series","markdown":{"yaml":{"title":"Deep Learning for Time Series","format":{"html":{"code-fold":true}},"jupyter":"python3","engine":"knitr"},"headingText":"1. Introduction","containsRefs":false,"markdown":"\n\n\n```{python, warning=FALSE, message=FALSE,echo=FALSE}\nimport sys\nsys.setrecursionlimit(10000) \n#!pip install scikit-learn\n#!pip install tensorflow==2.13.0\n#!pip install yfinance\n#!pip install plotly\n#!pip install statsmodels\n#!pip install IPython\n#!pip install matplotlib\n#!pip install seaborn\n#!pip install jupyter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd \nimport yfinance as yf\nimport plotly.express as px\nimport statsmodels.api as sm \n#from IPython.display import IFrame\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\n```\n\nWithin this analysis, I will delve into forecasting time series data through the lens of deep learning. Specifically, I will explore and apply the nuances of Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), and Long Short-Term Memory networks (LSTMs). These advanced models will be meticulously evaluated against traditional approaches, namely ARMA, ARIMA, and SARIMA models, to discern their predictive prowess and applicability in time series analysis. This comparative study aims to illuminate the strengths and potential trade-offs between the deep learning methodologies and more conventional statistical models.\n\n# 2. Data Visualization\n\n```{python, warning=FALSE, message=FALSE,echo=FALSE}\n# Load the vaccination data\nvac_df = pd.read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show = ['date', 'location', 'daily_vaccinations_per_million', 'people_vaccinated_per_hundred', 'people_fully_vaccinated_per_hundred']\nvac_df = vac_df[cols_show]\n\n# Group by 'date' and summarize columns\nvac_df['date'] = pd.to_datetime(vac_df['date'])\nmonthly_vac = vac_df.groupby(vac_df['date'].dt.to_period('M')).agg({\n    'daily_vaccinations_per_million': np.mean,\n    'people_vaccinated_per_hundred': np.mean,\n    'people_fully_vaccinated_per_hundred': np.mean\n}).reset_index()\nmonthly_vac['date'] = monthly_vac['date'].dt.to_timestamp()\n\n# Load the confirmed cases data\ncon_df = pd.read_csv(\"Datasets/covid_confirmed_usafacts.csv\")\n\n# Pivot from wide to long format\ncon_df_long = pd.melt(con_df, id_vars=['countyFIPS', 'County Name', 'State', 'StateFIPS'], \n                      var_name='date', value_name='cases')\ncon_df_long['date'] = pd.to_datetime(con_df_long['date'])\n\n# Group by 'date' and summarize\nmonthly_cases = con_df_long.groupby(con_df_long['date'].dt.to_period('M')).agg({'cases': np.sum}).reset_index()\nmonthly_cases['date'] = monthly_cases['date'].dt.to_timestamp()\n\n# Load the death cases data\ndeaths_df = pd.read_csv(\"Datasets/covid_deaths_usafacts.csv\")\n\n# Pivot from wide to long format\ndeaths_df_long = pd.melt(deaths_df, id_vars=['countyFIPS', 'County Name', 'State', 'StateFIPS'], \n                         var_name='date', value_name='deaths')\ndeaths_df_long['date'] = pd.to_datetime(deaths_df_long['date'])\n\n# Group by 'date' and summarize\nmonthly_deaths = deaths_df_long.groupby(deaths_df_long['date'].dt.to_period('M')).agg({'deaths': np.sum}).reset_index()\nmonthly_deaths['date'] = monthly_deaths['date'].dt.to_timestamp()\n```\n\nIn our session, we would not fit all the datasets since the complexity of the datasets, but we want to mainly focus on three datasets, one is the vaccination rate, and others are the confirmed case number and death case number to see the future trend of these three variables.\n\n::: panel-tabset\n### Vaccination Number\n\n```{python, warning=FALSE, message=FALSE}\nimport plotly.express as px\nimport nbformat\n\nfig = px.line(monthly_vac, x='date', y=\"daily_vaccinations_per_million\")\nfig.update_layout(\n        xaxis_title='Time',\n        yaxis_title='Daily COVID-19 Vaccination Number',\n        title='Daily COVID-19 Vaccination Number in the US Over Time'\n    )\n#fig.show()\n```\n\n### Confirmed Case Number\n\n```{python, warning=FALSE, message=FALSE}\nfig = px.line(monthly_cases, x='date', y=\"cases\")\nfig.update_layout(\n        xaxis_title='Time',\n        yaxis_title='Daily COVID-19 Confimed Number',\n        title='Daily COVID-19 Confimed Number in the US Over Time'\n    )\n#fig.show()\n```\n\n### Death Case Number\n\n```{python, warning=FALSE, message=FALSE}\nfig = px.line(monthly_deaths, x='date', y=\"deaths\")\nfig.update_layout(\n        xaxis_title='Time',\n        yaxis_title='Daily COVID-19 Death Number',\n        title='Daily COVID-19 Death Number in the US Over Time'\n    )\n#fig.show()\n```\n\n:::\n\n# 3. Split Data & Normalize\n\nIn the next phase of analysis, I will partition the datasets into training and testing subsets to facilitate model evaluation and validation. Given the wide range of variable scales present in the original data, I will implement **normalization** techniques on the regression values. This step is crucial for optimizing model performance by ensuring that the data conforms to a uniform scale, thereby enhancing the accuracy and effectiveness of the predictive models.\n\n::: panel-tabset\n### Vaccination Number\n\n```{python, warning=FALSE, message=FALSE}\ndef get_train_test(data, split_percent = 0.8):\n    # Convert data to array\n    data = np.array(data)\n    \n    # Normalize data\n    data=(data-np.mean(data,axis=0))/np.std(data,axis=0)\n    \n    # define split point for splitting data into training and testing\n    n = len(data)\n    split = int(n*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    \n    # return the test splits\n    return train_data, test_data\n\nvac_train_data, vac_test_data = get_train_test(monthly_vac['daily_vaccinations_per_million'])\n\nprint(f'Original shape: {len(monthly_vac[\"daily_vaccinations_per_million\"])}')\nprint(f'Train shape: {vac_train_data.shape}')\nprint(f'Test shape: {vac_test_data.shape}')\n\nt1 = [*range(0, len(vac_train_data))]\nt2 = len(vac_train_data) + np.array([*range(0, len(vac_test_data))])\n\ndef plotly_plot(t, y, title = \"Plot\", x_label = \"Time (Month)\", y_label = \"Value\"):\n\n    \n    fig = px.line(x = t[0], y = y[0], title = title, render_mode = 'SVG')  \n    \n    # Plot the scatter points\n    for i in range(1,len(y)):\n        fig.add_scatter(x = t[i], y = y[i], mode='lines')\n    \n    # update the layout with labels and customization\n    fig.update_layout(\n        xaxis_title = x_label,\n        yaxis_title = y_label,\n        showlegend = False\n    )\n    # show the figure\n    fig.show()\n\nplotly_plot([t1, t2], [vac_train_data, vac_test_data], title = \"Vaccination Train & Test Data\")\n```\n\n### Confirmed Case Number\n\n```{python, warning=FALSE, message=FALSE}\ncase_train_data, case_test_data = get_train_test(monthly_cases['cases'])\n\nprint(f'Original shape: {len(monthly_cases[\"cases\"])}')\nprint(f'Train shape: {case_train_data.shape}')\nprint(f'Test shape: {case_test_data.shape}')\n\nt1 = [*range(0, len(case_train_data))]\nt2 = len(case_train_data) + np.array([*range(0, len(case_test_data))])\n\nplotly_plot([t1, t2], [case_train_data, case_test_data], title = \"Confirmed Case Train & Test Data\")\n```\n\n### Death Case Number\n\n```{python, warning=FALSE, message=FALSE}\ndeath_train_data, death_test_data = get_train_test(monthly_deaths['deaths'])\n\nprint(f'Original shape: {len(monthly_deaths[\"deaths\"])}')\nprint(f'Train shape: {death_train_data.shape}')\nprint(f'Test shape: {death_test_data.shape}')\n\nt1 = [*range(0, len(death_train_data))]\nt2 = len(death_train_data) + np.array([*range(0, len(death_test_data))])\n\nplotly_plot([t1, t2], [death_train_data, death_test_data], title = \"Death Case Train & Test Data\")\n```\n\n:::\n\n\n# 4. Mini-Batching\n\nTo enhance the efficacy of the training process, I will incorporate the use of mini-batching. This approach involves updating the gradients more frequently within each epoch, which is expected to significantly improve the overall performance of the model. By doing so, the model can learn more effectively and adaptively from smaller subsets of data, leading to more accurate and robust predictions.\n\n::: panel-tabset\n### Vaccination Number\n\n```{python, warning=FALSE, message=FALSE}\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n  # Initialize\n  i_start=0\n  count=0\n  x_out=[]\n  y_out=[]\n  # Sequentially build mini-batches\n  while i_start + lookback + delay < x.shape[0]:\n    i_stop = i_start + lookback\n    i_pred = i_stop + delay\n    # report if desired\n    if verbose and count < 2:\n      print(\"indice range:\",i_start, i_stop, \"-->\",i_pre)\n    # define arrays\n    indices_to_keep = []\n    j = i_stop\n    while j >= i_start:\n      indices_to_keep.append(j)\n      j -= step\n    # create mini-batch sample\n    xtmp = x[indices_to_keep,:]\n    xtmp = xtmp[:,feature_columns]\n    ytmp=x[i_pred,target_columns]\n    x_out.append(xtmp)\n    y_out.append(ytmp)\n    # report if desired\n    if verbose and count <2:\n      print(xtmp, \"-->\", ytmp)\n      print(\"shape:\", xtmp.shape, \"-->\",ytmp.shape)\n    # plot\n    if verbose and count <2:\n      fig, ax = plt.subplots()\n      ax.plot(x, 'b-')\n      ax.plot(x,'bx')\n      ax.plot(indices_to_keep, xtmp, 'go')\n      ax.plot(i_pred*np.ones(len(target_columns)),ytmp, 'ro')\n      plt.show()\n    # update start point\n    if unique:\n      i_start += lookback\n    else:\n      i_start += 1\n    count += 1\n  return np.array(x_out), np.array(y_out)\n\ntrain = vac_train_data.reshape(vac_train_data.shape[0],1)\ntest = vac_test_data.reshape(vac_test_data.shape[0],1)\n\nvac_trainX, vac_trainY = form_arrays(train,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\nvac_testX, vac_testY = form_arrays(test,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\n\nprint(f'Train shape: {vac_trainX.shape} , {vac_trainY.shape}')\nprint(f'Test shape: {vac_testX.shape} , {vac_testY.shape}')\n```\n\n### Confirmed Case Number\n\n```{python, warning=FALSE, message=FALSE}\ntrain = case_train_data.reshape(case_train_data.shape[0],1)\ntest = case_test_data.reshape(case_test_data.shape[0],1)\n\ncase_trainX, case_trainY = form_arrays(train,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\ncase_testX, case_testY = form_arrays(test,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\n\nprint(f'Train shape: {case_trainX.shape} , {case_trainY.shape}')\nprint(f'Test shape: {case_testX.shape} , {case_testY.shape}')\n```\n\n### Death Case Number\n\n```{python, warning=FALSE, message=FALSE}\ntrain = death_train_data.reshape(death_train_data.shape[0],1)\ntest = death_test_data.reshape(death_test_data.shape[0],1)\n\ndeath_trainX, death_trainY = form_arrays(train,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\ndeath_testX, death_testY = form_arrays(test,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\n\nprint(f'Train shape: {death_trainX.shape} , {death_trainY.shape}')\nprint(f'Test shape: {death_testX.shape} , {death_testY.shape}')\n```\n\n:::\n\n\n# 5. RNN\n\nIn this section, I will focus on training a Recurrent Neural Network (RNN), which is specifically engineered for handling sequential data by utilizing cyclic connections that retain memory of prior inputs. To enhance the robustness and generalizability of the RNN, I plan to train it both with and without regularization techniques. To streamline this process, I will encapsulate the modeling code within a single function that accepts parameters for data, model type, and a boolean indicating whether regularization should be applied. This modular approach will facilitate easy adjustments and retesting under various configurations, improving efficiency and experimental flexibility.\n\nHere is how we define each function to generate the model:\n```{python, warning=FALSE, message=FALSE}\nimport tensorflow as tf\n# Utility functions \ndef regression_report(yt,ytp,yv,yvp):\n  print(\"--------- Regression Report ---------\")\n  print(\"TRAINING:\")\n  train_mse = np.mean((yt - ytp) ** 2)\n  train_mae = np.mean(np.abs(yt - ytp))\n  print(\"MSE\", train_mse)\n  print(\"MAE\", train_mae)\n  \n  # PARITY PLOT\n  fig, ax = plt.subplots()\n  ax.plot(yt, ytp, 'ro')\n  ax.plot(yt, yt, 'b-')\n  ax.set(xlabel='y_data', ylabel='y_predicted',\n        title = 'Training data parity plot (line y=x represents a perfect fit)')\n  plt.show()\n  \n  # PLOT PART OF THE PREDICTED TIME-SERIES\n  frac_plot=1.0\n  upper=int(frac_plot*yt.shape[0]);\n  fig, ax = plt.subplots()\n  ax.plot(yt[0:upper], 'b-')\n  ax.plot(ytp[0:upper], 'r-', alpha=0.5)\n  ax.plot(ytp[0:upper], 'ro', alpha=0.25)\n  ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Training: Time-series prediction')\n  plt.show()\n  \n  print(\"VALIDATION:\")\n  val_mse = np.mean((yv - yvp) ** 2)\n  val_mae = np.mean(np.abs(yv - yvp))\n  print(\"MSE\", val_mse)\n  print(\"MAE\", val_mae)\n  \n  # PARITY PLOT\n  fig,ax = plt.subplots()\n  ax.plot(yv,yvp, 'ro')\n  ax.plot(yv, yv,'b-')\n  ax.set(xlabel='y_data', ylabel='y_predicted',title='Validation data parity plot (line y=x represents a perfect fit)')\n  \n  # PLOT PART OF THE PREDICTED TIME-SERIES\n  upper=int(frac_plot*yv.shape[0])\n  fig,ax = plt.subplots()\n  ax.plot(yv[0:upper], 'b-')\n  ax.plot(yvp[0:upper], 'r-', alpha=0.5)\n  ax.plot(yvp[0:upper], 'ro', alpha=0.25)\n  ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Validation: Time-series prediction')\n  plt.show()\n  return train_mse, train_mae, val_mse, val_mae\n\ndef history_plot(history):\n  FS=18 #fontsize\n  history_dict = history.history\n  loss_values = history_dict['loss']\n  val_loss_values = history_dict['val_loss']\n  epochs = range(1, len(loss_values) + 1)\n  plt.plot(epochs, loss_values, 'bo', label='Training Loss')\n  plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n  plt.title('Training and Validation Loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n  plt.ylim(bottom=0) \n  plt.legend()\n  plt.show()\n\n# Define model function\ndef train_model(model_type, train_x, train_y, val_x, val_y, regularization = True, L2=1e-4):\n  if regularization:\n    reg = regularizers.L2(L2)\n  else:\n    reg = None\n  \n  # Define parameters\n  optimizer=\"rmsprop\"\n  loss_function = 'mean_squared_error'\n  learning_rate=0.01\n  numbers_epochs=200\n  input_shape=(train_x.shape[1],train_x.shape[2])\n  train_x1 = train_x.reshape(train_x.shape[0],train_x.shape[1]*train_x.shape[2])\n  batch_size=len(train_x1)              # batch training\n  \n  # BUILD MODEL\n  recurrent_hidden_units=32\n  \n  # CREATE MODEL\n  model = keras.Sequential()\n  \n  # ADD RECURRENT LAYER\n  if model_type == 'RNN':\n    model.add(SimpleRNN(\n    units=recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=input_shape, \n    # recurrent_dropout=0.8,\n    recurrent_regularizer=reg,\n    activation='relu')\n              )\n  elif model_type == 'GRU':\n    model.add(GRU(\n    units=recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=input_shape, \n    # recurrent_dropout=0.8,\n    recurrent_regularizer=reg,\n    activation='relu')\n              ) \n  elif model_type == 'LSTM':\n    model.add(LSTM(\n    units=recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=input_shape, \n    # recurrent_dropout=0.8,\n    recurrent_regularizer=reg,\n    activation='relu')\n              )\n  else:\n    print('Wrong model type')\n  \n  # NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \n  model.add(Dense(units=1, activation='linear'))\n  \n  # MODEL SUMMARY\n  print(model.summary()); #print(x_train.shape,y_train.shape)  \n  \n  # COMPILING THE MODEL \n  opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n  model.compile(optimizer=opt, loss=loss_function)\n  \n  # TRAINING YOUR MODEL\n  history_techmu = model.fit(train_x,\n                      train_y,\n                      epochs=numbers_epochs,\n                      batch_size=batch_size, verbose=False,\n                      validation_data=(val_x, val_y))\n  # History plot\n  history_plot(history_techmu)\n  \n  # Predictions \n  train_pred=model.predict(train_x)\n  val_pred=model.predict(val_x) \n  train_mse, train_mae, val_mse, val_mae = regression_report(train_y,train_pred,val_y,val_pred)\n  return train_mse, train_mae, val_mse, val_mae\n```\n\n\n::: panel-tabset\n### Vaccination Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults = []\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Confirmed Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Death Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n:::\n\n\n# 6. LSTM\n\nIn this section, I will be training a Long Short-Term Memory (LSTM) network, a specialized form of Recurrent Neural Network (RNN). LSTMs are designed to overcome the vanishing gradient problem inherent in traditional RNNs through a sophisticated gating mechanism. This mechanism effectively regulates the information flow into and out of the network's memory cells, facilitating the learning of long-term dependencies. I plan to evaluate the LSTM's performance both with and without the application of regularization techniques, to ascertain their impact on the model's ability to generalize.\n\n::: panel-tabset\n### Vaccination Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Confirmed Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Death Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n:::\n\n\n# 7. GRU\n\n::: panel-tabset\n### Vaccination Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Confirmed Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Death Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n:::\n\n# 8. Discussion\n\n\nIn constructing our models, we design a framework tailored for RNN, LSTM, and GRU networks to tackle specific tasks. Each model begins with defining regularization practices to ensure generalization, using L2 regularization as needed. We configure the model using the RMSprop optimizer, a mean squared error loss function, and set a learning rate. Depending on the model type—RNN, GRU, or LSTM—we adjust the architecture by setting the number of recurrent hidden units and whether the model returns sequences. After assembling the model, we compile it, train it on our dataset over numerous epochs, and validate it using a separate dataset. This systematic approach allows us to fine-tune and compare the performance of each neural network architecture effectively.\n\n\n### 8.1 How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?\n```{python, warning=FALSE, message=FALSE}\n# Combine results into table\nresult_df = pd.DataFrame(results)\nresult_df['train_rmse'] = result_df['train_mse'].apply(lambda x: x ** .5)\nresult_df['val_rmse'] = result_df['val_mse'].apply(lambda x: x ** .5)\nresult_df[['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n```\n**Best Vaccination Number DL Model:**\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Vaccination'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True).head(1)\n```\n\nWe can see the best DL model for vaccination number is RNN model without regulation since it has the lowest validation RMSE value.\n\n**Best Confirmed Case DL Model:**\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Confirmed Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True).head(1)\n```\n\nWe can see the best DL model for confirmed case is RNN model without regulation since it has the lowest validation RMSE value.\n\n**Best Death Case DL Model:**\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Death Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True).head(1)\n```\n\nWe can see the best DL model for death case is LSTM model with regulation since it has the lowest validation RMSE value.\n\nIn our evaluation, the RNN and LSTM models demonstrated superior performance, excelling in their predictive capabilities. Conversely, the GRU model showed relatively lower effectiveness. This divergence in performance highlights the distinct advantages and challenges associated with each type of neural network architecture, emphasizing the importance of choosing the right model based on the specific requirements and nuances of the dataset at hand.\n\n### 8.2 What effect does including regularization have on your results?\n\nRegularization is crucial in machine learning, adding a penalty to the loss function to reduce the complexity of the model, thus minimizing overfitting to the training data. This technique varies in effectiveness across different datasets. For example, an un-regularized LSTM model achieved the lowest RMSE for emissions data, while a regularized GRU model was most effective for the temperature dataset. Across various datasets, a consistent observation was a higher RMSE in the validation set compared to the training set, indicating persistent overfitting despite the regularization's attempt to bridge the gap between training and validation performance.\n\n### 8.3 How far into the future can the deep learning model accurately predict the future?\n\nDeep learning models' ability to predict future events hinges on the data they have encountered during training. These models, fundamentally supervised, learn and generate predictions based on the sequences they've been exposed to. Thus, their predictive power extends only as far as recognizing patterns similar to those within their training datasets. The farther into the future a prediction extends, the more reliant it becomes on the quality and representativeness of historical data, limiting accuracy when facing novel scenarios or trends not previously seen during training.\n\n\n### 8.4. How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models from HW-3?\n\n::: panel-tabset\n#### Vaccination Number\n\nFrom Arima model, below are the computed metrics:\n\n| Metric       | Value               |\n|--------------|---------------------|\n| ME           | -1.260901e+04       |\n| RMSE         | 4.404350e+04        |\n| MAE          | 2.961164e+04        |\n| MPE          | -1.819884e+00       |\n| MAPE         | 2.997255e+01        |\n| MASE         | 5.644007e-01        |\n| ACF1         | -1.726600e-01       |\n\nFrom Deep Learning model, below are the computed metrics:\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Vaccination'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n```\n\n#### Confirmed Case Number\n\nFrom Arima model, below are the computed metrics:\n\n| Metric       | Value               |\n|--------------|---------------------|\n| ME           | 5.887429e+04       |\n| RMSE         | 2.036015e+06        |\n| MAE          | 1.092948e+06        |\n| MPE          | 1.626543e+04       |\n| MAPE         | 1.876329e+04        |\n| MASE         | 2.083170e+01        |\n| ACF1         | 1.041066e-01       |\n\nFrom Deep Learning model, below are the computed metrics:\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Confirmed Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n```\n\n#### Death Case Number\n\nFrom Arima model, below are the computed metrics:\n\n| Metric       | Value               |\n|--------------|---------------------|\n| ME           | 1373.71612640       |\n| RMSE         | 8276.58694400        |\n| MAE          | 6716.48728344        |\n| MPE          | 64.68052062       |\n| MAPE         | 74.15331696         |\n| MASE         | 0.12801688        |\n| ACF1         | -0.09912142        |\n\nFrom Deep Learning model, below are the computed metrics:\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Death Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n```\n\n:::\n\n\nThe performance analysis shows that DL models often outperform traditional models, notably in terms of RMSE, showcasing their robustness in handling diverse datasets. However, the efficacy of these models is somewhat constrained by the size of the available data. Small datasets tend to skew performance, highlighting a potential limitation in scenarios where expansive data is not available.\n\nFor ARIMA models, their predictive capability is generally limited when dealing with noisy, fluctuating data, often producing overly simplistic forecasts. In contrast, ARIMAX and VAR models manage to capture underlying trends more effectively, albeit with some inconsistency in the presence of volatility. Deep Learning models, on the other hand, excel with larger datasets, achieving near-perfect predictions. The performance disparity underscores the importance of dataset size in leveraging the full potential of advanced DL models.\n\n\n\n# 9. Write a discussion paragraph Comparing your models (use RMSE) and forecasts from these sections with your Deep Learning Models.\n\nThis analysis has demonstrated the potent capabilities of deep learning in forecasting time series data. The efficacy of deep learning models relative to traditional approaches is challenging to evaluate directly due to discrepancies in the training and validation datasets. However, the Root Mean Square Error (RMSE) metric reveals considerably higher values for traditional time series models, implying that these models may be less adept at capturing the variability within the dataset. Conversely, deep learning models have consistently produced RMSE values under 1, suggesting superior performance in tracking the actual data trends.\n\nA closer inspection of the training and validation performance of deep learning models against the actual forecasts from traditional models, like ARIMA, underscores this contrast. The ARIMA models, despite their sophistication, often faltered in accounting for the dataset's inherent variance. In contrast, deep learning models demonstrated an ability to assimilate and reflect this variance in their predictions. Notably, the hierarchy of deep learning model performance typically favored the GRU, followed by the LSTM, and then the RNN, aligning with the evolutionary complexity of these architectures.\n\nTraditional time series models necessitate transforming data into a stationary form, with consistent mean and variance, to make accurate extrapolations. This often involves processes such as differencing, which may not always be sufficient for effective forecasting. On the other hand, deep learning models function as universal approximators, capable of modeling any form of data variance. This attribute renders them particularly suitable for complex time series datasets characterized by pronounced fluctuations and seasonal variations, enabling them to deliver more accurate and robust forecasts.\n\n\n","srcMarkdownNoYaml":"\n\n\n```{python, warning=FALSE, message=FALSE,echo=FALSE}\nimport sys\nsys.setrecursionlimit(10000) \n#!pip install scikit-learn\n#!pip install tensorflow==2.13.0\n#!pip install yfinance\n#!pip install plotly\n#!pip install statsmodels\n#!pip install IPython\n#!pip install matplotlib\n#!pip install seaborn\n#!pip install jupyter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd \nimport yfinance as yf\nimport plotly.express as px\nimport statsmodels.api as sm \n#from IPython.display import IFrame\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\n```\n\n# 1. Introduction\nWithin this analysis, I will delve into forecasting time series data through the lens of deep learning. Specifically, I will explore and apply the nuances of Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), and Long Short-Term Memory networks (LSTMs). These advanced models will be meticulously evaluated against traditional approaches, namely ARMA, ARIMA, and SARIMA models, to discern their predictive prowess and applicability in time series analysis. This comparative study aims to illuminate the strengths and potential trade-offs between the deep learning methodologies and more conventional statistical models.\n\n# 2. Data Visualization\n\n```{python, warning=FALSE, message=FALSE,echo=FALSE}\n# Load the vaccination data\nvac_df = pd.read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show = ['date', 'location', 'daily_vaccinations_per_million', 'people_vaccinated_per_hundred', 'people_fully_vaccinated_per_hundred']\nvac_df = vac_df[cols_show]\n\n# Group by 'date' and summarize columns\nvac_df['date'] = pd.to_datetime(vac_df['date'])\nmonthly_vac = vac_df.groupby(vac_df['date'].dt.to_period('M')).agg({\n    'daily_vaccinations_per_million': np.mean,\n    'people_vaccinated_per_hundred': np.mean,\n    'people_fully_vaccinated_per_hundred': np.mean\n}).reset_index()\nmonthly_vac['date'] = monthly_vac['date'].dt.to_timestamp()\n\n# Load the confirmed cases data\ncon_df = pd.read_csv(\"Datasets/covid_confirmed_usafacts.csv\")\n\n# Pivot from wide to long format\ncon_df_long = pd.melt(con_df, id_vars=['countyFIPS', 'County Name', 'State', 'StateFIPS'], \n                      var_name='date', value_name='cases')\ncon_df_long['date'] = pd.to_datetime(con_df_long['date'])\n\n# Group by 'date' and summarize\nmonthly_cases = con_df_long.groupby(con_df_long['date'].dt.to_period('M')).agg({'cases': np.sum}).reset_index()\nmonthly_cases['date'] = monthly_cases['date'].dt.to_timestamp()\n\n# Load the death cases data\ndeaths_df = pd.read_csv(\"Datasets/covid_deaths_usafacts.csv\")\n\n# Pivot from wide to long format\ndeaths_df_long = pd.melt(deaths_df, id_vars=['countyFIPS', 'County Name', 'State', 'StateFIPS'], \n                         var_name='date', value_name='deaths')\ndeaths_df_long['date'] = pd.to_datetime(deaths_df_long['date'])\n\n# Group by 'date' and summarize\nmonthly_deaths = deaths_df_long.groupby(deaths_df_long['date'].dt.to_period('M')).agg({'deaths': np.sum}).reset_index()\nmonthly_deaths['date'] = monthly_deaths['date'].dt.to_timestamp()\n```\n\nIn our session, we would not fit all the datasets since the complexity of the datasets, but we want to mainly focus on three datasets, one is the vaccination rate, and others are the confirmed case number and death case number to see the future trend of these three variables.\n\n::: panel-tabset\n### Vaccination Number\n\n```{python, warning=FALSE, message=FALSE}\nimport plotly.express as px\nimport nbformat\n\nfig = px.line(monthly_vac, x='date', y=\"daily_vaccinations_per_million\")\nfig.update_layout(\n        xaxis_title='Time',\n        yaxis_title='Daily COVID-19 Vaccination Number',\n        title='Daily COVID-19 Vaccination Number in the US Over Time'\n    )\n#fig.show()\n```\n\n### Confirmed Case Number\n\n```{python, warning=FALSE, message=FALSE}\nfig = px.line(monthly_cases, x='date', y=\"cases\")\nfig.update_layout(\n        xaxis_title='Time',\n        yaxis_title='Daily COVID-19 Confimed Number',\n        title='Daily COVID-19 Confimed Number in the US Over Time'\n    )\n#fig.show()\n```\n\n### Death Case Number\n\n```{python, warning=FALSE, message=FALSE}\nfig = px.line(monthly_deaths, x='date', y=\"deaths\")\nfig.update_layout(\n        xaxis_title='Time',\n        yaxis_title='Daily COVID-19 Death Number',\n        title='Daily COVID-19 Death Number in the US Over Time'\n    )\n#fig.show()\n```\n\n:::\n\n# 3. Split Data & Normalize\n\nIn the next phase of analysis, I will partition the datasets into training and testing subsets to facilitate model evaluation and validation. Given the wide range of variable scales present in the original data, I will implement **normalization** techniques on the regression values. This step is crucial for optimizing model performance by ensuring that the data conforms to a uniform scale, thereby enhancing the accuracy and effectiveness of the predictive models.\n\n::: panel-tabset\n### Vaccination Number\n\n```{python, warning=FALSE, message=FALSE}\ndef get_train_test(data, split_percent = 0.8):\n    # Convert data to array\n    data = np.array(data)\n    \n    # Normalize data\n    data=(data-np.mean(data,axis=0))/np.std(data,axis=0)\n    \n    # define split point for splitting data into training and testing\n    n = len(data)\n    split = int(n*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    \n    # return the test splits\n    return train_data, test_data\n\nvac_train_data, vac_test_data = get_train_test(monthly_vac['daily_vaccinations_per_million'])\n\nprint(f'Original shape: {len(monthly_vac[\"daily_vaccinations_per_million\"])}')\nprint(f'Train shape: {vac_train_data.shape}')\nprint(f'Test shape: {vac_test_data.shape}')\n\nt1 = [*range(0, len(vac_train_data))]\nt2 = len(vac_train_data) + np.array([*range(0, len(vac_test_data))])\n\ndef plotly_plot(t, y, title = \"Plot\", x_label = \"Time (Month)\", y_label = \"Value\"):\n\n    \n    fig = px.line(x = t[0], y = y[0], title = title, render_mode = 'SVG')  \n    \n    # Plot the scatter points\n    for i in range(1,len(y)):\n        fig.add_scatter(x = t[i], y = y[i], mode='lines')\n    \n    # update the layout with labels and customization\n    fig.update_layout(\n        xaxis_title = x_label,\n        yaxis_title = y_label,\n        showlegend = False\n    )\n    # show the figure\n    fig.show()\n\nplotly_plot([t1, t2], [vac_train_data, vac_test_data], title = \"Vaccination Train & Test Data\")\n```\n\n### Confirmed Case Number\n\n```{python, warning=FALSE, message=FALSE}\ncase_train_data, case_test_data = get_train_test(monthly_cases['cases'])\n\nprint(f'Original shape: {len(monthly_cases[\"cases\"])}')\nprint(f'Train shape: {case_train_data.shape}')\nprint(f'Test shape: {case_test_data.shape}')\n\nt1 = [*range(0, len(case_train_data))]\nt2 = len(case_train_data) + np.array([*range(0, len(case_test_data))])\n\nplotly_plot([t1, t2], [case_train_data, case_test_data], title = \"Confirmed Case Train & Test Data\")\n```\n\n### Death Case Number\n\n```{python, warning=FALSE, message=FALSE}\ndeath_train_data, death_test_data = get_train_test(monthly_deaths['deaths'])\n\nprint(f'Original shape: {len(monthly_deaths[\"deaths\"])}')\nprint(f'Train shape: {death_train_data.shape}')\nprint(f'Test shape: {death_test_data.shape}')\n\nt1 = [*range(0, len(death_train_data))]\nt2 = len(death_train_data) + np.array([*range(0, len(death_test_data))])\n\nplotly_plot([t1, t2], [death_train_data, death_test_data], title = \"Death Case Train & Test Data\")\n```\n\n:::\n\n\n# 4. Mini-Batching\n\nTo enhance the efficacy of the training process, I will incorporate the use of mini-batching. This approach involves updating the gradients more frequently within each epoch, which is expected to significantly improve the overall performance of the model. By doing so, the model can learn more effectively and adaptively from smaller subsets of data, leading to more accurate and robust predictions.\n\n::: panel-tabset\n### Vaccination Number\n\n```{python, warning=FALSE, message=FALSE}\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n  # Initialize\n  i_start=0\n  count=0\n  x_out=[]\n  y_out=[]\n  # Sequentially build mini-batches\n  while i_start + lookback + delay < x.shape[0]:\n    i_stop = i_start + lookback\n    i_pred = i_stop + delay\n    # report if desired\n    if verbose and count < 2:\n      print(\"indice range:\",i_start, i_stop, \"-->\",i_pre)\n    # define arrays\n    indices_to_keep = []\n    j = i_stop\n    while j >= i_start:\n      indices_to_keep.append(j)\n      j -= step\n    # create mini-batch sample\n    xtmp = x[indices_to_keep,:]\n    xtmp = xtmp[:,feature_columns]\n    ytmp=x[i_pred,target_columns]\n    x_out.append(xtmp)\n    y_out.append(ytmp)\n    # report if desired\n    if verbose and count <2:\n      print(xtmp, \"-->\", ytmp)\n      print(\"shape:\", xtmp.shape, \"-->\",ytmp.shape)\n    # plot\n    if verbose and count <2:\n      fig, ax = plt.subplots()\n      ax.plot(x, 'b-')\n      ax.plot(x,'bx')\n      ax.plot(indices_to_keep, xtmp, 'go')\n      ax.plot(i_pred*np.ones(len(target_columns)),ytmp, 'ro')\n      plt.show()\n    # update start point\n    if unique:\n      i_start += lookback\n    else:\n      i_start += 1\n    count += 1\n  return np.array(x_out), np.array(y_out)\n\ntrain = vac_train_data.reshape(vac_train_data.shape[0],1)\ntest = vac_test_data.reshape(vac_test_data.shape[0],1)\n\nvac_trainX, vac_trainY = form_arrays(train,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\nvac_testX, vac_testY = form_arrays(test,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\n\nprint(f'Train shape: {vac_trainX.shape} , {vac_trainY.shape}')\nprint(f'Test shape: {vac_testX.shape} , {vac_testY.shape}')\n```\n\n### Confirmed Case Number\n\n```{python, warning=FALSE, message=FALSE}\ntrain = case_train_data.reshape(case_train_data.shape[0],1)\ntest = case_test_data.reshape(case_test_data.shape[0],1)\n\ncase_trainX, case_trainY = form_arrays(train,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\ncase_testX, case_testY = form_arrays(test,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\n\nprint(f'Train shape: {case_trainX.shape} , {case_trainY.shape}')\nprint(f'Test shape: {case_testX.shape} , {case_testY.shape}')\n```\n\n### Death Case Number\n\n```{python, warning=FALSE, message=FALSE}\ntrain = death_train_data.reshape(death_train_data.shape[0],1)\ntest = death_test_data.reshape(death_test_data.shape[0],1)\n\ndeath_trainX, death_trainY = form_arrays(train,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\ndeath_testX, death_testY = form_arrays(test,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\n\nprint(f'Train shape: {death_trainX.shape} , {death_trainY.shape}')\nprint(f'Test shape: {death_testX.shape} , {death_testY.shape}')\n```\n\n:::\n\n\n# 5. RNN\n\nIn this section, I will focus on training a Recurrent Neural Network (RNN), which is specifically engineered for handling sequential data by utilizing cyclic connections that retain memory of prior inputs. To enhance the robustness and generalizability of the RNN, I plan to train it both with and without regularization techniques. To streamline this process, I will encapsulate the modeling code within a single function that accepts parameters for data, model type, and a boolean indicating whether regularization should be applied. This modular approach will facilitate easy adjustments and retesting under various configurations, improving efficiency and experimental flexibility.\n\nHere is how we define each function to generate the model:\n```{python, warning=FALSE, message=FALSE}\nimport tensorflow as tf\n# Utility functions \ndef regression_report(yt,ytp,yv,yvp):\n  print(\"--------- Regression Report ---------\")\n  print(\"TRAINING:\")\n  train_mse = np.mean((yt - ytp) ** 2)\n  train_mae = np.mean(np.abs(yt - ytp))\n  print(\"MSE\", train_mse)\n  print(\"MAE\", train_mae)\n  \n  # PARITY PLOT\n  fig, ax = plt.subplots()\n  ax.plot(yt, ytp, 'ro')\n  ax.plot(yt, yt, 'b-')\n  ax.set(xlabel='y_data', ylabel='y_predicted',\n        title = 'Training data parity plot (line y=x represents a perfect fit)')\n  plt.show()\n  \n  # PLOT PART OF THE PREDICTED TIME-SERIES\n  frac_plot=1.0\n  upper=int(frac_plot*yt.shape[0]);\n  fig, ax = plt.subplots()\n  ax.plot(yt[0:upper], 'b-')\n  ax.plot(ytp[0:upper], 'r-', alpha=0.5)\n  ax.plot(ytp[0:upper], 'ro', alpha=0.25)\n  ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Training: Time-series prediction')\n  plt.show()\n  \n  print(\"VALIDATION:\")\n  val_mse = np.mean((yv - yvp) ** 2)\n  val_mae = np.mean(np.abs(yv - yvp))\n  print(\"MSE\", val_mse)\n  print(\"MAE\", val_mae)\n  \n  # PARITY PLOT\n  fig,ax = plt.subplots()\n  ax.plot(yv,yvp, 'ro')\n  ax.plot(yv, yv,'b-')\n  ax.set(xlabel='y_data', ylabel='y_predicted',title='Validation data parity plot (line y=x represents a perfect fit)')\n  \n  # PLOT PART OF THE PREDICTED TIME-SERIES\n  upper=int(frac_plot*yv.shape[0])\n  fig,ax = plt.subplots()\n  ax.plot(yv[0:upper], 'b-')\n  ax.plot(yvp[0:upper], 'r-', alpha=0.5)\n  ax.plot(yvp[0:upper], 'ro', alpha=0.25)\n  ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Validation: Time-series prediction')\n  plt.show()\n  return train_mse, train_mae, val_mse, val_mae\n\ndef history_plot(history):\n  FS=18 #fontsize\n  history_dict = history.history\n  loss_values = history_dict['loss']\n  val_loss_values = history_dict['val_loss']\n  epochs = range(1, len(loss_values) + 1)\n  plt.plot(epochs, loss_values, 'bo', label='Training Loss')\n  plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n  plt.title('Training and Validation Loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n  plt.ylim(bottom=0) \n  plt.legend()\n  plt.show()\n\n# Define model function\ndef train_model(model_type, train_x, train_y, val_x, val_y, regularization = True, L2=1e-4):\n  if regularization:\n    reg = regularizers.L2(L2)\n  else:\n    reg = None\n  \n  # Define parameters\n  optimizer=\"rmsprop\"\n  loss_function = 'mean_squared_error'\n  learning_rate=0.01\n  numbers_epochs=200\n  input_shape=(train_x.shape[1],train_x.shape[2])\n  train_x1 = train_x.reshape(train_x.shape[0],train_x.shape[1]*train_x.shape[2])\n  batch_size=len(train_x1)              # batch training\n  \n  # BUILD MODEL\n  recurrent_hidden_units=32\n  \n  # CREATE MODEL\n  model = keras.Sequential()\n  \n  # ADD RECURRENT LAYER\n  if model_type == 'RNN':\n    model.add(SimpleRNN(\n    units=recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=input_shape, \n    # recurrent_dropout=0.8,\n    recurrent_regularizer=reg,\n    activation='relu')\n              )\n  elif model_type == 'GRU':\n    model.add(GRU(\n    units=recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=input_shape, \n    # recurrent_dropout=0.8,\n    recurrent_regularizer=reg,\n    activation='relu')\n              ) \n  elif model_type == 'LSTM':\n    model.add(LSTM(\n    units=recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=input_shape, \n    # recurrent_dropout=0.8,\n    recurrent_regularizer=reg,\n    activation='relu')\n              )\n  else:\n    print('Wrong model type')\n  \n  # NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \n  model.add(Dense(units=1, activation='linear'))\n  \n  # MODEL SUMMARY\n  print(model.summary()); #print(x_train.shape,y_train.shape)  \n  \n  # COMPILING THE MODEL \n  opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n  model.compile(optimizer=opt, loss=loss_function)\n  \n  # TRAINING YOUR MODEL\n  history_techmu = model.fit(train_x,\n                      train_y,\n                      epochs=numbers_epochs,\n                      batch_size=batch_size, verbose=False,\n                      validation_data=(val_x, val_y))\n  # History plot\n  history_plot(history_techmu)\n  \n  # Predictions \n  train_pred=model.predict(train_x)\n  val_pred=model.predict(val_x) \n  train_mse, train_mae, val_mse, val_mae = regression_report(train_y,train_pred,val_y,val_pred)\n  return train_mse, train_mae, val_mse, val_mae\n```\n\n\n::: panel-tabset\n### Vaccination Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults = []\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Confirmed Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Death Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'RNN'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n:::\n\n\n# 6. LSTM\n\nIn this section, I will be training a Long Short-Term Memory (LSTM) network, a specialized form of Recurrent Neural Network (RNN). LSTMs are designed to overcome the vanishing gradient problem inherent in traditional RNNs through a sophisticated gating mechanism. This mechanism effectively regulates the information flow into and out of the network's memory cells, facilitating the learning of long-term dependencies. I plan to evaluate the LSTM's performance both with and without the application of regularization techniques, to ascertain their impact on the model's ability to generalize.\n\n::: panel-tabset\n### Vaccination Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Confirmed Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Death Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'LSTM'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n:::\n\n\n# 7. GRU\n\n::: panel-tabset\n### Vaccination Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Confirmed Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n### Death Case Number\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model's parameters for more robust predictions.\n\n#### With Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n\n#### Without Regularization\n\n```{python, warning=FALSE, message=FALSE}\nmodel_type = 'GRU'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n```\n\n:::\n\n# 8. Discussion\n\n\nIn constructing our models, we design a framework tailored for RNN, LSTM, and GRU networks to tackle specific tasks. Each model begins with defining regularization practices to ensure generalization, using L2 regularization as needed. We configure the model using the RMSprop optimizer, a mean squared error loss function, and set a learning rate. Depending on the model type—RNN, GRU, or LSTM—we adjust the architecture by setting the number of recurrent hidden units and whether the model returns sequences. After assembling the model, we compile it, train it on our dataset over numerous epochs, and validate it using a separate dataset. This systematic approach allows us to fine-tune and compare the performance of each neural network architecture effectively.\n\n\n### 8.1 How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?\n```{python, warning=FALSE, message=FALSE}\n# Combine results into table\nresult_df = pd.DataFrame(results)\nresult_df['train_rmse'] = result_df['train_mse'].apply(lambda x: x ** .5)\nresult_df['val_rmse'] = result_df['val_mse'].apply(lambda x: x ** .5)\nresult_df[['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n```\n**Best Vaccination Number DL Model:**\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Vaccination'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True).head(1)\n```\n\nWe can see the best DL model for vaccination number is RNN model without regulation since it has the lowest validation RMSE value.\n\n**Best Confirmed Case DL Model:**\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Confirmed Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True).head(1)\n```\n\nWe can see the best DL model for confirmed case is RNN model without regulation since it has the lowest validation RMSE value.\n\n**Best Death Case DL Model:**\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Death Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True).head(1)\n```\n\nWe can see the best DL model for death case is LSTM model with regulation since it has the lowest validation RMSE value.\n\nIn our evaluation, the RNN and LSTM models demonstrated superior performance, excelling in their predictive capabilities. Conversely, the GRU model showed relatively lower effectiveness. This divergence in performance highlights the distinct advantages and challenges associated with each type of neural network architecture, emphasizing the importance of choosing the right model based on the specific requirements and nuances of the dataset at hand.\n\n### 8.2 What effect does including regularization have on your results?\n\nRegularization is crucial in machine learning, adding a penalty to the loss function to reduce the complexity of the model, thus minimizing overfitting to the training data. This technique varies in effectiveness across different datasets. For example, an un-regularized LSTM model achieved the lowest RMSE for emissions data, while a regularized GRU model was most effective for the temperature dataset. Across various datasets, a consistent observation was a higher RMSE in the validation set compared to the training set, indicating persistent overfitting despite the regularization's attempt to bridge the gap between training and validation performance.\n\n### 8.3 How far into the future can the deep learning model accurately predict the future?\n\nDeep learning models' ability to predict future events hinges on the data they have encountered during training. These models, fundamentally supervised, learn and generate predictions based on the sequences they've been exposed to. Thus, their predictive power extends only as far as recognizing patterns similar to those within their training datasets. The farther into the future a prediction extends, the more reliant it becomes on the quality and representativeness of historical data, limiting accuracy when facing novel scenarios or trends not previously seen during training.\n\n\n### 8.4. How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models from HW-3?\n\n::: panel-tabset\n#### Vaccination Number\n\nFrom Arima model, below are the computed metrics:\n\n| Metric       | Value               |\n|--------------|---------------------|\n| ME           | -1.260901e+04       |\n| RMSE         | 4.404350e+04        |\n| MAE          | 2.961164e+04        |\n| MPE          | -1.819884e+00       |\n| MAPE         | 2.997255e+01        |\n| MASE         | 5.644007e-01        |\n| ACF1         | -1.726600e-01       |\n\nFrom Deep Learning model, below are the computed metrics:\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Vaccination'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n```\n\n#### Confirmed Case Number\n\nFrom Arima model, below are the computed metrics:\n\n| Metric       | Value               |\n|--------------|---------------------|\n| ME           | 5.887429e+04       |\n| RMSE         | 2.036015e+06        |\n| MAE          | 1.092948e+06        |\n| MPE          | 1.626543e+04       |\n| MAPE         | 1.876329e+04        |\n| MASE         | 2.083170e+01        |\n| ACF1         | 1.041066e-01       |\n\nFrom Deep Learning model, below are the computed metrics:\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Confirmed Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n```\n\n#### Death Case Number\n\nFrom Arima model, below are the computed metrics:\n\n| Metric       | Value               |\n|--------------|---------------------|\n| ME           | 1373.71612640       |\n| RMSE         | 8276.58694400        |\n| MAE          | 6716.48728344        |\n| MPE          | 64.68052062       |\n| MAPE         | 74.15331696         |\n| MASE         | 0.12801688        |\n| ACF1         | -0.09912142        |\n\nFrom Deep Learning model, below are the computed metrics:\n\n```{python, warning=FALSE, message=FALSE}\nresult_df[result_df.data_type=='Death Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n```\n\n:::\n\n\nThe performance analysis shows that DL models often outperform traditional models, notably in terms of RMSE, showcasing their robustness in handling diverse datasets. However, the efficacy of these models is somewhat constrained by the size of the available data. Small datasets tend to skew performance, highlighting a potential limitation in scenarios where expansive data is not available.\n\nFor ARIMA models, their predictive capability is generally limited when dealing with noisy, fluctuating data, often producing overly simplistic forecasts. In contrast, ARIMAX and VAR models manage to capture underlying trends more effectively, albeit with some inconsistency in the presence of volatility. Deep Learning models, on the other hand, excel with larger datasets, achieving near-perfect predictions. The performance disparity underscores the importance of dataset size in leveraging the full potential of advanced DL models.\n\n\n\n# 9. Write a discussion paragraph Comparing your models (use RMSE) and forecasts from these sections with your Deep Learning Models.\n\nThis analysis has demonstrated the potent capabilities of deep learning in forecasting time series data. The efficacy of deep learning models relative to traditional approaches is challenging to evaluate directly due to discrepancies in the training and validation datasets. However, the Root Mean Square Error (RMSE) metric reveals considerably higher values for traditional time series models, implying that these models may be less adept at capturing the variability within the dataset. Conversely, deep learning models have consistently produced RMSE values under 1, suggesting superior performance in tracking the actual data trends.\n\nA closer inspection of the training and validation performance of deep learning models against the actual forecasts from traditional models, like ARIMA, underscores this contrast. The ARIMA models, despite their sophistication, often faltered in accounting for the dataset's inherent variance. In contrast, deep learning models demonstrated an ability to assimilate and reflect this variance in their predictions. Notably, the hierarchy of deep learning model performance typically favored the GRU, followed by the LSTM, and then the RNN, aligning with the evolutionary complexity of these architectures.\n\nTraditional time series models necessitate transforming data into a stationary form, with consistent mean and variance, to make accurate extrapolations. This often involves processes such as differencing, which may not always be sufficient for effective forecasting. On the other hand, deep learning models function as universal approximators, capable of modeling any form of data variance. This attribute renders them particularly suitable for complex time series datasets characterized by pronounced fluctuations and seasonal variations, enabling them to deliver more accurate and robust forecasts.\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"DL.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"lumen","title":"Deep Learning for Time Series","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}