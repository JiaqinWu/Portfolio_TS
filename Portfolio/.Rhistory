ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Validation: Time-series prediction')
plt.show()
return train_mse, train_mae, val_mse, val_mae
def history_plot(history):
FS=18 #fontsize
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, 'bo', label='Training Loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Define model function
def train_model(model_type, train_x, train_y, val_x, val_y, regularization = True, L2=1e-4):
if regularization:
reg = regularizers.L2(L2)
else:
reg = None
# Define parameters
optimizer="rmsprop"
loss_function="MeanSquaredError"
learning_rate=0.01
numbers_epochs=200
input_shape=(train_x.shape[1],train_x.shape[2])
train_x1 = train_x.reshape(train_x.shape[0],train_x.shape[1]*train_x.shape[2])
batch_size=len(train_x1)              # batch training
# BUILD MODEL
recurrent_hidden_units=32
# CREATE MODEL
model = keras.Sequential()
# ADD RECURRENT LAYER
if model_type == 'RNN':
model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
elif model_type == 'GRU':
model.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
elif model_type == 'LSTM':
model.add(LSTM(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
else:
print('Wrong model type')
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR
model.add(Dense(units=1, activation='linear'))
# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)
# COMPILING THE MODEL
opt = keras.optimizers.legacy.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)
# TRAINING YOUR MODEL
history_techmu = model.fit(train_x,
train_y,
epochs=numbers_epochs,
batch_size=batch_size, verbose=False,
validation_data=(val_x, val_y))
# History plot
history_plot(history_techmu)
# Predictions
train_pred=model.predict(train_x)
val_pred=model.predict(val_x)
train_mse, train_mae, val_mse, val_mae = regression_report(train_y,train_pred,val_y,val_pred)
return train_mse, train_mae, val_mse, val_mae
model_type = 'RNN'
reg = True
train_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)
result_dict = {
'data_type':'Vaccination',
'model_type':model_type,
'reg':reg,
'train_mse':train_mse,
'train_mae':train_mae,
'val_mse':val_mse,
'val_mae':val_mae
}
results = []
results.append(result_dict)
print(f'Model results: {result_dict}')
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.optimizers import RMSprop
# Utility functions
def regression_report(yt,ytp,yv,yvp):
print("--------- Regression Report ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='y_data', ylabel='y_predicted',
title = 'Training data parity plot (line y=x represents a perfect fit)')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot=1.0
upper=int(frac_plot*yt.shape[0]);
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Training: Time-series prediction')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig,ax = plt.subplots()
ax.plot(yv,yvp, 'ro')
ax.plot(yv, yv,'b-')
ax.set(xlabel='y_data', ylabel='y_predicted',title='Validation data parity plot (line y=x represents a perfect fit)')
# PLOT PART OF THE PREDICTED TIME-SERIES
upper=int(frac_plot*yv.shape[0])
fig,ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Validation: Time-series prediction')
plt.show()
return train_mse, train_mae, val_mse, val_mae
def history_plot(history):
FS=18 #fontsize
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, 'bo', label='Training Loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Define model function
def train_model(model_type, train_x, train_y, val_x, val_y, regularization = True, L2=1e-4):
if regularization:
reg = regularizers.L2(L2)
else:
reg = None
# Define parameters
optimizer="rmsprop"
loss_function="MeanSquaredError"
learning_rate=0.01
numbers_epochs=200
input_shape=(train_x.shape[1],train_x.shape[2])
train_x1 = train_x.reshape(train_x.shape[0],train_x.shape[1]*train_x.shape[2])
batch_size=len(train_x1)              # batch training
# BUILD MODEL
recurrent_hidden_units=32
# CREATE MODEL
model = keras.Sequential()
# ADD RECURRENT LAYER
if model_type == 'RNN':
model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
elif model_type == 'GRU':
model.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
elif model_type == 'LSTM':
model.add(LSTM(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
else:
print('Wrong model type')
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR
model.add(Dense(units=1, activation='linear'))
# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)
# COMPILING THE MODEL
opt = RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)
# TRAINING YOUR MODEL
history_techmu = model.fit(train_x,
train_y,
epochs=numbers_epochs,
batch_size=batch_size, verbose=False,
validation_data=(val_x, val_y))
# History plot
history_plot(history_techmu)
# Predictions
train_pred=model.predict(train_x)
val_pred=model.predict(val_x)
train_mse, train_mae, val_mse, val_mae = regression_report(train_y,train_pred,val_y,val_pred)
return train_mse, train_mae, val_mse, val_mae
from tensorflow.keras.optimizers import RMSprop
#from tensorflow.keras.optimizers import RMSprop
# Utility functions
def regression_report(yt,ytp,yv,yvp):
print("--------- Regression Report ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='y_data', ylabel='y_predicted',
title = 'Training data parity plot (line y=x represents a perfect fit)')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot=1.0
upper=int(frac_plot*yt.shape[0]);
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Training: Time-series prediction')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig,ax = plt.subplots()
ax.plot(yv,yvp, 'ro')
ax.plot(yv, yv,'b-')
ax.set(xlabel='y_data', ylabel='y_predicted',title='Validation data parity plot (line y=x represents a perfect fit)')
# PLOT PART OF THE PREDICTED TIME-SERIES
upper=int(frac_plot*yv.shape[0])
fig,ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Validation: Time-series prediction')
plt.show()
return train_mse, train_mae, val_mse, val_mae
def history_plot(history):
FS=18 #fontsize
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, 'bo', label='Training Loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Define model function
def train_model(model_type, train_x, train_y, val_x, val_y, regularization = True, L2=1e-4):
if regularization:
reg = regularizers.L2(L2)
else:
reg = None
# Define parameters
optimizer="rmsprop"
loss_function="MeanSquaredError"
learning_rate=0.01
numbers_epochs=200
input_shape=(train_x.shape[1],train_x.shape[2])
train_x1 = train_x.reshape(train_x.shape[0],train_x.shape[1]*train_x.shape[2])
batch_size=len(train_x1)              # batch training
# BUILD MODEL
recurrent_hidden_units=32
# CREATE MODEL
model = keras.Sequential()
# ADD RECURRENT LAYER
if model_type == 'RNN':
model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
elif model_type == 'GRU':
model.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
elif model_type == 'LSTM':
model.add(LSTM(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
else:
print('Wrong model type')
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR
model.add(Dense(units=1, activation='linear'))
# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)
# COMPILING THE MODEL
opt = RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)
# TRAINING YOUR MODEL
history_techmu = model.fit(train_x,
train_y,
epochs=numbers_epochs,
batch_size=batch_size, verbose=False,
validation_data=(val_x, val_y))
# History plot
history_plot(history_techmu)
# Predictions
train_pred=model.predict(train_x)
val_pred=model.predict(val_x)
train_mse, train_mae, val_mse, val_mae = regression_report(train_y,train_pred,val_y,val_pred)
return train_mse, train_mae, val_mse, val_mae
model_type = 'RNN'
reg = True
train_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)
result_dict = {
'data_type':'Vaccination',
'model_type':model_type,
'reg':reg,
'train_mse':train_mse,
'train_mae':train_mae,
'val_mse':val_mse,
'val_mae':val_mae
}
results = []
results.append(result_dict)
print(f'Model results: {result_dict}')
import tensorflow as tf
# Utility functions
def regression_report(yt,ytp,yv,yvp):
print("--------- Regression Report ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='y_data', ylabel='y_predicted',
title = 'Training data parity plot (line y=x represents a perfect fit)')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot=1.0
upper=int(frac_plot*yt.shape[0]);
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Training: Time-series prediction')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig,ax = plt.subplots()
ax.plot(yv,yvp, 'ro')
ax.plot(yv, yv,'b-')
ax.set(xlabel='y_data', ylabel='y_predicted',title='Validation data parity plot (line y=x represents a perfect fit)')
# PLOT PART OF THE PREDICTED TIME-SERIES
upper=int(frac_plot*yv.shape[0])
fig,ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Validation: Time-series prediction')
plt.show()
return train_mse, train_mae, val_mse, val_mae
def history_plot(history):
FS=18 #fontsize
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, 'bo', label='Training Loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Define model function
def train_model(model_type, train_x, train_y, val_x, val_y, regularization = True, L2=1e-4):
if regularization:
reg = regularizers.L2(L2)
else:
reg = None
# Define parameters
optimizer="rmsprop"
loss_function="MeanSquaredError"
learning_rate=0.01
numbers_epochs=200
input_shape=(train_x.shape[1],train_x.shape[2])
train_x1 = train_x.reshape(train_x.shape[0],train_x.shape[1]*train_x.shape[2])
batch_size=len(train_x1)              # batch training
# BUILD MODEL
recurrent_hidden_units=32
# CREATE MODEL
model = keras.Sequential()
# ADD RECURRENT LAYER
if model_type == 'RNN':
model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
elif model_type == 'GRU':
model.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
elif model_type == 'LSTM':
model.add(LSTM(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape,
# recurrent_dropout=0.8,
recurrent_regularizer=reg,
activation='relu')
)
else:
print('Wrong model type')
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR
model.add(Dense(units=1, activation='linear'))
# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)
# COMPILING THE MODEL
opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)
model.compile(optimizer=opt, loss=loss_function)
# TRAINING YOUR MODEL
history_techmu = model.fit(train_x,
train_y,
epochs=numbers_epochs,
batch_size=batch_size, verbose=False,
validation_data=(val_x, val_y))
# History plot
history_plot(history_techmu)
# Predictions
train_pred=model.predict(train_x)
val_pred=model.predict(val_x)
train_mse, train_mae, val_mse, val_mae = regression_report(train_y,train_pred,val_y,val_pred)
return train_mse, train_mae, val_mse, val_mae
model_type = 'RNN'
reg = True
train_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)
result_dict = {
'data_type':'Vaccination',
'model_type':model_type,
'reg':reg,
'train_mse':train_mse,
'train_mae':train_mae,
'val_mse':val_mse,
'val_mae':val_mae
}
results = []
results.append(result_dict)
print(f'Model results: {result_dict}')
import tensorflow as tf
print(tf.__version__)
reticulate::repl_python()
