[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site. Leowu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series. A time series is a sequence of data points or observations collected or recorded over a period of time at specific, equally spaced intervals. Each data point in a time series is associated with a particular timestamp or time period, making it possible to analyze and study how a particular variable or phenomenon changes over time. Time series data can be found in various domains and can represent a wide range of phenomena, including financial data, economic indicators, weather measurements, stock prices, sales figures, and more.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\n\nKey characteristics of time series data include:\nTemporal Order: Time series data is ordered chronologically, with each data point representing an observation at a specific point in time. The order of data points is critical for understanding trends and patterns over time.\nEqually Spaced Intervals: In most cases, time series data is collected at regular intervals, such as hourly, daily, weekly, monthly, or yearly. However, irregularly spaced time series data can also exist.\nDependency: Time series data often exhibits temporal dependency, meaning that the value at a given time is influenced by or related to the values at previous times. This dependency can take various forms, including trends, seasonality. This serial correlation is called as autocorrelation.\nComponents: Time series data can typically be decomposed into various components, including:\nTrend: The long-term movement or direction in the data. Seasonality: Repeating patterns or cycles that occur at fixed intervals. Noise/Irregularity: Random fluctuations or variability in the data that cannot be attributed to the trend or seasonality.\nApplications: Time series data is widely used for various applications, including forecasting future values, identifying patterns and anomalies, understanding underlying trends, and making informed decisions based on historical data.\nAnalyzing time series data involves techniques like time series decomposition, smoothing, statistical modeling, and forecasting. This class will cover but not be limited to traditional time series modeling including ARIMA, SARIMA, the multivariate Time Series modeling including; ARIMAX, SARIMAX, and VAR models, Financial Time Series modeling including; ARCH, GARCH models, and E-GARCH, M-GARCH..ect, Bayesian structural time series (BSTS) models, Spectral Analysis and Deep Learning Techniques for Time Series. Researchers and analysts use software tools like Python, R, and specialized time series libraries to work with and analyze time series data effectively.\nTime series analysis is essential in fields such as finance, economics, epidemiology, environmental science, engineering, and many others, as it provides insights into how variables change over time and allows for the development of predictive models to forecast future trends and outcomes."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The emergence of the COVID-19 pandemic marked an unprecedented juncture in human history. As the virus proliferated, nations worldwide found themselves grappling with the need for swift and decisive action to curb its relentless spread. Soon after the outbreak of the pandemic, states in the U.S. began implementing various community mitigation strategies (e.g., mandatory stay at home orders and business closures) to curb the spread of COVID-19. In total, 42 U.S. states and territories issued mandatory stay-at-home orders, covering 73% of U.S. counties. In the United States, a paradigm shift unfolded, compelling individuals and organizations to reevaluate and adapt their daily routines. A notable consequence of this upheaval was the widespread adoption of remote work, a strategic response aimed at mitigating the transmission of the virus while ensuring the continuity of essential services. Internationally, countries like China responded to the crisis by enacting localized policies, effectively delineating geographic boundaries to staunch the virus’s advance. These measures, ranging from city-wide lockdowns to regional containment strategies, reflected the urgency and gravity of the situation. As societies navigated the uncharted territory of a novel and highly infectious pathogen, a collective sense of unfamiliarity and uncertainty permeated daily life. In the face of this existential threat, the global medical community rallied with unparalleled speed and collaboration. Scientific institutions, pharmaceutical companies, and researchers joined forces to develop and deploy effective vaccines.\nHowever, as these vaccines became available, a nuanced narrative unfolded, marked by public skepticism and questions about their efficacy, given that no vaccine could guarantee absolute immunity. This project endeavors to unravel the multifaceted impact of COVID-19 vaccination rates over time, exploring its influence across diverse realms. Beyond the standard metrics of confirmed cases, mortality rates, and hospitalizations caused by COVID-19, the analysis extends its gaze to encompass economic indicators, stock prices, and even political dynamics. The intricacies of vaccine acceptance and its dynamic impact will be dissected through our time series analysis, revealing the social influence shaped by the ongoing vaccination efforts.\n\n\n\n\nIn the pursuit of a comprehensive understanding, our research embarks on a meticulous examination of public data sources, seeking to address pivotal questions that encapsulate the intricate interplay between COVID-19 vaccination rates and multifaceted dimensions. Through the lens of data-driven analysis, our endeavor unfolds as a systematic exploration into the evolving dynamics of vaccination and its cascading impact across diverse realms.\nOne of the central inquiries guiding this investigation is the temporal evolution of COVID-19 vaccination rates and their influence on various sectors. Delving into the data, we seek to discern patterns and shifts in vaccination rates over time, unraveling the nuanced story of how the collective endeavor to immunize populations unfolds. We examine the effectiveness of the vaccine from a quantitative perspective, scrutinizing its role in mitigating the spread of the virus and potentially altering the trajectory of the pandemic.\nHospitalization rates stand as a critical metric in gauging the efficacy of vaccination efforts. We probe into whether the vaccine, by conferring immunity, contributes to alleviating the burden on healthcare systems. Through rigorous data analysis, we aim to illuminate the extent to which vaccination rates correlate with fluctuations in hospitalization numbers, providing valuable insights into the broader public health landscape.\nThe economic ramifications of COVID-19 vaccination constitute another facet of our exploration. By scrutinizing the data, we seek to ascertain whether higher vaccination rates correspond to economic recovery. Unpacking the intricate relationship between vaccination efforts and economic indicators, our analysis endeavors to shed light on the potential role of vaccination campaigns in fostering economic resilience.\nIn the financial realm, we examine the impact of vaccination on medical corporations, particularly in the context of stock prices. This inquiry navigates the nexus between vaccination investments and market dynamics, unraveling the intricate dance between public health imperatives and corporate performance.\nBeyond these dimensions, our research extends its gaze into the political arena, exploring the relationship between vaccination rates and party support. By dissecting the data, we aim to uncover whether vaccination rates influence political sentiments, offering a nuanced understanding of how public health measures intertwine with political dynamics.\n\n\n\n\nIn the post-pandemic landscape, scholars have delved into comprehensive research on COVID-19 and the ramifications of vaccination, yielding valuable insights. Moghadas et al. (2020) highlight the substantial benefits of COVID-19 vaccines, demonstrating their potential to significantly reduce future infection rates, hospitalizations, and deaths, even with limited protection against infection. Conversely, Barro’s (2022) findings suggest sizable negative effects of vaccination on critical metrics until early December 2021, indicating potential waning efficacy over time.\nFurther nuanced insights emerge from Guo et al. (2022), indicating complex associations between county-level socioeconomic factors and vaccination rates. Per capita income is negatively linked to vaccination rates in counties with higher proportions of BIPOC individuals, while the unemployment rate shows a negative association in counties with higher proportions of non-Hispanic White individuals.\nKhalfaoui et al. (2021) contribute to the literature by revealing a positive and significant influence of COVID-19 vaccination, infection rates, and case fatality ratios on S&P 500 returns at various business cycle frequencies, suggesting an interconnected relationship with financial markets.\nAdditionally, examining the intersection of vaccination and political dynamics, Galston et al. (2022) reveal a correlation between COVID-19 vaccination rates and party support. States with vaccination rates above the national average predominantly favored Joe Biden in the last November elections, while those below the average leaned towards Donald Trump. The vaccination-rate gap between counties won by Biden and Trump increased significantly, underscoring the evolving political landscape influenced by vaccination trends.\nReference\n\nMoghadas, S. M., Vilches, T. N., Zhang, K., Wells, C. R., Shoukat, A., Singer, B. H., Meyers, L. A., Neuzil, K. M., Langley, J. M., Fitzpatrick, M. C., & Galvani, A. P. (2021). The impact of vaccination on COVID-19 outbreaks in the United States. medRxiv : the preprint server for health sciences, 2020.11.27.20240051. https://doi.org/10.1101/2020.11.27.20240051\nGalston, W. A., Kamarck, E., West, D. M., & Cecilia Elena Rouse, A. P. (2022, March 9). For covid-19 vaccinations, party affiliation matters more than race and ethnicity. Brookings. https://www.brookings.edu/articles/for-covid-19-vaccinations-party-affiliation-matters-more-than-race-and-ethnicity/\nGuo Y, Kaniuka AR, Gao J, Sims OT. An Epidemiologic Analysis of Associations between County-Level Per Capita Income, Unemployment Rate, and COVID-19 Vaccination Rates in the United States. International Journal of Environmental Research and Public Health. 2022; 19(3):1755. https://doi.org/10.3390/ijerph19031755\nKhalfaoui, R., Nammouri, H., Labidi, O., & Ben Jabeur, S. (2021). Is the COVID-19 vaccine effective on the US financial market? Public Health, 198, 177–179. https://doi.org/10.1016/j.puhe.2021.07.026\nBarro, R. (2022). Vaccination Rates and Covid Outcomes across U.S. States. https://doi.org/10.3386/w29884\n\n\n\n\n\nOur investigation employs five distinct analytical angles, each offering a unique lens through which we comprehensively address the data science question and navigate the intricate landscape of COVID-19 vaccination dynamics.\n1. Temporal Evolution of Vaccination Rates: We scrutinize patterns and shifts in vaccination rates over time, unraveling the nuanced story of the collective endeavor to immunize populations and its temporal evolution.\n2. Impact on Healthcare Systems: Hospitalization rates stand as a critical metric in gauging the efficacy of vaccination efforts. We probe into whether vaccination, by conferring immunity, contributes to alleviating the burden on healthcare systems.\n3. Economic Recovery: Scrutinizing the economic ramifications of COVID-19 vaccination, we aim to ascertain whether higher vaccination rates correspond to economic recovery, unpacking the intricate relationship between vaccination efforts and economic indicators.\n4. Financial Impact: In the financial realm, we examine the impact of vaccination on medical corporations, particularly in the context of stock prices. This inquiry navigates the nexus between vaccination investments and market dynamics.\n5. Political Dynamics: Our research extends into the political arena, exploring the nexus between vaccination rates and party support. By dissecting the data, we aim to uncover whether vaccination rates influence political sentiments, offering insights into the intersection of public health measures and political dynamics.\n\n\n\n\n\n\n\nHow have COVID-19 vaccination rates evolved over time, and what patterns can be discerned?\nTo what extent has the vaccine proven effective in mitigating the spread of the virus, and how does this effectiveness manifest quantitatively?\nDoes higher vaccination rates correlate with a reduction in hospitalization numbers, contributing to a more resilient healthcare system?\nCan we observe a correlation between vaccination rates and economic recovery, and if so, what are the nuances of this relationship?\nHow do vaccination efforts impact the stock prices of medical corporations, and what are the broader financial implications?\nTo what extent do vaccination rates influence political sentiments and party support, unveiling the intersection of public health and political dynamics?\nAre there geographical variations in vaccination rates, and how do regional differences impact the overall efficacy of vaccination campaigns?\nHow do demographic factors contribute to vaccination disparities, and what are the implications for public health interventions?\nWhat lessons can be drawn from the global response to COVID-19 vaccination for future pandemics and public health emergencies?\nCan time series analysis and forecasting models predict the future impact of the COVID-19 vaccination in the post-pandemic era?"
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "Data Sources",
    "section": "",
    "text": "The dataset utilized in this project is from the Our World in Data Github Repository, a reputable and freely accessible data resource website. Our World in Data relies on the collaborative efforts of various contributors, making it a valuable platform for comprehensive datasets. Specifically, we leveraged state-by-state COVID-19 vaccination rate data sourced from the United States Centers for Disease Control and Prevention (CDC) daily updates, ensuring the reliability and accuracy of the information used in our analysis.\n\n\n\n\nCode\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# data glimpse\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million', 'people_vaccinated_per_hundred', 'people_fully_vaccinated_per_hundred')\nt &lt;- vac_df[, cols_show]\nrbind(head(t, 3), tail(t, 3)) %&gt;%\n  kbl(row.names = FALSE) %&gt;%\n  kable_styling()\n\n\n\n\n\ndate\nlocation\ndaily_vaccinations_per_million\npeople_vaccinated_per_hundred\npeople_fully_vaccinated_per_hundred\n\n\n\n\n2021-01-12\nAlabama\nNA\n1.45\n0.15\n\n\n2021-01-13\nAlabama\n1205\n1.53\n0.19\n\n\n2021-01-14\nAlabama\n1445\n1.64\nNA\n\n\n2023-05-08\nWyoming\n213\nNA\nNA\n\n\n2023-05-09\nWyoming\n252\nNA\nNA\n\n\n2023-05-10\nWyoming\n294\n61.12\n53.15\n\n\n\n\n\n\n\n\n\n\n\nlocation: name of the state or federal entity.\ndate: date of the observation.\ndaily_vaccinations: new doses administered per day (7-day smoothed). For countries that don’t report data on a daily basis, we assume that doses changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window. An example of how we perform this calculation can be found here.\ndaily_vaccinations_per_million: daily_vaccinations per 1,000,000 people in the total population of the state.\npeople_vaccinated: total number of people who received at least one vaccine dose. If a person receives the first dose of a 2-dose vaccine, this metric goes up by 1. If they receive the second dose, the metric stays the same.\npeople_vaccinated_per_hundred: people_vaccinated per 100 people in the total population of the state.\npeople_fully_vaccinated: total number of people who received all doses prescribed by the initial vaccination protocol. If a person receives the first dose of a 2-dose vaccine, this metric stays the same. If they receive the second dose, the metric goes up by 1.\npeople_fully_vaccinated_per_hundred: people_fully_vaccinated per 100 people in the total population of the state."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Jiaqin Wu\nPhone: (703) 294-9099\nEmail: jw2104@georgetown.edu\nGitHub: GitHub Link\nLinkedIn: LinkedIn Profile"
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "1. Vaccination Rate\nThis visualization is created by Tableau, check the interactive dashboard here.\n\nDaily Vaccination Number Per Million\n\nIn this analysis, we focus on three prominent regions in the United States: Washington D.C., New York, and California. By examining the daily vaccination numbers over time, we gain insights into the vaccination trends within each of these regions. Notably, May 2021 emerges as a peak period, marked by a significant surge in vaccination rates across all three regions, indicative of a concerted effort to inoculate a large portion of the population. However, we also observe sporadic peaks in certain time periods and states, such as December 2021 in New York and February 2022 in Washington D.C. These anomalies suggest localized events or circumstances driving heightened vaccination activity within specific regions. Given the absence of nationwide events impacting vaccination rates uniformly, we hypothesize that these sudden spikes are likely attributable to unique local factors or initiatives. Further investigation into the underlying causes of these localized surges could provide valuable insights into the dynamics of vaccination efforts at the regional level.\n\n\nNumber of People Vaccinated Per Hundred\n\nIn this analysis, we have chosen May 10th, 2023, as our cutoff date to examine the disparities in vaccination rates per hundred individuals across different states in the post-pandemic landscape. Our visualization reveals striking contrasts among states: Massachusetts, Washington D.C., and Maryland exhibit notably higher vaccination rates, reflecting proactive vaccination campaigns and robust healthcare infrastructure. Conversely, states situated in the heartland consistently register lower vaccination rates. However, it is encouraging to note that even in these regions, the overall vaccination rates remain above 56%, underscoring a nationwide effort to achieve widespread immunity. This observation suggests a collective commitment to combating the pandemic, albeit with variations in regional implementation and success.\n\n\nNumber of People Vaccinated Per Hundred\n In this analysis, we have selected Alabama, Alaska, and American Samoa as representative examples for analysis. We observe a consistent upward trend in the fully vaccinated rate until April 2022, after which the rate stabilizes. Notably, American Samoa emerges as the frontrunner with the highest fully vaccinated rate, surpassing 90%, indicative of a successful vaccination campaign and strong community engagement. In contrast, Alabama lags behind the other two states, with its fully vaccinated rate only surpassing 53%. This discrepancy underscores the importance of targeted interventions and resources to address disparities and ensure equitable access to vaccination across diverse regions.\n\n\n\n2. Newly Confirmed Cases & Death Cases\n\nNewly Confirmed Cases\n\n\nCode\nwide_data &lt;- read_csv(\"Datasets/covid_confirmed_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ncon_case_df &lt;- long_data %&gt;%\n  group_by(State, date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Calculate the daily new cases\ncon_case_df1 &lt;- con_case_df %&gt;%\n  group_by(State) %&gt;%\n  mutate(new_cases = value_sum - lag(value_sum, default = 0))\n\n# Convert date column to Date format\ncon_case_df1$date &lt;- as.Date(con_case_df1$date)\n\n# Filter data for DC\ncon_case_df2 &lt;- subset(con_case_df1, State == 'DC')\n\n# Visualize the plot\ngg&lt;-ggplot(data = con_case_df2, aes(x = date, y = new_cases)) +\n  geom_line(colour = \"#5a3196\") +\n  labs(x = \"Date\", y = \"Newly Confirmed Cases\", title = \"Trend of Newly Confirmed COVID-19 Cases in DC\") +\n  theme_minimal() \n\nplotly_gg &lt;- ggplotly(gg)\nplotly_gg\n\n\n\n\n\n\nUpon examining the visualization with Washington D.C. as our focal point, notable trends emerge. Between December 2021 and January 2022, a prominent peak in newly confirmed COVID-19 cases is observed, marking a period of heightened transmission within the region. This surge may be attributed to the emergence of viral mutations or other factors influencing transmission dynamics. Additionally, a smaller peak is evident in April 2022, suggesting fluctuations in case numbers over time. These findings underscore the dynamic nature of the COVID-19 pandemic and highlight the importance of monitoring and understanding temporal changes in case counts across different states.\n\n\nDeath Cases\n\n\nCode\nwide_data &lt;- read_csv(\"Datasets/covid_deaths_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ndead_case_df &lt;- long_data %&gt;%\n  group_by(State, date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Calculate the daily new cases\ndead_case_df1 &lt;- dead_case_df %&gt;%\n  group_by(State) %&gt;%\n  mutate(new_cases = value_sum - lag(value_sum, default = 0))\n\n# Convert date column to Date format\ndead_case_df1$date &lt;- as.Date(con_case_df1$date)\n\n# Filter data for DC\ndead_case_df2 &lt;- subset(dead_case_df1, State == 'DC')\n\n# Visualize the plot\ngg&lt;-ggplot(data = dead_case_df2, aes(x = date, y = new_cases)) +\n  geom_line(colour = \"#5a3196\") +\n  labs(x = \"Date\", y = \"Dead Cases\", title = \"Trend of Dead COVID-19 Cases in DC\") +\n  theme_minimal() \n\nplotly_gg &lt;- ggplotly(gg)\nplotly_gg\n\n\n\n\n\n\nExamining Washington D.C. as an example, we discern intriguing patterns in the trajectory of COVID-19 fatalities. In early 2020, a notable peak in deaths is observed, reflecting the initial surge of the pandemic. However, by July 2020, a discernible decline in fatalities begins, indicative of successful public health interventions and increased awareness. Despite this overall decline, intermittent peaks in January 2021, January 2022, and April 2022 punctuate the trend, suggesting periodic spikes in mortality rates. Despite these fluctuations, the overarching trend demonstrates a gradual decrease in fatalities over time, underscoring the effectiveness of ongoing mitigation strategies and vaccination efforts.\n\n\n\n3. COVID-19 Hospitalization Number\nThe visualization is created by Rshiny, you can check the interactive app here.\n You have the flexibility to choose the specific state, year, and month you wish to explore, as well as the variables you’d like to visualize in the figure.\n You can find descriptive information regarding the hospitalization bed capacity in Washington D.C. and other states as well, including the total number of inpatient beds, as well as the utilization rates for both general inpatient beds and those specifically allocated for COVID-19 patients.\n\n\n4. Economic Indicators (GDP Per Capita & Unemployment Rate)\n\nGDP Per Capita\n\n\nCode\ngdp &lt;- read_csv('Datasets/gdp.csv')\n\n# Convert DATE column from m/d/yy format to Date object and reformat to \"Year\" only for simplicity\ngdp$DATE &lt;- format(mdy(gdp$DATE), \"%Y/%m/%d\")\n\n# Convert GDP column to numeric (floating-point) format if not already\ngdp$GDP &lt;- as.numeric(gdp$GDP)\n\n# Visualize the plot using ggplot2\ngg &lt;- ggplot(data = gdp, aes(x = DATE, y = GDP, group = 1)) +  # 'group = 1' ensures the line plot considers all points\n  geom_line(colour = \"#5a3196\") +  # Use geom_line for line plot\n  geom_point(colour = \"#5a3196\") +  # Optional: add points on top of the line\n  labs(x = \"Year\", y = \"GDP Per Capita\", title = \"Trend of GDP Per Capita Over Years in US\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels to prevent overlap\n\n# Convert ggplot object to plotly for interactive plotting\nplotly_gg &lt;- ggplotly(gg)\n\n# Display the plotly object\nplotly_gg\n\n\n\n\n\n\nWe observe a consistent annual increase in GDP per capita in the United States in the most of time. But there is a sudden decrease in the second quarter in 2020, which is mainly caused by COVID-19. After that period, the GDP per capita began to recover and keep increasing consistently.\n\n\nUnemployment Rate\n\n\nCode\nunemp &lt;- read_csv('Datasets/unemployment.csv')\nkey_cols &lt;- c(\"Location\")\nvalue_cols &lt;- setdiff(names(unemp), key_cols)\nunemp1 &lt;- pivot_longer(\n  unemp,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"Unemployment\"\n)\n\n# Convert Time column to Date format\nunemp1$Time &lt;- as.Date(paste0(unemp1$Time, \"-01\"))\n\n# Convert Unemployment column to numeric (floating-point) format\nunemp1$Unemployment &lt;- as.numeric(unemp1$Unemployment)\n\n# Focus on US\nunemp2 &lt;- unemp1[unemp1$Location =='United States',]\n\n# Visualize the plot\ngg &lt;- ggplot(data = unemp2, aes(x = Time, y = Unemployment)) +\n  geom_point() +\n  geom_line(colour = \"#5a3196\") +\n  labs(x = \"Year\", y = \"Unemployment Rate\", title = \"Trend of Unemployment Rate by Month in US\") +\n  theme_minimal() \n\nplotly_gg &lt;- ggplotly(gg)\nplotly_gg\n\n\n\n\n\n\nAnalyzing the unemployment rate data in the US, a notable spike is observed after April 2020, marked as a distinct cutoff point in the plot. This sudden increase coincides with the onset of the pandemic outbreak, suggesting a correlation between the two events. Subsequently, post-April 2020, the unemployment rate gradually declined, eventually stabilizing by December 2021. Notably, this stabilization brings the unemployment rate close to pre-pandemic levels, indicating a gradual recovery in the labor market.\n\n\n\n5. Medical Corporation (Pfizer) Stock Price\n\n\nCode\n# Set options to suppress warnings\noptions(\"getSymbols.warning4.0\" = FALSE)\noptions(\"getSymbols.yahoo.warning\" = FALSE)\n\n# Define the tickers\ntickers &lt;- c(\"PFE\")\n\n# Loop through tickers to get stock data\nfor (ticker in tickers) {\n  getSymbols(ticker,\n             from = \"2020-01-01\",\n             to = \"2024-01-01\")\n}\n\n# Create a data frame with adjusted closing prices\nstock &lt;- data.frame(date = index(PFE), value = Ad(PFE))\n\n# Visualize the plot\ngg &lt;- ggplot(data = stock, aes(x = date, y = PFE.Adjusted)) +\n  geom_line(colour = \"#5a3196\") +\n  labs(x = \"Date\", y = \"Adjusted Price\", title = \"Trend of Pfizer Stock Price (2020-2024)\") +\n  theme_minimal() \n\nplotly_gg &lt;- ggplotly(gg)\nplotly_gg\n\n\n\n\n\n\nAnalyzing the fluctuations in Pfizer’s stock price, we observe a discernible pattern linked to the COVID-19 pandemic. Initially, during the onset of the pandemic, the stock price experienced a decline, reflecting the uncertainties surrounding the medical corporation’s operations amidst the global health crisis. However, a significant shift occurred post-August 2021, likely attributable to the widespread adoption of Pfizer’s COVID-19 vaccine. This surge in demand propelled the stock price to higher levels. Subsequently, starting March 2023, a noticeable downtrend emerges, possibly indicating waning interest and widespread adoption of vaccination. This sustained decline underscores a shift in market dynamics and investor sentiment towards Pfizer’s products, necessitating a deeper examination of the factors driving this trend.\n\n\n6. Party Support Rate\n\n\nCode\ndemo &lt;- read_excel('Datasets/party.xlsx',sheet = 'Democrat')\ninde &lt;- read_excel('Datasets/party.xlsx',sheet = 'Independent')\nrep &lt;- read_excel('Datasets/party.xlsx',sheet = 'Republican')\n\n# Transform the wide dataframe into a long dataframe\nkey_cols &lt;- c(\"Attitude\")\nvalue_cols &lt;- setdiff(names(demo), key_cols)\ndemo1 &lt;- pivot_longer(\n  demo,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"democrat\"\n)\n\ninde1 &lt;- pivot_longer(\n  inde,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"independent\"\n)\n\nrep1 &lt;- pivot_longer(\n  rep,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"republican\"\n)\n\n# Combine these three datasets together\ncombined_data &lt;- full_join(demo1, inde1, by = c(\"Time\", \"Attitude\")) %&gt;%\n  full_join(rep1, by = c(\"Time\", \"Attitude\"))\n\n# Convert date column to Date format\ncombined_data$Time &lt;- as.Date(combined_data$Time)\n\n# Visualize the plot\ngg &lt;- ggplot(combined_data, aes(x = Time, y = democrat, color = Attitude)) +\n  geom_line() +\n  labs(title = \"Attitudes on Democratic Over Time\", x = \"Time\", y = \"Percentage\") +\n  theme_minimal()\n\nplotly_gg &lt;- ggplotly(gg)\nplotly_gg\n\n\n\n\n\n\nUtilizing the Democratic attitude as a focal point for our visualization, we observe the evolving sentiments towards the Democratic party over the course of the pandemic. Upon analysis, the plot reveals nuanced fluctuations in attitudes over time. However, an overarching trend of significant increase or decrease in each attitude is not readily discernible from this visualization."
  },
  {
    "objectID": "other.html",
    "href": "other.html",
    "title": "Other: Interrupted TS/ARFIMA/Spectral Analysis",
    "section": "",
    "text": "Given the profound disruptions caused by the COVID-19 pandemic, which has significantly interrupted daily life and global events, employing an interrupted time series analysis is particularly apt. This method allows us to quantitatively assess the pandemic’s impact on key socio-economic indicators. In our project, we have chosen to focus on Pfizer stock prices and GDP per capita as two pivotal metrics. By analyzing these indicators, we aim to capture and understand the extent of COVID-19’s effects on both the corporate sector, specifically pharmaceuticals, and broader economic health. This approach will enable us to isolate the direct impacts of the pandemic from other variables and gain insights into the resilience and responsiveness of different economic sectors during such global crises. In these examples, we all selected 2020 February 20th as the treatment date to research.\n\n\n\n\nCode\ngdp &lt;- read_csv('Datasets/gdp.csv')\n\n# Convert DATE column from m/d/yy format to Date object and reformat to \"Year\" only for simplicity\ngdp$DATE &lt;- format(mdy(gdp$DATE), \"%Y/%m/%d\")\n\n# Convert GDP column to numeric (floating-point) format if not already\ngdp$GDP &lt;- as.numeric(gdp$GDP)\n\n# Visualize the plot\nstart_year &lt;- year(min(gdp$DATE))\nstart_month &lt;- month(min(gdp$DATE))\nend_year &lt;- year(max(gdp$DATE))\nend_month &lt;- month(max(gdp$DATE))\n\n# Calculate the number of observations from start to end\nnum_obs &lt;- (end_year - start_year) * 4 + ceiling((end_month - start_month) / 3)\n\n\n# data glimpse\nhead(gdp, 3)\n\n\n# A tibble: 3 × 2\n  DATE          GDP\n  &lt;chr&gt;       &lt;dbl&gt;\n1 2017/01/01 19280.\n2 2017/04/01 19439.\n3 2017/07/01 19693.\n\n\nCode\nfig &lt;- plot_ly(gdp, x = ~DATE, y = ~GDP,name = 'GDP Per Capita', type = 'scatter', mode = 'lines')\n\nfig\n\n\n\n\n\n\nCode\n# Create the time series object\n#gdp_ts &lt;- ts(gdp$GDP, start = c(start_year, start_month), frequency = 4, deltat = 0.25)\n\ndataTS &lt;-data.frame(\"Y\"=gdp$GDP)\ndataTS$Ts&lt;-seq(1:length(gdp$GDP))\ndataTS$D &lt;- ifelse(gdp$DATE &lt; as.Date(\"2020-02-20\"), 0, 1)\ndataTS$P&lt;- seq_along(gdp$DATE)\ndataTS$P[gdp$DATE &lt; as.Date(\"2020-02-20\")] &lt;- 0\n\nhead(dataTS)\n\n\n         Y Ts D P\n1 19280.08  1 0 0\n2 19438.64  2 0 0\n3 19692.60  3 0 0\n4 20037.09  4 0 0\n5 20328.55  5 0 0\n6 20580.91  6 0 0\n\n\nCode\nindex_of_2020_02_20 &lt;- which(gdp$DATE == as.Date(\"2020-04-01\"))\n#index_of_2020_02_20 #14\n\nplot( dataTS$Ts, dataTS$Y,\n      bty=\"n\", pch=19, col=\"gray\",\n      xlab = \"Time\", \n      ylab = \"GDP Per Capita\" )\n\n# Line marking the interruption\nabline( v=13, col=\"firebrick\", lty=2 )\ntext( 9, 24000, \"Start of COVID\", col=\"firebrick\", cex=1.3, pos=4 )\n\n# Add the regression line\nts &lt;- lm( Y ~ Ts + D + P, data=dataTS )\nlines( dataTS$Ts, ts$fitted.values, col=\"steelblue\", lwd=2 )\n\n\n\n\n\n\nPre-COVID Trend: Prior to the dashed red line labeled “Start of COVID,” there is a positive trend in GDP per capita, indicating economic growth. This is represented by the blue line, which is the fitted line from a regression model, showing the upward trajectory of GDP per capita over time.\nCOVID-19 Impact: The dashed red line signifies the point at which the COVID-19 pandemic began. Following this line, there’s a noticeable dip in GDP per capita, reflecting the immediate economic impact of the pandemic. This downturn signifies a break from the previous growth trend.\nPost-COVID Recovery: After the initial dip, the blue fitted line indicates that GDP per capita began to recover, continuing on an upward trajectory, although starting from a lower point than where the pre-COVID trend would have predicted.\nRecovery Trend: The post-COVID section of the blue line is steeper than the pre-COVID section, suggesting that the rate of growth in GDP per capita after the initial pandemic shock may be faster than the growth rate before the pandemic.\n\n\n\nCode\nstargazer( ts, \n           type = \"text\", \n           dep.var.labels = (\"GDP Per Capita\"),\n           column.labels = (\"Model results\"),\n           covariate.labels = c(\"Time\", \"Treatment\", \"Time Since Treatment\"),\n           omit.stat = \"all\", \n           digits = 2 )\n\n\n\n================================================\n                         Dependent variable:    \n                     ---------------------------\n                           GDP Per Capita       \n                            Model results       \n------------------------------------------------\nTime                          224.26***         \n                               (23.05)          \n                                                \nTreatment                   -5,566.75***        \n                              (438.41)          \n                                                \nTime Since Treatment          306.49***         \n                               (29.61)          \n                                                \nConstant                    19,112.23***        \n                              (182.95)          \n                                                \n================================================\n================================================\nNote:                *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nThe interrupted regression result presented describes the impact of the onset of the COVID-19 pandemic on GDP per capita over time. Here’s a detailed breakdown of the model’s components and the statistical outputs:\n\nTime (Ts): The coefficient of 224.26 (with a standard error of 23.05) and significance indicated by three asterisks (***), suggesting a p-value less than 0.01, indicates that for every unit increase in time, the GDP per capita increases by approximately 224.26 units, assuming no treatment and not considering the passage of time since the treatment. This shows a general trend of GDP growth over time under normal circumstances.\nTreatment (D): The treatment effect is -5,566.75 (with a standard error of 438.41), also highly significant (***). This suggests that the introduction of the treatment (the onset of the pandemic) leads to an immediate drop in GDP per capita by approximately 5,566.75 units. This captures the initial shock of the event on the economy.\nTime Since Treatment (P): The coefficient of 306.49 (with a standard error of 29.61) and significance (***), indicates that for each unit of time after the treatment, GDP per capita increases by about 306.49 units. This represents a recovery trajectory where, after the initial shock, GDP per capita begins to recover at this rate.\n\nIn summary, the model indicates that while the treatment had a significant negative impact on GDP per capita, there has been a subsequent positive rate of recovery over time following the initial shock. This provides a quantified insight into the economic impact of the event and its aftermath.\n\n\n\n\n\nCode\n# Set options to suppress warnings\noptions(\"getSymbols.warning4.0\" = FALSE)\noptions(\"getSymbols.yahoo.warning\" = FALSE)\n\n# Define the tickers\ntickers &lt;- c(\"PFE\")\n\n# Loop through tickers to get stock data\nfor (ticker in tickers) {\n  getSymbols(ticker,\n             from = \"2016-01-01\",\n             to = \"2024-01-01\")\n}\n\n# Create a data frame with adjusted closing prices\nstock &lt;- data.frame(date = index(PFE), value = Ad(PFE))\n\n# data glimpse\nhead(stock, 3)\n\n\n                 date PFE.Adjusted\n2016-01-04 2016-01-04     22.07899\n2016-01-05 2016-01-05     22.23794\n2016-01-06 2016-01-06     21.84404\n\n\nCode\nfig &lt;- plot_ly(stock, x = ~date, y = ~PFE.Adjusted,name = 'Pfizer Stock Price', type = 'scatter', mode = 'lines')\n\nfig\n\n\n\n\n\n\nCode\ndataTS &lt;-data.frame(\"Y\"=stock$PFE.Adjusted)\ndataTS$Ts&lt;-seq(1:length(stock$PFE.Adjusted))\ndataTS$D &lt;- ifelse(stock$date &lt; as.Date(\"2020-02-20\"), 0, 1)\ndataTS$P&lt;- seq_along(stock$date)\ndataTS$P[stock$date &lt; as.Date(\"2020-02-20\")] &lt;- 0\n\nhead(dataTS)\n\n\n         Y Ts D P\n1 22.07899  1 0 0\n2 22.23794  2 0 0\n3 21.84404  3 0 0\n4 21.69892  4 0 0\n5 21.42250  5 0 0\n6 21.47087  6 0 0\n\n\nCode\nindex_of_2020_02_20 &lt;- which(stock$date == as.Date(\"2020-02-20\"))\n#index_of_2020_02_20 #1040\n\nplot( dataTS$Ts, dataTS$Y,\n      bty=\"n\", pch=19, col=\"gray\",\n      xlab = \"Time (days)\", \n      ylab = \"Pfizer Stock Price\" )\n\n# Line marking the interruption\nabline( v=1039, col=\"firebrick\", lty=2 )\ntext( 700, 40, \"Start of COVID\", col=\"firebrick\", cex=1.3, pos=4 )\n\n# Add the regression line\nts &lt;- lm( Y ~ Ts + D + P, data=dataTS )\nlines( dataTS$Ts, ts$fitted.values, col=\"steelblue\", lwd=2 )\n\n\n\n\n\n\nTrend Before COVID-19: Before the dashed red vertical line that denotes the start of COVID-19, there is a blue line indicating a general upward trend in Pfizer’s stock price. This suggests that Pfizer’s stock was gradually increasing in value over time before the pandemic began.\nStart of COVID-19: The dashed red vertical line indicates the point at which the COVID-19 pandemic started. This is a crucial time marker for the analysis.\nTrend After COVID-19: After the start of COVID-19, the plot shows significant volatility in Pfizer’s stock price. The stock price initially follows the pre-pandemic trend and continues to increase but then shows some fluctuation before sharply rising. This sharp rise could be due to various factors, potentially including positive news about vaccine development or other pharmaceutical advances related to COVID-19 made by Pfizer.\nOverall Pattern: Towards the right end of the plot, after a peak, there is a noticeable decline in the stock price, which may indicate a period of correction or response to other market or global factors.\n\n\n\nCode\nstargazer( ts, \n           type = \"text\", \n           dep.var.labels = (\"Pfizer Stock Price\"),\n           column.labels = (\"Model results\"),\n           covariate.labels = c(\"Time\", \"Treatment\", \"Time Since Treatment\"),\n           omit.stat = \"all\", \n           digits = 2 )\n\n\n\n================================================\n                         Dependent variable:    \n                     ---------------------------\n                         Pfizer Stock Price     \n                            Model results       \n------------------------------------------------\nTime                           0.01***          \n                               (0.001)          \n                                                \nTreatment                      2.29**           \n                               (0.92)           \n                                                \nTime Since Treatment          -0.002***         \n                               (0.001)          \n                                                \nConstant                      21.56***          \n                               (0.31)           \n                                                \n================================================\n================================================\nNote:                *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nThe regression results presented show impact of the onset of the COVID-19 pandemic on the stock price of Pfizer. The regression model includes variables for time, treatment, and the time since treatment was introduced. Here’s a breakdown of each component of the output:\n\nTime (Ts): The coefficient for time is 0.01 with a p-value of less than 0.01 (indicated by ***), suggesting that it is highly statistically significant. This implies that, on average, the stock price of Pfizer has been increasing by 0.01 units each time unit regardless of other factors in the model.\nTreatment (D): The treatment variable has a coefficient of 2.29 with a p-value of less than 0.05 (**), indicating a statistically significant positive effect on the Pfizer stock price. This suggests that the onset of the COVID-19 pandemic led to an average increase in Pfizer’s stock price by 2.29 units at the time of the treatment’s implementation.\nTime Since Treatment (P): The coefficient for time since treatment is -0.002 with a p-value of less than 0.01 (***), which is highly significant. This indicates that following the treatment, the stock price decreases by an average of 0.002 units per time unit. This could suggest that the initial positive impact of the treatment diminishes over time.\n\nThe model suggests that the Pfizer stock price has a general upward trend over time. There was a significant increase in the stock price at the time of treatment, which means the investment of COVID-19 vaccinates could be helpful for the stock price during some periods of COVID-19. However, the effect of COVID-19 diminishes slightly over time, indicating that while the intervention had a positive immediate impact, its effect lessens as time progresses."
  },
  {
    "objectID": "FTS.html",
    "href": "FTS.html",
    "title": "FTS Models",
    "section": "",
    "text": "In this section, I will analyze financial time series data related to the COVID-19 pandemic. The time series data will consist of stocks from companies directly involved with COVID-19 or its vaccination efforts. Specifically, I will focus on Pfizer (PFE) and BioNTech SE (BNTX) due to their significant roles in developing and distributing COVID-19 vaccines.\n\n\nPfizer, a longstanding giant in the pharmaceutical industry, played a pivotal role during the COVID-19 pandemic through the development and distribution of a highly effective vaccine. Partnering with BioNTech, a German biotechnology company, Pfizer developed one of the first mRNA-based vaccines, which was granted emergency use authorization by the FDA in December 2020. This vaccine, branded as Comirnaty, showcased a new approach in vaccine technology with remarkable efficacy rates above 90% in preventing COVID-19 infection. Pfizer’s vaccine not only marked a significant scientific milestone but also became a critical tool in global efforts to combat the pandemic, leading to substantial impacts on its stock valuation.\n\n\n\nBioNTech SE, while not as widely recognized globally as Pfizer before the pandemic, quickly became a household name due to its collaboration with Pfizer in developing the Comirnaty COVID-19 vaccine. Specializing in mRNA technology, BioNTech’s innovative approach was crucial in the rapid development and success of the vaccine. The company, founded in 2008, focused on patient-specific immunotherapies for cancer before pivoting to infectious diseases, which positioned it uniquely for the swift response to COVID-19. The collaboration with Pfizer not only accelerated the vaccine’s development and distribution but also significantly enhanced BioNTech’s market profile and stock performance.\nTo conduct the time series analysis of the financial data from Pfizer and BioNTech, I will employ ARCH (Autoregressive Conditional Heteroskedasticity) and GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models. These models are well-suited for studying financial market data as they effectively model the changing volatility, a common characteristic of stock return data. By analyzing the volatility of the returns for both PFE and BNTX stocks, we can gain insights into how market perceptions of these companies’ roles in addressing the COVID-19 crisis have influenced their stock prices over time."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "1. Time-series Plots\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID Hospitalization NumberEconomic Indicators (GDP Per Capita & Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\n# Read the dataset\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million', 'people_vaccinated_per_hundred', 'people_fully_vaccinated_per_hundred')\nt &lt;- vac_df[, cols_show]\n\n# Group by 'date' and summarize columns, ignoring NA values\nt1 &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    daily_vaccinations_per_million = sum(daily_vaccinations_per_million, na.rm = TRUE),\n    people_vaccinated_per_hundred = mean(people_vaccinated_per_hundred, na.rm = TRUE),\n    people_fully_vaccinated_per_hundred = mean(people_fully_vaccinated_per_hundred, na.rm = TRUE)\n  )\n\n# Convert date column to Date format\nt1$date &lt;- as.Date(t1$date)\n\n# Aggregate data to monthly level using mean for each column\nt1 &lt;- t1 %&gt;%\n  group_by(date = format(date, \"%Y-%m\")) %&gt;%\n  summarize(daily_vaccinations_per_million = mean(daily_vaccinations_per_million, na.rm = TRUE),\n            people_vaccinated_per_hundred = mean(people_vaccinated_per_hundred, na.rm = TRUE),\n            people_fully_vaccinated_per_hundred = mean(people_fully_vaccinated_per_hundred, na.rm = TRUE))\n\n# Transform the date column type with specified format\nt1$date &lt;- as.Date(paste0(t1$date, \"-01-01\"))\n\n# Visualize the plot\nd_vacc_ts &lt;- ts(t1$daily_vaccinations_per_million, start = c(year(min(t1$date)), month(min(t1$date))), end = c(year(max(t1$date)), month(max(t1$date))), frequency = 12)\npe&lt;-autoplot(d_vacc_ts, xlab = \"Time\", ylab = \"Daily Vaccinations per Million\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Daily Vaccinations per Million in the US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\nCode\n# Visualize the plot\np_vacc_ts &lt;- ts(t1$people_vaccinated_per_hundred, start = c(year(min(t1$date)), month(min(t1$date))), end = c(year(max(t1$date)), month(max(t1$date))), frequency = 12)\npe&lt;-autoplot(p_vacc_ts, xlab = \"Time\", ylab = \"People Vaccinated per Hundred\", colour = \"#5a3196\")+ggtitle('Time Series Plot of People Vaccinated per Hundred in the US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\nCode\n# Visualize the plot\npf_vacc_ts &lt;- ts(t1$people_fully_vaccinated_per_hundred, start = c(year(min(t1$date)), month(min(t1$date))), end = c(year(max(t1$date)), month(max(t1$date))), frequency = 12)\npe&lt;-autoplot(pf_vacc_ts, xlab = \"Time\", ylab = \"People Fully Vaccinated per Hundred\", colour = \"#5a3196\")+ggtitle('Time Series Plot of People Fully Vaccinated per Hundred in the US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\n\n\n\n\nCode\n# Newly confirmed cases\nwide_data &lt;- read_csv(\"Datasets/covid_confirmed_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ncon_case_df &lt;- long_data %&gt;%\n  group_by(date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Convert date column to Date format\ncon_case_df$date &lt;- as.Date(con_case_df$date)\n\n# Aggregate data to monthly level using mean for each column\ncon_case_df &lt;- con_case_df %&gt;%\n  group_by(date = format(date, \"%Y-%m\")) %&gt;%\n  summarize(value_sum = mean(value_sum, na.rm = TRUE))\n\n# Transform the date column type with specified format\ncon_case_df$date &lt;- as.Date(paste0(con_case_df$date, \"-01-01\"))\n\n# Visualize the plot\ncase_ts &lt;- ts(con_case_df$value_sum, start = c(year(min(con_case_df$date)), month(min(con_case_df$date))), end = c(year(max(con_case_df$date)), month(max(con_case_df$date))), frequency = 12)\npe&lt;-autoplot(case_ts, xlab = \"Time\", ylab = \"Newly Confirmed Cases\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Newly Confirmed COVID Cases in the US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\nCode\n# Death cases\nwide_data &lt;- read_csv(\"Datasets/covid_deaths_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ndead_case_df &lt;- long_data %&gt;%\n  group_by(date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Convert date column to Date format\ndead_case_df$date &lt;- as.Date(dead_case_df$date)\n\n# Aggregate data to monthly level using mean for each column\ndead_case_df &lt;- dead_case_df %&gt;%\n  group_by(date = format(date, \"%Y-%m\")) %&gt;%\n  summarize(value_sum = mean(value_sum, na.rm = TRUE))\n\n# Transform the date column type with specified format\ndead_case_df$date &lt;- as.Date(paste0(dead_case_df$date, \"-01-01\"))\n\n# Visualize the plot\ndead_ts &lt;- ts(dead_case_df$value_sum, start = c(year(min(dead_case_df$date)), month(min(dead_case_df$date))), end = c(year(max(dead_case_df$date)), month(max(dead_case_df$date))), frequency = 12)\npe&lt;-autoplot(dead_ts, xlab = \"Time\", ylab = \"Dead Cases\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Dead COVID Cases in the US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\n\n\n\n\nCode\nhos_df &lt;- read_csv('Datasets/COVID-19_hos.csv')\n\n# data glimpse\ncols_show &lt;- c('state', 'date', 'inpatient_beds', 'inpatient_beds_used_covid', 'inpatient_bed_covid_utilization')\nt &lt;- hos_df[, cols_show]\n\n# Group by 'date', and calculate the sum of Confirmed Cases\nhos &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(value_sum1 = sum(inpatient_beds, na.rm = TRUE),\n            value_sum2 = sum(inpatient_beds_used_covid, na.rm = TRUE),\n            value_sum3 = mean(inpatient_bed_covid_utilization, na.rm = TRUE))\n\n# Convert date column to Date format\nhos$date &lt;- as.Date(hos$date)\n\n# Aggregate data to monthly level using mean for each column\nhos &lt;- hos %&gt;%\n  group_by(date = format(date, \"%Y-%m\")) %&gt;%\n  summarize(value_sum1 = mean(value_sum1, na.rm = TRUE),\n            value_sum2 = mean(value_sum2, na.rm = TRUE),\n            value_sum3 = mean(value_sum3, na.rm = TRUE))\n\n# Transform the date column type with specified format\nhos$date &lt;- as.Date(paste0(hos$date, \"-01-01\"))\n\n# Visualize the plot\nhos_ts1 &lt;- ts(hos$value_sum1, start = c(year(min(hos$date)), month(min(hos$date))), end = c(year(max(hos$date)), month(max(hos$date))), frequency = 12)\npe&lt;-autoplot(hos_ts1, xlab = \"Time\", ylab = \"Number of Inpatient Beds\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Number of Inpatient Beds in the US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\nCode\n# Visualize the plot\nhos_ts2 &lt;- ts(hos$value_sum2, start = c(year(min(hos$date)), month(min(hos$date))), end = c(year(max(hos$date)), month(max(hos$date))), frequency = 12)\npe&lt;-autoplot(hos_ts2, xlab = \"Time\", ylab = \"Number of Inpatient Beds Used for COVID\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Number of Inpatient Beds Used for COVID in the US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\nCode\n# Visualize the plot\nhos_ts3 &lt;- ts(hos$value_sum3, start = c(year(min(hos$date)), month(min(hos$date))), end = c(year(max(hos$date)), month(max(hos$date))), frequency = 12)\npe&lt;-autoplot(hos_ts3, xlab = \"Time\", ylab = \"Utilization Rate of Inpatient Beds for COVID\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Utilization Rate of Inpatient Beds for COVID in the US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\n\n\n\n\nCode\ngdp &lt;- read_csv('Datasets/gdp.csv')\n\n# Convert DATE column from m/d/yy format to Date object and reformat to \"Year\" only for simplicity\ngdp$DATE &lt;- format(mdy(gdp$DATE), \"%Y/%m/%d\")\n\n# Convert GDP column to numeric (floating-point) format if not already\ngdp$GDP &lt;- as.numeric(gdp$GDP)\n\n# Visualize the plot\nstart_year &lt;- year(min(gdp$DATE))\nstart_month &lt;- month(min(gdp$DATE))\nend_year &lt;- year(max(gdp$DATE))\nend_month &lt;- month(max(gdp$DATE))\n\n# Calculate the number of observations from start to end\nnum_obs &lt;- (end_year - start_year) * 4 + ceiling((end_month - start_month) / 3)\n\n# Create the time series object\ngdp_ts &lt;- ts(gdp$GDP, start = c(start_year, start_month), frequency = 4, deltat = 0.25)\n\npe&lt;-autoplot(gdp_ts, xlab = \"Time\", ylab = \"GDP Per Capita\", colour = \"#5a3196\")+ggtitle('Time Series Plot of GDP Per Capita by Year in US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\nCode\nunemp &lt;- read_csv('Datasets/unemployment.csv')\nkey_cols &lt;- c(\"Location\")\nvalue_cols &lt;- setdiff(names(unemp), key_cols)\nunemp1 &lt;- pivot_longer(\n  unemp,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"Unemployment\"\n)\n\n# Convert Time column to Date format\nunemp1$Time &lt;- as.Date(paste0(unemp1$Time, \"-01\"))\n\n# Convert Unemployment column to numeric (floating-point) format\nunemp1$Unemployment &lt;- as.numeric(unemp1$Unemployment)\n\n# Focus on US\nunemp2 &lt;- unemp1[unemp1$Location =='United States',]\n\n# Visualize the plot\nemployees_ts &lt;- ts(unemp2$Unemployment, start = c(year(min(unemp2$Time)), month(min(unemp2$Time))), end = c(year(max(unemp2$Time)), month(max(unemp2$Time))), frequency = 12)\npe&lt;-autoplot(employees_ts, xlab = \"Time\", ylab = \"Unemployment Rate\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Unemployment Rate by Month in US')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\n\n\n\n\nCode\n# Set options to suppress warnings\noptions(\"getSymbols.warning4.0\" = FALSE)\noptions(\"getSymbols.yahoo.warning\" = FALSE)\n\n# Define the tickers\ntickers &lt;- c(\"PFE\")\n\n# Loop through tickers to get stock data\nfor (ticker in tickers) {\n  getSymbols(ticker,\n             from = \"2020-01-01\",\n             to = \"2024-01-01\")\n}\n\n# Create a data frame with adjusted closing prices\nstock &lt;- data.frame(date = index(PFE), value = Ad(PFE))\n\n# Visualize the plot\nstock_ts &lt;- ts(stock$PFE.Adjusted, start = c(year(min(stock$date)), month(min(stock$date))), end = c(year(max(stock$date)), month(max(stock$date))), frequency = 12)\npe&lt;-autoplot(stock_ts, xlab = \"Time\", ylab = \"Adjusted Price\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Pfizer Stock Price (2020-2024)')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\n\n\n\n\nCode\ndemo &lt;- read_excel('Datasets/party.xlsx',sheet = 'Democrat')\ninde &lt;- read_excel('Datasets/party.xlsx',sheet = 'Independent')\nrep &lt;- read_excel('Datasets/party.xlsx',sheet = 'Republican')\n\n# Transform the wide dataframe into a long dataframe\nkey_cols &lt;- c(\"Attitude\")\nvalue_cols &lt;- setdiff(names(demo), key_cols)\ndemo1 &lt;- pivot_longer(\n  demo,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"democrat\"\n)\n\ninde1 &lt;- pivot_longer(\n  inde,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"independent\"\n)\n\nrep1 &lt;- pivot_longer(\n  rep,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"republican\"\n)\n\n# Combine these three datasets together\ncombined_data &lt;- full_join(demo1, inde1, by = c(\"Time\", \"Attitude\")) %&gt;%\n  full_join(rep1, by = c(\"Time\", \"Attitude\"))\ncombined_data1 &lt;- combined_data[combined_data$Attitude=='Favorable',]\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"Attitude\", \"Time\")\nvalue_cols &lt;- setdiff(names(combined_data1), key_cols)\n\n# Pivot the data from wide to long\ncombined_data2 &lt;- pivot_longer(\n  combined_data1,\n  cols = value_cols,\n  names_to = \"Party\",\n  values_to = \"value\"\n)\n\n# Convert date column to Date format\ncombined_data2$Time &lt;- as.Date(combined_data2$Time)\n\n# Subset to each party\ndemo_data &lt;- combined_data2[combined_data2$Party=='democrat',]\ninde_data &lt;- combined_data2[combined_data2$Party=='independent',]\nrep_data &lt;- combined_data2[combined_data2$Party=='republican',]\n\n# Visualize the plot\ngg &lt;- ggplot(combined_data2, aes(x = Time, y = value, color = Party)) +\n  geom_line() +\n  labs(title = \"Support Rate for Different Paties Over Time\", x = \"Time\", y = \"Percentage\") +\n  theme_minimal()\n\nplotly_gg &lt;- ggplotly(gg)\nplotly_gg\n\n\n\n\n\n\nCode\n# Visualize the plot\ndemo_ts &lt;- ts(demo_data$value, start = c(year(min(demo_data$Time)), month(min(demo_data$Time))), end = c(year(max(demo_data$Time)), month(max(demo_data$Time))), frequency = 12)\npe&lt;-autoplot(demo_ts, xlab = \"Time\", ylab = \"Percentage\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Support Rate for Democratic')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\nCode\n# Visualize the plot\ninde_ts &lt;- ts(inde_data$value, start = c(year(min(inde_data$Time)), month(min(inde_data$Time))), end = c(year(max(inde_data$Time)), month(max(inde_data$Time))), frequency = 12)\npe&lt;-autoplot(inde_ts, xlab = \"Time\", ylab = \"Percentage\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Support Rate for Independent')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\nCode\n# Visualize the plot\nrep_ts &lt;- ts(rep_data$value, start = c(year(min(rep_data$Time)), month(min(rep_data$Time))), end = c(year(max(rep_data$Time)), month(max(rep_data$Time))), frequency = 12)\npe&lt;-autoplot(rep_ts, xlab = \"Time\", ylab = \"Percentage\", colour = \"#5a3196\")+ggtitle('Time Series Plot of Support Rate for Republican')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\n\n\n\nIf the time series is additive, it follows this equation: \\(Series = Trend + Seasonal + Random\\)\nIf the time series is multiplicative, it follows this equation: \\(Series = Trend * Seasonal * Random\\)\nNumber of Daily Vaccinations Per Million: This metrics reflects the dynamic nature of our battle against the virus, with fluctuations influenced primarily by the evolving strains and their virulence. Noteworthy peaks occurred in April and October of 2021, possibly corresponding to strategic shifts in vaccination drives or response measures. In terms of seasonality, there is no clear sign of seasonality being present, therefore, we can conclude that there isn’t seasonality in the series based on unperceptive pattern. Moreover, the series is multiplicative.\nNumber of People Vaccinated Per Hundred: This metrics reveals a consistent upward trend over time. Notably, from January 2021 to July 2021, the rate of increase surged significantly compared to the period after July 2021, suggesting a pronounced acceleration in vaccination efforts during that timeframe and indicating a potential decline in the virus’s threat level. Seasonality analysis yields no discernible patterns, as vaccination rates are driven by individual choices rather than seasonal influences. Thus, the absence of any seasonal trends is evident. Moreover, the series is multiplicative.\nNumber of People Fully Vaccinated Per Hundred: This metrics is similar to the trend of the number of people vaccinated per hundred, which also reveals a consistent upward trend over time. Notably, from January 2021 to July 2021, the rate of increase surged significantly compared to the period after July 2021, suggesting a pronounced acceleration in vaccination efforts during that timeframe and indicating a potential decline in the virus’s threat level. Seasonality analysis yields no discernible patterns, as vaccination rates are driven by individual choices rather than seasonal influences. Thus, the absence of any seasonal trends is evident. Moreover, the series is multiplicative.\nNumber of Newly Confirmed Cases: This metric demonstrates a consistent upward trend over time, albeit with varying rates of increase observed across different periods. Notably, from October 2021 to February 2022, there was a notable acceleration in the rise of newly confirmed COVID-19 cases. In contrast, during other time periods, the pace of increase remained relatively steady. Despite thorough analysis, no discernible seasonal patterns emerge from this time series, suggesting that the incidence of confirmed cases is not influenced by seasonal factors. Moreover, the series is multiplicative.\nNumber of Death Cases: The trajectory of death cases is similar to that of newly confirmed cases, depicting an overall upward trend. Noteworthy spikes in the rate of fatalities occurred during specific intervals, especially during October 2020 to April 2021 and October 2021 to February 2022, suggesting heightened mortality rates during these periods. Despite analyzing seasonality, no discernible patterns emerge, indicating that mortality trends are not influenced by seasonal variations. Moreover, the series is multiplicative.\nNumber of Inpatient Beds: Initially, this series exhibited a sharp increase during the early stages of the COVID-19 pandemic. And starting from October 2020, the number of inpatient beds stabilized and reached a much higher number. The number occasionally fluctuated within a certain range after October 2020. Analysis of seasonality reveals no discernible patterns, indicating the absence of any seasonal influences on the series. Moreover, the series is multiplicative.\nNumber of Inpatient Beds Used for COVID: This metric exhibits significant fluctuations over time, with notable peaks observed in periods such as January 2021, September 2021, and January 2022, and nowadays the number of inpatient beds used for COVID became much lower. These peaks are not attributable to seasonal variations but rather correlate with surges in COVID-19 cases, often driven by the emergence of new virus variants. Consequently, the absence of any discernible seasonal patterns is evident in this time series. Moreover, the series is multiplicative.\nUtilization Rate for Inpatient Beds Used for COVID: This metric mirrors the fluctuations observed in the number of inpatient beds occupied by COVID patients, with significant peaks evident during key periods such as January 2021, September 2021, and January 2022. Notably, recent data reflects a marked decline in COVID-related bed occupancy, stabilizing at approximately 5%. These fluctuations are not merely a result of seasonal variations but rather closely align with spikes in COVID-19 cases, often coinciding with the emergence of new virus variants. As such, the absence of discernible seasonal patterns underscores the dynamic nature of this metric, dictated by the evolving landscape of the pandemic. Moreover, the series is multiplicative.\nGDP Per Capita: The trajectory of GDP per capita demonstrates a consistent upward trend, indicating economic growth and prosperity over time. Notably, this trend is not subject to seasonal fluctuations, as it is inherently tied to annual economic performance rather than periodic factors. Moreover, the series is multiplicative.\nUnemployment Rate: The unemployment time series witnessed a sharp escalation in the early stages of the COVID-19 pandemic, particularly evident in March, as businesses grappled with widespread closures and economic uncertainty. However, in response to this crisis, governments worldwide swiftly implemented policies aimed at stimulating job creation and fostering economic recovery. Consequently, the unemployment rate began a gradual descent post-March 2020, signaling a positive trajectory towards stabilizing the job market. Presently, the unemployment rate hovers around 4%, indicative of a more stabilized employment landscape. Notably, similar to other pandemic-related metrics, the unemployment rate displays minimal seasonality, as its fluctuations are primarily driven by the virus’s impact on economic activity rather than seasonal patterns. Moreover, the series is multiplicative.\nPfizer Stock Price: The Pfizer stock price has exhibited significant fluctuations, with notable peaks occurring in October 2020 and February 2021. These spikes can largely be attributed to the successful development and widespread adoption of booster vaccinations during critical phases of the COVID-19 pandemic. The surge in stock value during these periods reflects investor confidence in Pfizer’s pivotal role in combating the virus. However, following March 2023, a discernible downward trajectory in stock price emerged, signaling a broader trend of decline within the medical sector during the post-pandemic era. Despite these fluctuations, the stock price demonstrates no clear seasonality, as its movements are intricately linked to the ebb and flow of the COVID-19 virus itself. Moreover, the series is multiplicative.\nParty Support Rate: The party support rate exhibits continuous fluctuations, reflecting the dynamic nature of political sentiment. However, a prevailing trend emerges: the support rate for the Democratic party consistently surpasses that of independent and Republican factions. This enduring pattern underscores the enduring appeal and resonance of Democratic ideals among the populace. The absence of discernible seasonality in these fluctuations underscores that the shifts are driven primarily by the ever-evolving opinions and perceptions of the public rather than external factors tied to specific seasons or events. Party support rates in the US can vary significantly over time due to various factors such as political events, policy decisions, economic conditions, and societal changes. Moreover, the series is multiplicative.\n\n\n2. Lag Plots\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID-19 Hospitalization NumberEconomic Indicators (Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\ngglagplot(d_vacc_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Daily Vaccinations per Million\")+ggtitle(\"Lag Plot for Daily Vaccinations per Million\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nCode\ngglagplot(p_vacc_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"People Vaccinated Per Hundred\")+ggtitle(\"Lag Plot for People Vaccinated Per Hundred\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nCode\ngglagplot(pf_vacc_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"People Fully Vaccinated Per Hundred\")+ggtitle(\"Lag Plot for People Fully Vaccinated Per Hundred\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(case_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Newly Confirmed Cases\")+ggtitle(\"Lag Plot for Newly Confirmed Cases\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nCode\ngglagplot(dead_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Dead Cases\")+ggtitle(\"Lag Plot for Dead Cases\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(hos_ts1, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Number of Inpatient Beds\")+ggtitle(\"Lag Plot for Number of Inpatient Beds\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nCode\ngglagplot(hos_ts2, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Number of Inpatient Beds Used for COVID\")+ggtitle(\"Lag Plot for Number of Inpatient Beds Used for COVID\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nCode\ngglagplot(hos_ts3, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Utilization Rate of Inpatient Beds for COVID\")+ggtitle(\"Lag Plot for Utilization Rate of Inpatient Beds for COVID\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(employees_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Unemployment Rate\")+ggtitle(\"Lag Plot for Unemployment Rate\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(stock_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Adjusted Price\")+ggtitle(\"Lag Plot for Adjusted Price  of Pfizer\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(demo_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Percentage\")+ggtitle(\"Lag Plot for Support Rate for Democratic\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nCode\ngglagplot(inde_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Percentage\")+ggtitle(\"Lag Plot for Support Rate for Independent\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nCode\ngglagplot(rep_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Percentage\")+ggtitle(\"Lag Plot for Support Rate for Republican\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\n\n\n\nNumber of Daily Vaccinations Per Million: We observe a pronounced positive autocorrelation in the initial lag (lag 1), gradually diminishing in strength as we progress through subsequent lags, until reaching a point where no discernible autocorrelation exists, or where any remaining autocorrelation is exceedingly weak, particularly evident in lags 10 through 16.\nNumber of People Vaccinated Per Hundred: We observe a pronounced positive autocorrelation in the first 3 lags, gradually diminishing in strength as we progress through subsequent lags, until reaching a point where no discernible autocorrelation exists, or where any remaining autocorrelation is exceedingly weak, particularly evident in lags 7 through 16.\nNumber of People Fully Vaccinated Per Hundred: We observe a pronounced positive autocorrelation in the first 3 lags, gradually diminishing in strength as we progress through subsequent lags, until reaching a point where no discernible autocorrelation exists, or where any remaining autocorrelation is exceedingly weak, particularly evident in lags 7 through 16.\nNumber of Newly Confirmed Cases: We observe a pronounced positive autocorrelation in the first 2 lags, gradually diminishing in strength as we progress through subsequent lags, until reaching a point where no discernible autocorrelation exists, or where any remaining autocorrelation is exceedingly weak, particularly evident in lags 13 through 16.\nNumber of Death Cases: We observe a pronounced positive autocorrelation in the first 3 lags, gradually diminishing in strength as we progress through subsequent lags, until reaching a point where no discernible autocorrelation exists, or where any remaining autocorrelation is exceedingly weak, particularly evident in lags 12 through 16.\nNumber of Inpatient Beds: We observe a positive autocorrelation within the first two lags, followed by a cessation of autocorrelation or the presence of exceedingly faint positive autocorrelation in subsequent lags.\nNumber of Inpatient Beds Used for COVID: We observe a positive autocorrelation within the initial lag (lag 1), followed by a cessation of autocorrelation or the presence of exceedingly faint positive autocorrelation in subsequent lags.\nUtilization Rate for Inpatient Beds Used for COVID: We observe a positive autocorrelation within the initial lag (lag 1), followed by a cessation of autocorrelation or the presence of exceedingly faint positive autocorrelation in subsequent lags.\nUnemployment Rate: We observe a positive autocorrelation within the first two lags, followed by a cessation of autocorrelation or the presence of exceedingly faint positive autocorrelation in subsequent lags.\nPfizer Stock Price:  We observe a pronounced positive autocorrelation in the first 3 lags, gradually diminishing in strength as we progress through subsequent lags, until reaching a point where no discernible autocorrelation exists, or where any remaining autocorrelation is exceedingly weak, particularly evident in lags 9 through 16.\nSupport Rate for Democratic: We observe there is no significant autocorrelation across all lags. This absence suggests that there is no systematic relationship between the support rate for the Democratic party at one point in time and its support rate at subsequent points. In other words, fluctuations in support for the Democratic party appear to occur independently over time, without any discernible pattern of correlation or dependency between successive observations. This finding implies a lack of persistent trends or cyclical patterns in the support rate for the Democratic party, reflecting the stochastic nature of political opinion dynamics.\nSupport Rate for Independent: We observe there is no significant autocorrelation across all lags. This absence suggests that there is no systematic relationship between the support rate for the Independent party at one point in time and its support rate at subsequent points. In other words, fluctuations in support for the Independent party appear to occur independently over time, without any discernible pattern of correlation or dependency between successive observations. This finding implies a lack of persistent trends or cyclical patterns in the support rate for the Independent party, reflecting the stochastic nature of political opinion dynamics.\nSupport Rate for Republican: We observe there is no significant autocorrelation across all lags. This absence suggests that there is no systematic relationship between the support rate for the Republican party at one point in time and its support rate at subsequent points. In other words, fluctuations in support for the Republican party appear to occur independently over time, without any discernible pattern of correlation or dependency between successive observations. This finding implies a lack of persistent trends or cyclical patterns in the support rate for the Republican party, reflecting the stochastic nature of political opinion dynamics.\n\n\n3. Decomposition\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID-19 Hospitalization NumberEconomic Indicators (Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\ndecomposed &lt;- decompose(d_vacc_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Daily Vaccinations per Million\")+theme_bw()\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(p_vacc_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For People Vaccinated per Hundred\")+theme_bw()\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(pf_vacc_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For People Fully Vaccinated per Hundred\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(case_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Newly Confirmed Cases\")+theme_bw()\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(dead_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Dead Cases\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(hos_ts1, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Number of Inpatient Beds\")+theme_bw()\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(hos_ts2, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Number of Inpatient Beds Used for COVID\")+theme_bw()\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(hos_ts3, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Utilization Rate of Inpatient Beds for COVID\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(employees_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Unemployment Rate\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(stock_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Pfizer Stock Price\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(demo_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Support Rate for Democratic\")+theme_bw()\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(inde_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Support Rate for Independent\")+theme_bw()\n\n\n\n\n\nCode\ndecomposed &lt;- decompose(rep_ts, \"multiplicative\")\nautoplot(decomposed, colour = \"#5a3196\", main = \"Decomposition Plot For Support Rate for Republican\")+theme_bw()\n\n\n\n\n\n\n\n\nThe decomposition plots generated for each data series further substantiate the observations highlighted in Part 1 - Time Series Plots. Through meticulous analysis, these plots unravel the underlying patterns within our datasets, lending robust support to our initial claims. By dissecting the components of the time series—trend, seasonality, and residual—these visual representations not only corroborate our earlier findings but also enhance our understanding of the dynamics at play. This congruence between the decomposition plots and the initial time series observations serves as a compelling validation of our analytical framework, reinforcing the insights derived from our preliminary exploration.\n\n\n4. ACF & PACF Plots\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID-19 Hospitalization NumberEconomic Indicators (Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\nd_vacc_acf &lt;- ggAcf(d_vacc_ts)+ggtitle(\"ACF Plot for Daily Vaccinations per Million\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nd_vacc_pacf &lt;- ggPacf(d_vacc_ts)+ggtitle(\"PACF Plot for Daily Vaccinations per Million\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(d_vacc_acf, d_vacc_pacf, nrow=2)\n\n\n\n\n\nCode\np_vacc_acf &lt;- ggAcf(p_vacc_ts)+ggtitle(\"ACF Plot for People Vaccinated per Hundred\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \np_vacc_pacf &lt;- ggPacf(p_vacc_ts)+ggtitle(\"PACF Plot for People Vaccinated per Hundred\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(p_vacc_acf, p_vacc_pacf, nrow=2)\n\n\n\n\n\nCode\npf_vacc_acf &lt;- ggAcf(pf_vacc_ts)+ggtitle(\"ACF Plot for People Fully Vaccinated per Hundred\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \npf_vacc_pacf &lt;- ggPacf(pf_vacc_ts)+ggtitle(\"PACF Plot for People Fully Vaccinated per Hundred\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(pf_vacc_acf, pf_vacc_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\ncase_acf &lt;- ggAcf(case_ts)+ggtitle(\"ACF Plot for Newly Confirmed Cases\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ncase_pacf &lt;- ggPacf(case_ts)+ggtitle(\"PACF Plot for Newly Confirmed Cases\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(case_acf, case_pacf, nrow=2)\n\n\n\n\n\nCode\ndead_acf &lt;- ggAcf(dead_ts)+ggtitle(\"ACF Plot for Dead Cases\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ndead_pacf &lt;- ggPacf(dead_ts)+ggtitle(\"PACF Plot for Dead Cases\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(dead_acf, dead_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nhos1_acf &lt;- ggAcf(hos_ts1)+ggtitle(\"ACF Plot for Number of Inpatient Beds\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos1_pacf &lt;- ggPacf(hos_ts1)+ggtitle(\"PACF Plot for Number of Inpatient Beds\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(hos1_acf, hos1_pacf, nrow=2)\n\n\n\n\n\nCode\nhos2_acf &lt;- ggAcf(hos_ts2)+ggtitle(\"ACF Plot for Number of Inpatient Beds Used for COVID\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos2_pacf &lt;- ggPacf(hos_ts2)+ggtitle(\"PACF Plot for Number of Inpatient Beds Used for COVID\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(hos2_acf, hos2_pacf, nrow=2)\n\n\n\n\n\nCode\nhos3_acf &lt;- ggAcf(hos_ts3)+ggtitle(\"ACF Plot for Utilization Rate of Inpatient Beds for COVID\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos3_pacf &lt;- ggPacf(hos_ts3)+ggtitle(\"PACF Plot for Utilization Rate of Inpatient Beds for COVID\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(hos3_acf, hos3_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nemp_acf &lt;- ggAcf(employees_ts)+ggtitle(\"ACF Plot for Unemployment Rate\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nemp_pacf &lt;- ggPacf(employees_ts)+ggtitle(\"PACF Plot for Unemployment Rate\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(emp_acf, emp_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nstock_acf &lt;- ggAcf(stock_ts)+ggtitle(\"ACF Plot for Pfizer Stock Price\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nstock_pacf &lt;- ggPacf(stock_ts)+ggtitle(\"PACF Plot for Pfizer Stock Price\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(stock_acf, stock_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\ndemo_acf &lt;- ggAcf(demo_ts)+ggtitle(\"ACF Plot for Support Rate for Democratic\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ndemo_pacf &lt;- ggPacf(demo_ts)+ggtitle(\"PACF Plot for Support Rate for Democratic\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(demo_acf, demo_pacf, nrow=2)\n\n\n\n\n\nCode\ninde_acf &lt;- ggAcf(inde_ts)+ggtitle(\"ACF Plot for Support Rate for Independent\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ninde_pacf &lt;- ggPacf(inde_ts)+ggtitle(\"PACF Plot for Support Rate for Independent\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(inde_acf, inde_pacf, nrow=2)\n\n\n\n\n\nCode\nrep_acf &lt;- ggAcf(rep_ts)+ggtitle(\"ACF Plot for Support Rate for Republican\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nrep_pacf &lt;- ggPacf(rep_ts)+ggtitle(\"PACF Plot for Support Rate for Republican\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(rep_acf, rep_pacf, nrow=2)\n\n\n\n\n\n\n\n\nNumber of Daily Vaccinations Per Million: ACF Plot has significant lags at 1 and 2 so p = 1, 2. PACF Plot has significant lags at 1 and 2 so q = 1, 2. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nNumber of People Vaccinated Per Hundred: ACF Plot has significant lags at 1-3 so p = 1, 2, 3. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nNumber of People Fully Vaccinated Per Hundred: ACF Plot has significant lags at 1-3 so p = 1, 2, 3. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nNumber of Newly Confirmed Cases: ACF Plot has significant lags at 1-10 so p = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. However, in general we care about only the first couple of lags, in this case the first 3. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nNumber of Death Cases: ACF Plot has significant lags at 1-10 so p = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. However, in general we care about only the first couple of lags, in this case the first 3. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nNumber of Inpatient Beds: ACF Plot has significant lags at 1 and 2 so p = 1, 2. PACF Plot has significant lags at 1 and 4 so q = 1, 4. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nNumber of Inpatient Beds Used for COVID: ACF Plot has significant lags at 1 and 2 so p = 1, 2. PACF Plot has significant lags at 1 and 2 so q = 1, 2. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nUtilization Rate for Inpatient Beds Used for COVID: ACF Plot has significant lags at 1 so p = 1. PACF Plot has significant lags at 1-3 so q = 1, 2, 3. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nUnemployment Rate: ACF Plot has significant lags at 1 so p = 1. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nPfizer Stock Price:  ACF Plot has significant lags at 1-10 so p = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. However, in general we care about only the first couple of lags, in this case the first 3. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\nSupport Rate for Democratic: There are no lags over the dashed line in the ACF plot, which indicates that there is no significant autocorrelation in the series beyond the lag indicated by the highest peak. In this cases, the ACF plot suggests that there is no systematic relationship between the observations at different time points. This lack of autocorrelation implies that the series is likely stationary, as there is no discernible pattern of dependence between consecutive observations.\nSupport Rate for Independent: There are no lags over the dashed line in the ACF plot, which indicates that there is no significant autocorrelation in the series beyond the lag indicated by the highest peak. In this cases, the ACF plot suggests that there is no systematic relationship between the observations at different time points. This lack of autocorrelation implies that the series is likely stationary, as there is no discernible pattern of dependence between consecutive observations.\nSupport Rate for Republican: ACF Plot has significant lags at 1 so p = 1. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. This observation indicates a lack of stationarity, as high autocorrelation signifies a series that exhibits dependence between consecutive observations. As such, our series demonstrates non-stationary behavior, as it retains significant autocorrelation across multiple lags.\n\n\n5. Dickey-Fuller Test\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID-19 Hospitalization NumberEconomic Indicators (Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\ntseries::adf.test(d_vacc_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  d_vacc_ts\nDickey-Fuller = -4.1125, Lag order = 3, p-value = 0.0183\nalternative hypothesis: stationary\n\n\nCode\np_vacc_ts1 &lt;- na.omit(p_vacc_ts)\ntseries::adf.test(p_vacc_ts1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  p_vacc_ts1\nDickey-Fuller = -3.3701, Lag order = 3, p-value = 0.08067\nalternative hypothesis: stationary\n\n\nCode\npf_vacc_ts1 &lt;- na.omit(pf_vacc_ts)\ntseries::adf.test(pf_vacc_ts1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  pf_vacc_ts1\nDickey-Fuller = -7.4927, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(case_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  case_ts\nDickey-Fuller = -1.8178, Lag order = 3, p-value = 0.6457\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(dead_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dead_ts\nDickey-Fuller = -0.54605, Lag order = 3, p-value = 0.9754\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(hos_ts1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  hos_ts1\nDickey-Fuller = -12.058, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(hos_ts2)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  hos_ts2\nDickey-Fuller = -2.9428, Lag order = 3, p-value = 0.1968\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(hos_ts3)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  hos_ts3\nDickey-Fuller = -2.8936, Lag order = 3, p-value = 0.2165\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(employees_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  employees_ts\nDickey-Fuller = -8.8759, Lag order = 2, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(stock_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  stock_ts\nDickey-Fuller = -3.0918, Lag order = 3, p-value = 0.1381\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(demo_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  demo_ts\nDickey-Fuller = -3.4688, Lag order = 3, p-value = 0.05607\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(inde_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  inde_ts\nDickey-Fuller = -4.2027, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(rep_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  rep_ts\nDickey-Fuller = -3.4937, Lag order = 3, p-value = 0.05222\nalternative hypothesis: stationary\n\n\n\n\n\nIn our project, we delve into an array of statistical series to discern patterns and ascertain stationarity, crucial for understanding sentiment impacts on the stock prices of leading tech companies and broader socio-economic indicators. Our methodology employs rigorous statistical tests, complemented by Autocorrelation Function (ACF) plots, to scrutinize the data’s behavior over time.\nNumber of Daily Vaccinations Per Million: A p-value below 0.05 signals sufficient grounds to reject the null hypothesis at a 5% significance level, indicating stationarity in our series. This finding, however, contrasts with prior conclusions, suggesting the ACF plot’s superior accuracy, which points toward non-stationarity.\nNumber of People Vaccinated Per Hundred: The p-value, exceeding 0.05, reveals an insufficient basis to reject the null hypothesis, indicating a non-stationary series. This necessitates further modifications for stationarity, reinforcing conclusions from earlier analyses, including a significant lag order of 3.\nNumber of People Fully Vaccinated Per Hundred: With a p-value below 0.05, we find adequate evidence to reject the null hypothesis, suggesting stationarity. Yet, this contradicts previous findings, with the ACF plot indicating non-stationarity, challenging our initial conclusion.\nNumber of Newly Confirmed Cases: A p-value above 0.05 indicates a lack of sufficient evidence to dismiss the null hypothesis, suggesting non-stationarity. This aligns with earlier observations, necessitating adjustments for stationarity, including a noted lag order of 3.\nNumber of Death Cases: The p-value, again above 0.05, underscores a lack of adequate evidence to reject the null hypothesis, signaling a non-stationary series and the need for further data adjustments. This finding is consistent with prior analyses.\nNumber of Inpatient Beds: Here, a p-value below 0.05 provides enough justification to reject the null hypothesis, suggesting a stationary series. Nevertheless, this result is at odds with previous analyses, indicating non-stationarity based on the ACF plot.\nNumber of Inpatient Beds Used for COVID: The p-value surpassing 0.05 suggests insufficient evidence to reject the null hypothesis, pointing to a non-stationary series that requires adjustments, corroborating earlier findings and the significance of a lag order of 3.\nUtilization Rate for Inpatient Beds Used for COVID: A high p-value indicates the series’ non-stationarity, echoing the need for adjustments to achieve stationarity and supporting earlier conclusions, including a lag order of 3.\nUnemployment Rate: A low p-value indicates sufficient evidence to reject the null hypothesis, suggesting stationarity. However, this contrasts with previous examples, with the ACF plot indicating non-stationarity.\nPfizer Stock Price:  With a p-value exceeding 0.05, there’s insufficient evidence to reject the null hypothesis, indicating a non-stationary series requiring adjustments, consistent with earlier findings, including a lag order of 3.\nSupport Rate for Democratic: A high p-value reveals a lack of evidence to reject the null hypothesis, suggesting non-stationarity and the need for adjustments, contradicting earlier conclusions of stationarity.\nSupport Rate for Independent: A low p-value provides ample evidence to reject the null hypothesis, indicating a stationary series. This finding aligns with prior conclusions, affirming the series’ stationarity.\nSupport Rate for Republican: The p-value, exceeding 0.05, indicates insufficient evidence to reject the null hypothesis, suggesting a non-stationary series that necessitates adjustments, in line with earlier analyses.\nThrough this detailed exploration, we meticulously gauge the stationarity of diverse series, juxtaposing statistical test results against ACF plot insights to draw nuanced conclusions on the dynamic interplay between sentiment, stock price movements, and broader socio-economic indicators.\n\n\n6. Detrend VS Difference\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID-19 Hospitalization NumberEconomic Indicators (Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\nfit_d_vacc = lm(d_vacc_ts~time(d_vacc_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_d_vacc), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(d_vacc_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_p_vacc = lm(p_vacc_ts1~time(p_vacc_ts1), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_p_vacc), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(p_vacc_ts1), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_pf_vacc = lm(pf_vacc_ts1~time(pf_vacc_ts1), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_pf_vacc), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(pf_vacc_ts1), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_case = lm(case_ts~time(case_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_case), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(case_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_dead = lm(dead_ts~time(dead_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_dead), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(dead_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_hos1 = lm(hos_ts1~time(hos_ts1), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_hos1), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(hos_ts1), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_hos2 = lm(hos_ts2~time(hos_ts2), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_hos2), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(hos_ts2), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_hos3 = lm(hos_ts3~time(hos_ts3), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_hos3), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(hos_ts3), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_emp = lm(employees_ts~time(employees_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_emp), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(employees_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_stock = lm(stock_ts~time(stock_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_stock), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(stock_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_demo = lm(demo_ts~time(demo_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_demo), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(demo_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_inde = lm(inde_ts~time(inde_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_inde), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(inde_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_rep = lm(rep_ts~time(rep_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_rep), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(rep_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\nDetrending and differencing stand as pivotal techniques in the realm of time series analysis, each aimed at achieving the crucial condition of stationarity within a dataset. While navigating the same goal of trend elimination, these methodologies diverge in their approach and application nuances.\nDetrending is a targeted process aimed squarely at eradicating the underlying trend from the dataset. This is accomplished by first meticulously estimating the trend component that permeates the time series and then subtracting this estimated trend from the original dataset. The outcome is a transformed series where the original mean has been adjusted to center around zero, effectively neutralizing the trend influence. However, this transformation is not a panacea; detrended data can still exhibit non-stationary characteristics, such as seasonality or variance instabilities, that require further intervention.\nConversely, differencing operates under a broader scope, addressing stationarity by focusing on the differences between consecutive observations. This method is encapsulated by the formula:\n\\[\\Delta y_t = y_t - y_{t-1}\\]\nwhere \\(\\Delta y_t\\) represents the difference between the current observation \\(y_t\\) and its predecessor \\(y_{t-1}\\). Through this simple yet effective mechanism, differencing excels at mitigating linear trends and highlighting the dynamic changes between data points. Its strength lies particularly in contexts where the time series displays a consistent directional trend, making it a robust choice for such scenarios.\nHowever, it’s worth noting that while differencing is adept at ironing out linear trends, it may falter when faced with nonlinear trends or pronounced seasonal fluctuations. The essence of differencing lies in its ability to simplify the series to a form where patterns and structures become more discernable, albeit at the potential cost of oversimplification in certain complex scenarios.\nThe decision to employ detrending or differencing hinges on a thorough examination of the time series at hand. The specific characteristics of the dataset, including the nature of its trends and seasonalities, dictate the most appropriate method for achieving stationarity. This choice is not merely technical but strategic, laying the foundation for deeper insights and more accurate forecasts in the pursuit of time series analysis.\n\n\n7. Moving Average Smoothing\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID-19 Hospitalization NumberEconomic Indicators (Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(d_vacc_ts), Value = as.vector(d_vacc_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"Daily Vaccination Number (Million)\") +\n  ggtitle(\"Daily Vaccination Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(p_vacc_ts), Value = as.vector(p_vacc_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"People Vaccinated Per Hundred\") +\n  ggtitle(\"People Vaccinated Per Hundred Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(pf_vacc_ts), Value = as.vector(pf_vacc_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"People Fully Vaccinated Per Hundred\") +\n  ggtitle(\"People Fully Vaccinated Per Hundred Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(case_ts), Value = as.vector(case_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"COVID Confirmed Case Number\") +\n  ggtitle(\"COVID Confirmed Case Number Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(dead_ts), Value = as.vector(dead_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"COVID Dead Case Number\") +\n  ggtitle(\"COVID Dead Case Number Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(hos_ts1), Value = as.vector(hos_ts1))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"Inpatient Beds Number\") +\n  ggtitle(\"Inpatient Beds Number Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(hos_ts2), Value = as.vector(hos_ts2))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"Inpatient Beds Used for COVID Number\") +\n  ggtitle(\"Inpatient Beds Used for COVID Number Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(hos_ts3), Value = as.vector(hos_ts3))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"Utilization Rate\") +\n  ggtitle(\"Utilization Rate of Inpatient Beds for COVID Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(employees_ts), Value = as.vector(employees_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"Unemployment Rate\") +\n  ggtitle(\"Unemployment Rate Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(stock_ts), Value = as.vector(stock_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"Stock Price\") +\n  ggtitle(\"Pfizer Stock Price Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(demo_ts), Value = as.vector(demo_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"Support Rate\") +\n  ggtitle(\"Support Rate for Democratics Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(inde_ts), Value = as.vector(inde_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"Support Rate\") +\n  ggtitle(\"Support Rate for Independent Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\nCode\n# Put time series into dataframe\ndf1 &lt;- data.frame(Year = time(rep_ts), Value = as.vector(rep_ts))\n\n# Calculate moving average\nma1 &lt;- zoo::rollapply(df1$Value, 3, mean, fill = NA, align = \"right\")\nma2 &lt;- zoo::rollapply(df1$Value, 12, mean, fill = NA, align = \"right\")\nma3 &lt;- zoo::rollapply(df1$Value, 16, mean, fill = NA, align = \"right\")\nma4 &lt;- zoo::rollapply(df1$Value, 24, mean, fill = NA, align = \"right\")\n\n# Add moving averages to df\ndf1$ma1 &lt;- ma1\ndf1$ma2 &lt;- ma2\ndf1$ma3 &lt;- ma3\ndf1$ma4 &lt;- ma4\n\n# Create plot using ggplot2\nggplot(df1, aes(x = Year, y = Value, color = \"Data\")) +\n  geom_line() +\n  geom_line(aes(y = ma1, color = \"3 SMA\")) +\n  geom_line(aes(y = ma2, color = \"12 SMA\")) +\n  geom_line(aes(y = ma3, color = \"16 SMA\")) +\n  geom_line(aes(y = ma4, color = \"24 SMA\")) +\n  xlab(\"Time\") +\n  ylab(\"Support Rate\") +\n  ggtitle(\"Support Rate for Republican Moving Average Smoothing\") +\n  scale_color_manual(name = \"Type\", values = c(\"Data\" = \"gray\", \"3 SMA\" = \"blue\", \"12 SMA\" = \"orange\", \"16 SMA\" = \"green\", \"24 SMA\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\nTo further refine and enrich the explanation of the use of moving averages in data analysis:\nIn our pursuit of deeper insights, we expanded our analytical toolkit by integrating four distinct moving average windows into the original dataset. This methodical selection spans from agile short-term perspectives to more deliberate long-term vistas. Specifically, we introduced a nimble 3-period moving average (3-MA) to catch immediate trends, alongside two medium-term averages of 12 periods (12-MA) and 16 periods (16-MA), culminating in a comprehensive 24-period moving average (24-MA) to gauge extended trends.\nUpon thorough examination, the 12-MA distinctively stands out for its adeptness across various data series. While the 3-MA provides a glimpse into the data’s immediate direction, it often glosses over the dataset’s more impactful shifts. On the other end of the spectrum, the 16-MA and 24-MA, with their broader strokes, tend to dilute the dataset, masking subtle yet significant patterns, such as seasonal variations that are crucial for holistic analysis.\nRemarkably, the 12-MA window adeptly navigates between these extremes. It offers a balanced lens through which meaningful data fluctuations are brought to the forefront, with just enough smoothing to clarify the analysis without erasing important details. This equilibrium is especially beneficial for analyzing spending data, characterized by its smoother fluctuations. Here, the slightly gentler touch of the 12-MA allows us to unearth and understand the underlying trends with greater precision."
  },
  {
    "objectID": "DL.html",
    "href": "DL.html",
    "title": "Deep Learning for Time Series",
    "section": "",
    "text": "1. Introduction\nWithin this analysis, I will delve into forecasting time series data through the lens of deep learning. Specifically, I will explore and apply the nuances of Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), and Long Short-Term Memory networks (LSTMs). These advanced models will be meticulously evaluated against traditional approaches, namely ARMA, ARIMA, and SARIMA models, to discern their predictive prowess and applicability in time series analysis. This comparative study aims to illuminate the strengths and potential trade-offs between the deep learning methodologies and more conventional statistical models.\n\n\n2. Data Visualization\nIn our session, we would not fit all the datasets since the complexity of the datasets, but we want to mainly focus on three datasets, one is the vaccination rate, and others are the confirmed case number and death case number to see the future trend of these three variables.\n\nVaccination NumberConfirmed Case NumberDeath Case Number\n\n\n\n\nCode\nimport plotly.express as px\nimport nbformat\n\nfig = px.line(monthly_vac, x='date', y=\"daily_vaccinations_per_million\")\nfig.update_layout(\n        xaxis_title='Time',\n        yaxis_title='Daily COVID-19 Vaccination Number',\n        title='Daily COVID-19 Vaccination Number in the US Over Time'\n    )\n\n\n\n                        \n                                            \n\n\nCode\n#fig.show()\n\n\n\n\n\n\nCode\nfig = px.line(monthly_cases, x='date', y=\"cases\")\nfig.update_layout(\n        xaxis_title='Time',\n        yaxis_title='Daily COVID-19 Confimed Number',\n        title='Daily COVID-19 Confimed Number in the US Over Time'\n    )\n\n\n\n                        \n                                            \n\n\nCode\n#fig.show()\n\n\n\n\n\n\nCode\nfig = px.line(monthly_deaths, x='date', y=\"deaths\")\nfig.update_layout(\n        xaxis_title='Time',\n        yaxis_title='Daily COVID-19 Death Number',\n        title='Daily COVID-19 Death Number in the US Over Time'\n    )\n\n\n\n                        \n                                            \n\n\nCode\n#fig.show()\n\n\n\n\n\n\n\n3. Split Data & Normalize\nIn the next phase of analysis, I will partition the datasets into training and testing subsets to facilitate model evaluation and validation. Given the wide range of variable scales present in the original data, I will implement normalization techniques on the regression values. This step is crucial for optimizing model performance by ensuring that the data conforms to a uniform scale, thereby enhancing the accuracy and effectiveness of the predictive models.\n\nVaccination NumberConfirmed Case NumberDeath Case Number\n\n\n\n\nCode\ndef get_train_test(data, split_percent = 0.8):\n    # Convert data to array\n    data = np.array(data)\n    \n    # Normalize data\n    data=(data-np.mean(data,axis=0))/np.std(data,axis=0)\n    \n    # define split point for splitting data into training and testing\n    n = len(data)\n    split = int(n*split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    \n    # return the test splits\n    return train_data, test_data\n\nvac_train_data, vac_test_data = get_train_test(monthly_vac['daily_vaccinations_per_million'])\n\nprint(f'Original shape: {len(monthly_vac[\"daily_vaccinations_per_million\"])}')\n\n\nOriginal shape: 30\n\n\nCode\nprint(f'Train shape: {vac_train_data.shape}')\n\n\nTrain shape: (24,)\n\n\nCode\nprint(f'Test shape: {vac_test_data.shape}')\n\n\nTest shape: (6,)\n\n\nCode\n\nt1 = [*range(0, len(vac_train_data))]\nt2 = len(vac_train_data) + np.array([*range(0, len(vac_test_data))])\n\ndef plotly_plot(t, y, title = \"Plot\", x_label = \"Time (Month)\", y_label = \"Value\"):\n\n    \n    fig = px.line(x = t[0], y = y[0], title = title, render_mode = 'SVG')  \n    \n    # Plot the scatter points\n    for i in range(1,len(y)):\n        fig.add_scatter(x = t[i], y = y[i], mode='lines')\n    \n    # update the layout with labels and customization\n    fig.update_layout(\n        xaxis_title = x_label,\n        yaxis_title = y_label,\n        showlegend = False\n    )\n    # show the figure\n    fig.show()\n\nplotly_plot([t1, t2], [vac_train_data, vac_test_data], title = \"Vaccination Train & Test Data\")\n\n\n\n\n\n\nCode\ncase_train_data, case_test_data = get_train_test(monthly_cases['cases'])\n\nprint(f'Original shape: {len(monthly_cases[\"cases\"])}')\n\n\nOriginal shape: 43\n\n\nCode\nprint(f'Train shape: {case_train_data.shape}')\n\n\nTrain shape: (34,)\n\n\nCode\nprint(f'Test shape: {case_test_data.shape}')\n\n\nTest shape: (9,)\n\n\nCode\n\nt1 = [*range(0, len(case_train_data))]\nt2 = len(case_train_data) + np.array([*range(0, len(case_test_data))])\n\nplotly_plot([t1, t2], [case_train_data, case_test_data], title = \"Confirmed Case Train & Test Data\")\n\n\n\n\n\n\nCode\ndeath_train_data, death_test_data = get_train_test(monthly_deaths['deaths'])\n\nprint(f'Original shape: {len(monthly_deaths[\"deaths\"])}')\n\n\nOriginal shape: 43\n\n\nCode\nprint(f'Train shape: {death_train_data.shape}')\n\n\nTrain shape: (34,)\n\n\nCode\nprint(f'Test shape: {death_test_data.shape}')\n\n\nTest shape: (9,)\n\n\nCode\n\nt1 = [*range(0, len(death_train_data))]\nt2 = len(death_train_data) + np.array([*range(0, len(death_test_data))])\n\nplotly_plot([t1, t2], [death_train_data, death_test_data], title = \"Death Case Train & Test Data\")\n\n\n\n\n\n\n\n4. Mini-Batching\nTo enhance the efficacy of the training process, I will incorporate the use of mini-batching. This approach involves updating the gradients more frequently within each epoch, which is expected to significantly improve the overall performance of the model. By doing so, the model can learn more effectively and adaptively from smaller subsets of data, leading to more accurate and robust predictions.\n\nVaccination NumberConfirmed Case NumberDeath Case Number\n\n\n\n\nCode\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n  # Initialize\n  i_start=0\n  count=0\n  x_out=[]\n  y_out=[]\n  # Sequentially build mini-batches\n  while i_start + lookback + delay &lt; x.shape[0]:\n    i_stop = i_start + lookback\n    i_pred = i_stop + delay\n    # report if desired\n    if verbose and count &lt; 2:\n      print(\"indice range:\",i_start, i_stop, \"--&gt;\",i_pre)\n    # define arrays\n    indices_to_keep = []\n    j = i_stop\n    while j &gt;= i_start:\n      indices_to_keep.append(j)\n      j -= step\n    # create mini-batch sample\n    xtmp = x[indices_to_keep,:]\n    xtmp = xtmp[:,feature_columns]\n    ytmp=x[i_pred,target_columns]\n    x_out.append(xtmp)\n    y_out.append(ytmp)\n    # report if desired\n    if verbose and count &lt;2:\n      print(xtmp, \"--&gt;\", ytmp)\n      print(\"shape:\", xtmp.shape, \"--&gt;\",ytmp.shape)\n    # plot\n    if verbose and count &lt;2:\n      fig, ax = plt.subplots()\n      ax.plot(x, 'b-')\n      ax.plot(x,'bx')\n      ax.plot(indices_to_keep, xtmp, 'go')\n      ax.plot(i_pred*np.ones(len(target_columns)),ytmp, 'ro')\n      plt.show()\n    # update start point\n    if unique:\n      i_start += lookback\n    else:\n      i_start += 1\n    count += 1\n  return np.array(x_out), np.array(y_out)\n\ntrain = vac_train_data.reshape(vac_train_data.shape[0],1)\ntest = vac_test_data.reshape(vac_test_data.shape[0],1)\n\nvac_trainX, vac_trainY = form_arrays(train,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\nvac_testX, vac_testY = form_arrays(test,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\n\nprint(f'Train shape: {vac_trainX.shape} , {vac_trainY.shape}')\n\n\nTrain shape: (21, 3, 1) , (21, 1)\n\n\nCode\nprint(f'Test shape: {vac_testX.shape} , {vac_testY.shape}')\n\n\nTest shape: (3, 3, 1) , (3, 1)\n\n\n\n\n\n\nCode\ntrain = case_train_data.reshape(case_train_data.shape[0],1)\ntest = case_test_data.reshape(case_test_data.shape[0],1)\n\ncase_trainX, case_trainY = form_arrays(train,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\ncase_testX, case_testY = form_arrays(test,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\n\nprint(f'Train shape: {case_trainX.shape} , {case_trainY.shape}')\n\n\nTrain shape: (31, 3, 1) , (31, 1)\n\n\nCode\nprint(f'Test shape: {case_testX.shape} , {case_testY.shape}')\n\n\nTest shape: (6, 3, 1) , (6, 1)\n\n\n\n\n\n\nCode\ntrain = death_train_data.reshape(death_train_data.shape[0],1)\ntest = death_test_data.reshape(death_test_data.shape[0],1)\n\ndeath_trainX, death_trainY = form_arrays(train,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\ndeath_testX, death_testY = form_arrays(test,lookback=2, delay=1, step=1, feature_columns=[0], target_columns=[0], unique=False,verbose=False)\n\nprint(f'Train shape: {death_trainX.shape} , {death_trainY.shape}')\n\n\nTrain shape: (31, 3, 1) , (31, 1)\n\n\nCode\nprint(f'Test shape: {death_testX.shape} , {death_testY.shape}')\n\n\nTest shape: (6, 3, 1) , (6, 1)\n\n\n\n\n\n\n\n5. RNN\nIn this section, I will focus on training a Recurrent Neural Network (RNN), which is specifically engineered for handling sequential data by utilizing cyclic connections that retain memory of prior inputs. To enhance the robustness and generalizability of the RNN, I plan to train it both with and without regularization techniques. To streamline this process, I will encapsulate the modeling code within a single function that accepts parameters for data, model type, and a boolean indicating whether regularization should be applied. This modular approach will facilitate easy adjustments and retesting under various configurations, improving efficiency and experimental flexibility.\nHere is how we define each function to generate the model:\n\n\nCode\nimport tensorflow as tf\n# Utility functions \ndef regression_report(yt,ytp,yv,yvp):\n  print(\"--------- Regression Report ---------\")\n  print(\"TRAINING:\")\n  train_mse = np.mean((yt - ytp) ** 2)\n  train_mae = np.mean(np.abs(yt - ytp))\n  print(\"MSE\", train_mse)\n  print(\"MAE\", train_mae)\n  \n  # PARITY PLOT\n  fig, ax = plt.subplots()\n  ax.plot(yt, ytp, 'ro')\n  ax.plot(yt, yt, 'b-')\n  ax.set(xlabel='y_data', ylabel='y_predicted',\n        title = 'Training data parity plot (line y=x represents a perfect fit)')\n  plt.show()\n  \n  # PLOT PART OF THE PREDICTED TIME-SERIES\n  frac_plot=1.0\n  upper=int(frac_plot*yt.shape[0]);\n  fig, ax = plt.subplots()\n  ax.plot(yt[0:upper], 'b-')\n  ax.plot(ytp[0:upper], 'r-', alpha=0.5)\n  ax.plot(ytp[0:upper], 'ro', alpha=0.25)\n  ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Training: Time-series prediction')\n  plt.show()\n  \n  print(\"VALIDATION:\")\n  val_mse = np.mean((yv - yvp) ** 2)\n  val_mae = np.mean(np.abs(yv - yvp))\n  print(\"MSE\", val_mse)\n  print(\"MAE\", val_mae)\n  \n  # PARITY PLOT\n  fig,ax = plt.subplots()\n  ax.plot(yv,yvp, 'ro')\n  ax.plot(yv, yv,'b-')\n  ax.set(xlabel='y_data', ylabel='y_predicted',title='Validation data parity plot (line y=x represents a perfect fit)')\n  \n  # PLOT PART OF THE PREDICTED TIME-SERIES\n  upper=int(frac_plot*yv.shape[0])\n  fig,ax = plt.subplots()\n  ax.plot(yv[0:upper], 'b-')\n  ax.plot(yvp[0:upper], 'r-', alpha=0.5)\n  ax.plot(yvp[0:upper], 'ro', alpha=0.25)\n  ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)',title='Validation: Time-series prediction')\n  plt.show()\n  return train_mse, train_mae, val_mse, val_mae\n\ndef history_plot(history):\n  FS=18 #fontsize\n  history_dict = history.history\n  loss_values = history_dict['loss']\n  val_loss_values = history_dict['val_loss']\n  epochs = range(1, len(loss_values) + 1)\n  plt.plot(epochs, loss_values, 'bo', label='Training Loss')\n  plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n  plt.title('Training and Validation Loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n  plt.ylim(bottom=0) \n  plt.legend()\n  plt.show()\n\n# Define model function\ndef train_model(model_type, train_x, train_y, val_x, val_y, regularization = True, L2=1e-4):\n  if regularization:\n    reg = regularizers.L2(L2)\n  else:\n    reg = None\n  \n  # Define parameters\n  optimizer=\"rmsprop\"\n  loss_function = 'mean_squared_error'\n  learning_rate=0.01\n  numbers_epochs=200\n  input_shape=(train_x.shape[1],train_x.shape[2])\n  train_x1 = train_x.reshape(train_x.shape[0],train_x.shape[1]*train_x.shape[2])\n  batch_size=len(train_x1)              # batch training\n  \n  # BUILD MODEL\n  recurrent_hidden_units=32\n  \n  # CREATE MODEL\n  model = keras.Sequential()\n  \n  # ADD RECURRENT LAYER\n  if model_type == 'RNN':\n    model.add(SimpleRNN(\n    units=recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=input_shape, \n    # recurrent_dropout=0.8,\n    recurrent_regularizer=reg,\n    activation='relu')\n              )\n  elif model_type == 'GRU':\n    model.add(GRU(\n    units=recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=input_shape, \n    # recurrent_dropout=0.8,\n    recurrent_regularizer=reg,\n    activation='relu')\n              ) \n  elif model_type == 'LSTM':\n    model.add(LSTM(\n    units=recurrent_hidden_units,\n    return_sequences=False,\n    input_shape=input_shape, \n    # recurrent_dropout=0.8,\n    recurrent_regularizer=reg,\n    activation='relu')\n              )\n  else:\n    print('Wrong model type')\n  \n  # NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \n  model.add(Dense(units=1, activation='linear'))\n  \n  # MODEL SUMMARY\n  print(model.summary()); #print(x_train.shape,y_train.shape)  \n  \n  # COMPILING THE MODEL \n  opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n  model.compile(optimizer=opt, loss=loss_function)\n  \n  # TRAINING YOUR MODEL\n  history_techmu = model.fit(train_x,\n                      train_y,\n                      epochs=numbers_epochs,\n                      batch_size=batch_size, verbose=False,\n                      validation_data=(val_x, val_y))\n  # History plot\n  history_plot(history_techmu)\n  \n  # Predictions \n  train_pred=model.predict(train_x)\n  val_pred=model.predict(val_x) \n  train_mse, train_mae, val_mse, val_mae = regression_report(train_y,train_pred,val_y,val_pred)\n  return train_mse, train_mae, val_mse, val_mae\n\n\n\nVaccination NumberConfirmed Case NumberDeath Case Number\n\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model’s parameters for more robust predictions.\n\nWith Regularization\n\n\nCode\nmodel_type = 'RNN'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\n\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn (SimpleRNN)          │ (None, 32)             │         1,088 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 1,121 (4.38 KB)\n Trainable params: 1,121 (4.38 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.02507889151580927\nMAE 0.1149272712569786\nVALIDATION:\nMSE 0.006079864716768059\nMAE 0.07522609063899766\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults = []\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Vaccination', 'model_type': 'RNN', 'reg': True, 'train_mse': 0.02507889151580927, 'train_mae': 0.1149272712569786, 'val_mse': 0.006079864716768059, 'val_mae': 0.07522609063899766}\n\n\n\n\n\n\n\nWithout Regularization\n\n\nCode\nmodel_type = 'RNN'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\n\n\nModel: \"sequential_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_1 (SimpleRNN)        │ (None, 32)             │         1,088 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 1,121 (4.38 KB)\n Trainable params: 1,121 (4.38 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.00787940727730981\nMAE 0.06698729147534957\nVALIDATION:\nMSE 0.00920212609798102\nMAE 0.09152555238290379\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Vaccination', 'model_type': 'RNN', 'reg': False, 'train_mse': 0.00787940727730981, 'train_mae': 0.06698729147534957, 'val_mse': 0.00920212609798102, 'val_mae': 0.09152555238290379}\n\n\n\n\n\n\n\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model’s parameters for more robust predictions.\n\nWith Regularization\n\n\nCode\nmodel_type = 'RNN'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\n\n\nModel: \"sequential_2\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_2 (SimpleRNN)        │ (None, 32)             │         1,088 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 1,121 (4.38 KB)\n Trainable params: 1,121 (4.38 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.01208264434375932\nMAE 0.09365220881610602\nVALIDATION:\nMSE 0.287867368754712\nMAE 0.3990768925102359\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Confirmed Case', 'model_type': 'RNN', 'reg': True, 'train_mse': 0.01208264434375932, 'train_mae': 0.09365220881610602, 'val_mse': 0.287867368754712, 'val_mae': 0.3990768925102359}\n\n\n\n\n\n\n\nWithout Regularization\n\n\nCode\nmodel_type = 'RNN'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\n\n\nModel: \"sequential_3\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_3 (SimpleRNN)        │ (None, 32)             │         1,088 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 1,121 (4.38 KB)\n Trainable params: 1,121 (4.38 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.011068698360150505\nMAE 0.09134005644279669\nVALIDATION:\nMSE 0.28914222560102826\nMAE 0.40440739549714383\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Confirmed Case', 'model_type': 'RNN', 'reg': False, 'train_mse': 0.011068698360150505, 'train_mae': 0.09134005644279669, 'val_mse': 0.28914222560102826, 'val_mae': 0.40440739549714383}\n\n\n\n\n\n\n\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model’s parameters for more robust predictions.\n\nWith Regularization\n\n\nCode\nmodel_type = 'RNN'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\n\n\nModel: \"sequential_4\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_4 (SimpleRNN)        │ (None, 32)             │         1,088 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 1,121 (4.38 KB)\n Trainable params: 1,121 (4.38 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.010132386133306859\nMAE 0.08203391647657629\nVALIDATION:\nMSE 0.200721501491255\nMAE 0.31593309251768603\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Death Case', 'model_type': 'RNN', 'reg': True, 'train_mse': 0.010132386133306859, 'train_mae': 0.08203391647657629, 'val_mse': 0.200721501491255, 'val_mae': 0.31593309251768603}\n\n\n\n\n\n\n\nWithout Regularization\n\n\nCode\nmodel_type = 'RNN'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\n\n\nModel: \"sequential_5\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_5 (SimpleRNN)        │ (None, 32)             │         1,088 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 1,121 (4.38 KB)\n Trainable params: 1,121 (4.38 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.011191254598802181\nMAE 0.08335613950623097\nVALIDATION:\nMSE 0.21909917177845487\nMAE 0.3324218035569113\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Death Case', 'model_type': 'RNN', 'reg': False, 'train_mse': 0.011191254598802181, 'train_mae': 0.08335613950623097, 'val_mse': 0.21909917177845487, 'val_mae': 0.3324218035569113}\n\n\n\n\n\n\n\n\n\n\n\n6. LSTM\nIn this section, I will be training a Long Short-Term Memory (LSTM) network, a specialized form of Recurrent Neural Network (RNN). LSTMs are designed to overcome the vanishing gradient problem inherent in traditional RNNs through a sophisticated gating mechanism. This mechanism effectively regulates the information flow into and out of the network’s memory cells, facilitating the learning of long-term dependencies. I plan to evaluate the LSTM’s performance both with and without the application of regularization techniques, to ascertain their impact on the model’s ability to generalize.\n\nVaccination NumberConfirmed Case NumberDeath Case Number\n\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model’s parameters for more robust predictions.\n\nWith Regularization\n\n\nCode\nmodel_type = 'LSTM'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\n\n\nModel: \"sequential_6\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (LSTM)                     │ (None, 32)             │         4,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 4,385 (17.13 KB)\n Trainable params: 4,385 (17.13 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.10949200399905415\nMAE 0.2567433805785602\nVALIDATION:\nMSE 0.045478360226215715\nMAE 0.21310756376468887\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Vaccination', 'model_type': 'LSTM', 'reg': True, 'train_mse': 0.10949200399905415, 'train_mae': 0.2567433805785602, 'val_mse': 0.045478360226215715, 'val_mae': 0.21310756376468887}\n\n\n\n\n\n\n\nWithout Regularization\n\n\nCode\nmodel_type = 'LSTM'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\n\n\nModel: \"sequential_7\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm_1 (LSTM)                   │ (None, 32)             │         4,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 4,385 (17.13 KB)\n Trainable params: 4,385 (17.13 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.07068324949798901\nMAE 0.20521890681234387\nVALIDATION:\nMSE 0.3079376817918499\nMAE 0.5525765694288786\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Vaccination', 'model_type': 'LSTM', 'reg': False, 'train_mse': 0.07068324949798901, 'train_mae': 0.20521890681234387, 'val_mse': 0.3079376817918499, 'val_mae': 0.5525765694288786}\n\n\n\n\n\n\n\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model’s parameters for more robust predictions.\n\nWith Regularization\n\n\nCode\nmodel_type = 'LSTM'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\n\n\nModel: \"sequential_8\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm_2 (LSTM)                   │ (None, 32)             │         4,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 4,385 (17.13 KB)\n Trainable params: 4,385 (17.13 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.010772498417440294\nMAE 0.08536091643176864\nVALIDATION:\nMSE 0.15685404893477067\nMAE 0.27687905418936576\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Confirmed Case', 'model_type': 'LSTM', 'reg': True, 'train_mse': 0.010772498417440294, 'train_mae': 0.08536091643176864, 'val_mse': 0.15685404893477067, 'val_mae': 0.27687905418936576}\n\n\n\n\n\n\n\nWithout Regularization\n\n\nCode\nmodel_type = 'LSTM'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\n\n\nModel: \"sequential_9\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm_3 (LSTM)                   │ (None, 32)             │         4,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 4,385 (17.13 KB)\n Trainable params: 4,385 (17.13 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.010985848313029671\nMAE 0.08988362240819925\nVALIDATION:\nMSE 0.23687703248351913\nMAE 0.3491596388103492\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Confirmed Case', 'model_type': 'LSTM', 'reg': False, 'train_mse': 0.010985848313029671, 'train_mae': 0.08988362240819925, 'val_mse': 0.23687703248351913, 'val_mae': 0.3491596388103492}\n\n\n\n\n\n\n\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model’s parameters for more robust predictions.\n\nWith Regularization\n\n\nCode\nmodel_type = 'LSTM'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\n\n\nModel: \"sequential_10\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm_4 (LSTM)                   │ (None, 32)             │         4,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (Dense)                │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 4,385 (17.13 KB)\n Trainable params: 4,385 (17.13 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.01200209890345945\nMAE 0.08651425714075692\nVALIDATION:\nMSE 0.19144112076395683\nMAE 0.3055604438811861\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Death Case', 'model_type': 'LSTM', 'reg': True, 'train_mse': 0.01200209890345945, 'train_mae': 0.08651425714075692, 'val_mse': 0.19144112076395683, 'val_mae': 0.3055604438811861}\n\n\n\n\n\n\n\nWithout Regularization\n\n\nCode\nmodel_type = 'LSTM'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\n\n\nModel: \"sequential_11\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm_5 (LSTM)                   │ (None, 32)             │         4,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (Dense)                │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 4,385 (17.13 KB)\n Trainable params: 4,385 (17.13 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.012895536589574551\nMAE 0.096573344388118\nVALIDATION:\nMSE 0.27565403360039176\nMAE 0.3741007038837445\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Death Case', 'model_type': 'LSTM', 'reg': False, 'train_mse': 0.012895536589574551, 'train_mae': 0.096573344388118, 'val_mse': 0.27565403360039176, 'val_mae': 0.3741007038837445}\n\n\n\n\n\n\n\n\n\n\n\n7. GRU\n\nVaccination NumberConfirmed Case NumberDeath Case Number\n\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model’s parameters for more robust predictions.\n\nWith Regularization\n\n\nCode\nmodel_type = 'GRU'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\n\n\nModel: \"sequential_12\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gru (GRU)                       │ (None, 32)             │         3,360 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_12 (Dense)                │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 3,393 (13.25 KB)\n Trainable params: 3,393 (13.25 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.059621629644910724\nMAE 0.19088212280974756\nVALIDATION:\nMSE 0.11277090259496636\nMAE 0.3356135067451373\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Vaccination', 'model_type': 'GRU', 'reg': True, 'train_mse': 0.059621629644910724, 'train_mae': 0.19088212280974756, 'val_mse': 0.11277090259496636, 'val_mae': 0.3356135067451373}\n\n\n\n\n\n\n\nWithout Regularization\n\n\nCode\nmodel_type = 'GRU'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, vac_trainX, vac_trainY, vac_testX, vac_testY, regularization = reg)\n\n\nModel: \"sequential_13\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gru_1 (GRU)                     │ (None, 32)             │         3,360 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_13 (Dense)                │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 3,393 (13.25 KB)\n Trainable params: 3,393 (13.25 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.06284618651616207\nMAE 0.20501508582688174\nVALIDATION:\nMSE 0.31607067071891154\nMAE 0.5619419194368894\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Vaccination',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Vaccination', 'model_type': 'GRU', 'reg': False, 'train_mse': 0.06284618651616207, 'train_mae': 0.20501508582688174, 'val_mse': 0.31607067071891154, 'val_mae': 0.5619419194368894}\n\n\n\n\n\n\n\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model’s parameters for more robust predictions.\n\nWith Regularization\n\n\nCode\nmodel_type = 'GRU'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\n\n\nModel: \"sequential_14\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gru_2 (GRU)                     │ (None, 32)             │         3,360 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_14 (Dense)                │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 3,393 (13.25 KB)\n Trainable params: 3,393 (13.25 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.012879188038375282\nMAE 0.09812820178520207\nVALIDATION:\nMSE 0.27074395311402305\nMAE 0.3705160823744463\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Confirmed Case', 'model_type': 'GRU', 'reg': True, 'train_mse': 0.012879188038375282, 'train_mae': 0.09812820178520207, 'val_mse': 0.27074395311402305, 'val_mae': 0.3705160823744463}\n\n\n\n\n\n\n\nWithout Regularization\n\n\nCode\nmodel_type = 'GRU'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, case_trainX, case_trainY, case_testX, case_testY, regularization = reg)\n\n\nModel: \"sequential_15\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gru_3 (GRU)                     │ (None, 32)             │         3,360 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_15 (Dense)                │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 3,393 (13.25 KB)\n Trainable params: 3,393 (13.25 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.012173408547347405\nMAE 0.08446425152752393\nVALIDATION:\nMSE 0.1706351819486199\nMAE 0.28984830963675345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Confirmed Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Confirmed Case', 'model_type': 'GRU', 'reg': False, 'train_mse': 0.012173408547347405, 'train_mae': 0.08446425152752393, 'val_mse': 0.1706351819486199, 'val_mae': 0.28984830963675345}\n\n\n\n\n\n\n\n\nI will be training the model in two distinct scenarios: with regularization and without. This dual approach will allow us to evaluate the impact of regularization techniques on model performance, particularly in terms of preventing overfitting and enhancing generalization to new data. This comparative strategy is designed to optimize and fine-tune our model’s parameters for more robust predictions.\n\nWith Regularization\n\n\nCode\nmodel_type = 'GRU'\nreg = True\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\n\n\nModel: \"sequential_16\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gru_4 (GRU)                     │ (None, 32)             │         3,360 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_16 (Dense)                │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 3,393 (13.25 KB)\n Trainable params: 3,393 (13.25 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.013954300483919\nMAE 0.10031510864130365\nVALIDATION:\nMSE 0.2970078119234108\nMAE 0.3840727000321718\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Death Case', 'model_type': 'GRU', 'reg': True, 'train_mse': 0.013954300483919, 'train_mae': 0.10031510864130365, 'val_mse': 0.2970078119234108, 'val_mae': 0.3840727000321718}\n\n\n\n\n\n\n\nWithout Regularization\n\n\nCode\nmodel_type = 'GRU'\nreg = False\ntrain_mse, train_mae, val_mse, val_mae = train_model(model_type, death_trainX, death_trainY, death_testX, death_testY, regularization = reg)\n\n\nModel: \"sequential_17\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ gru_5 (GRU)                     │ (None, 32)             │         3,360 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_17 (Dense)                │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 3,393 (13.25 KB)\n Trainable params: 3,393 (13.25 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n--------- Regression Report ---------\nTRAINING:\nMSE 0.014024941900819576\nMAE 0.09984642953073165\nVALIDATION:\nMSE 0.2968673524738457\nMAE 0.38273439694302996\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresult_dict = {\n  'data_type':'Death Case',\n  'model_type':model_type,\n  'reg':reg,\n  'train_mse':train_mse,\n  'train_mae':train_mae,\n  'val_mse':val_mse,\n  'val_mae':val_mae\n  \n}\nresults.append(result_dict)\nprint(f'Model results: {result_dict}')\n\n\nModel results: {'data_type': 'Death Case', 'model_type': 'GRU', 'reg': False, 'train_mse': 0.014024941900819576, 'train_mae': 0.09984642953073165, 'val_mse': 0.2968673524738457, 'val_mae': 0.38273439694302996}\n\n\n\n\n\n\n\n\n\n\n\n8. Discussion\nIn constructing our models, we design a framework tailored for RNN, LSTM, and GRU networks to tackle specific tasks. Each model begins with defining regularization practices to ensure generalization, using L2 regularization as needed. We configure the model using the RMSprop optimizer, a mean squared error loss function, and set a learning rate. Depending on the model type—RNN, GRU, or LSTM—we adjust the architecture by setting the number of recurrent hidden units and whether the model returns sequences. After assembling the model, we compile it, train it on our dataset over numerous epochs, and validate it using a separate dataset. This systematic approach allows us to fine-tune and compare the performance of each neural network architecture effectively.\n\n8.1 How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?\n\n\nCode\n# Combine results into table\nresult_df = pd.DataFrame(results)\nresult_df['train_rmse'] = result_df['train_mse'].apply(lambda x: x ** .5)\nresult_df['val_rmse'] = result_df['val_mse'].apply(lambda x: x ** .5)\nresult_df[['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n\n\n         data_type model_type    reg  train_rmse  val_rmse\n8   Confirmed Case       LSTM   True    0.103791  0.396048\n15  Confirmed Case        GRU  False    0.110333  0.413080\n9   Confirmed Case       LSTM  False    0.104813  0.486700\n14  Confirmed Case        GRU   True    0.113487  0.520331\n2   Confirmed Case        RNN   True    0.109921  0.536533\n3   Confirmed Case        RNN  False    0.105208  0.537719\n10      Death Case       LSTM   True    0.109554  0.437540\n4       Death Case        RNN   True    0.100660  0.448020\n5       Death Case        RNN  False    0.105789  0.468080\n11      Death Case       LSTM  False    0.113559  0.525028\n17      Death Case        GRU  False    0.118427  0.544855\n16      Death Case        GRU   True    0.118128  0.544984\n0      Vaccination        RNN   True    0.158363  0.077973\n1      Vaccination        RNN  False    0.088766  0.095928\n6      Vaccination       LSTM   True    0.330896  0.213257\n12     Vaccination        GRU   True    0.244175  0.335814\n7      Vaccination       LSTM  False    0.265863  0.554921\n13     Vaccination        GRU  False    0.250691  0.562202\n\n\nBest Vaccination Number DL Model:\n\n\nCode\nresult_df[result_df.data_type=='Vaccination'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True).head(1)\n\n\n     data_type model_type   reg  train_rmse  val_rmse\n0  Vaccination        RNN  True    0.158363  0.077973\n\n\nWe can see the best DL model for vaccination number is RNN model without regulation since it has the lowest validation RMSE value.\nBest Confirmed Case DL Model:\n\n\nCode\nresult_df[result_df.data_type=='Confirmed Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True).head(1)\n\n\n        data_type model_type   reg  train_rmse  val_rmse\n8  Confirmed Case       LSTM  True    0.103791  0.396048\n\n\nWe can see the best DL model for confirmed case is RNN model without regulation since it has the lowest validation RMSE value.\nBest Death Case DL Model:\n\n\nCode\nresult_df[result_df.data_type=='Death Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True).head(1)\n\n\n     data_type model_type   reg  train_rmse  val_rmse\n10  Death Case       LSTM  True    0.109554   0.43754\n\n\nWe can see the best DL model for death case is LSTM model with regulation since it has the lowest validation RMSE value.\nIn our evaluation, the RNN and LSTM models demonstrated superior performance, excelling in their predictive capabilities. Conversely, the GRU model showed relatively lower effectiveness. This divergence in performance highlights the distinct advantages and challenges associated with each type of neural network architecture, emphasizing the importance of choosing the right model based on the specific requirements and nuances of the dataset at hand.\n\n\n8.2 What effect does including regularization have on your results?\nRegularization is crucial in machine learning, adding a penalty to the loss function to reduce the complexity of the model, thus minimizing overfitting to the training data. This technique varies in effectiveness across different datasets. For example, an un-regularized LSTM model achieved the lowest RMSE for emissions data, while a regularized GRU model was most effective for the temperature dataset. Across various datasets, a consistent observation was a higher RMSE in the validation set compared to the training set, indicating persistent overfitting despite the regularization’s attempt to bridge the gap between training and validation performance.\n\n\n8.3 How far into the future can the deep learning model accurately predict the future?\nDeep learning models’ ability to predict future events hinges on the data they have encountered during training. These models, fundamentally supervised, learn and generate predictions based on the sequences they’ve been exposed to. Thus, their predictive power extends only as far as recognizing patterns similar to those within their training datasets. The farther into the future a prediction extends, the more reliant it becomes on the quality and representativeness of historical data, limiting accuracy when facing novel scenarios or trends not previously seen during training.\n\n\n8.4. How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models from HW-3?\n\nVaccination NumberConfirmed Case NumberDeath Case Number\n\n\nFrom Arima model, below are the computed metrics:\n\n\n\nMetric\nValue\n\n\n\n\nME\n-1.260901e+04\n\n\nRMSE\n4.404350e+04\n\n\nMAE\n2.961164e+04\n\n\nMPE\n-1.819884e+00\n\n\nMAPE\n2.997255e+01\n\n\nMASE\n5.644007e-01\n\n\nACF1\n-1.726600e-01\n\n\n\nFrom Deep Learning model, below are the computed metrics:\n\n\nCode\nresult_df[result_df.data_type=='Vaccination'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n\n\n      data_type model_type    reg  train_rmse  val_rmse\n0   Vaccination        RNN   True    0.158363  0.077973\n1   Vaccination        RNN  False    0.088766  0.095928\n6   Vaccination       LSTM   True    0.330896  0.213257\n12  Vaccination        GRU   True    0.244175  0.335814\n7   Vaccination       LSTM  False    0.265863  0.554921\n13  Vaccination        GRU  False    0.250691  0.562202\n\n\n\n\nFrom Arima model, below are the computed metrics:\n\n\n\nMetric\nValue\n\n\n\n\nME\n5.887429e+04\n\n\nRMSE\n2.036015e+06\n\n\nMAE\n1.092948e+06\n\n\nMPE\n1.626543e+04\n\n\nMAPE\n1.876329e+04\n\n\nMASE\n2.083170e+01\n\n\nACF1\n1.041066e-01\n\n\n\nFrom Deep Learning model, below are the computed metrics:\n\n\nCode\nresult_df[result_df.data_type=='Confirmed Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n\n\n         data_type model_type    reg  train_rmse  val_rmse\n8   Confirmed Case       LSTM   True    0.103791  0.396048\n15  Confirmed Case        GRU  False    0.110333  0.413080\n9   Confirmed Case       LSTM  False    0.104813  0.486700\n14  Confirmed Case        GRU   True    0.113487  0.520331\n2   Confirmed Case        RNN   True    0.109921  0.536533\n3   Confirmed Case        RNN  False    0.105208  0.537719\n\n\n\n\nFrom Arima model, below are the computed metrics:\n\n\n\nMetric\nValue\n\n\n\n\nME\n1373.71612640\n\n\nRMSE\n8276.58694400\n\n\nMAE\n6716.48728344\n\n\nMPE\n64.68052062\n\n\nMAPE\n74.15331696\n\n\nMASE\n0.12801688\n\n\nACF1\n-0.09912142\n\n\n\nFrom Deep Learning model, below are the computed metrics:\n\n\nCode\nresult_df[result_df.data_type=='Death Case'][['data_type','model_type', 'reg','train_rmse','val_rmse']].sort_values(by=['data_type', 'val_rmse'], ascending=True)\n\n\n     data_type model_type    reg  train_rmse  val_rmse\n10  Death Case       LSTM   True    0.109554  0.437540\n4   Death Case        RNN   True    0.100660  0.448020\n5   Death Case        RNN  False    0.105789  0.468080\n11  Death Case       LSTM  False    0.113559  0.525028\n17  Death Case        GRU  False    0.118427  0.544855\n16  Death Case        GRU   True    0.118128  0.544984\n\n\n\n\n\nThe performance analysis shows that DL models often outperform traditional models, notably in terms of RMSE, showcasing their robustness in handling diverse datasets. However, the efficacy of these models is somewhat constrained by the size of the available data. Small datasets tend to skew performance, highlighting a potential limitation in scenarios where expansive data is not available.\nFor ARIMA models, their predictive capability is generally limited when dealing with noisy, fluctuating data, often producing overly simplistic forecasts. In contrast, ARIMAX and VAR models manage to capture underlying trends more effectively, albeit with some inconsistency in the presence of volatility. Deep Learning models, on the other hand, excel with larger datasets, achieving near-perfect predictions. The performance disparity underscores the importance of dataset size in leveraging the full potential of advanced DL models.\n\n\n\n9. Write a discussion paragraph Comparing your models (use RMSE) and forecasts from these sections with your Deep Learning Models.\nThis analysis has demonstrated the potent capabilities of deep learning in forecasting time series data. The efficacy of deep learning models relative to traditional approaches is challenging to evaluate directly due to discrepancies in the training and validation datasets. However, the Root Mean Square Error (RMSE) metric reveals considerably higher values for traditional time series models, implying that these models may be less adept at capturing the variability within the dataset. Conversely, deep learning models have consistently produced RMSE values under 1, suggesting superior performance in tracking the actual data trends.\nA closer inspection of the training and validation performance of deep learning models against the actual forecasts from traditional models, like ARIMA, underscores this contrast. The ARIMA models, despite their sophistication, often faltered in accounting for the dataset’s inherent variance. In contrast, deep learning models demonstrated an ability to assimilate and reflect this variance in their predictions. Notably, the hierarchy of deep learning model performance typically favored the GRU, followed by the LSTM, and then the RNN, aligning with the evolutionary complexity of these architectures.\nTraditional time series models necessitate transforming data into a stationary form, with consistent mean and variance, to make accurate extrapolations. This often involves processes such as differencing, which may not always be sufficient for effective forecasting. On the other hand, deep learning models function as universal approximators, capable of modeling any form of data variance. This attribute renders them particularly suitable for complex time series datasets characterized by pronounced fluctuations and seasonal variations, enabling them to deliver more accurate and robust forecasts."
  },
  {
    "objectID": "UT.html",
    "href": "UT.html",
    "title": "Univariate TS Models",
    "section": "",
    "text": "1. ACF & PACF Plots\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID-19 Hospitalization NumberEconomic Indicators (Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\nd_vacc_acf &lt;- ggAcf(d_vacc_ts)+ggtitle(\"ACF Plot for Daily Vaccinations per Million\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nd_vacc_acf1 &lt;- ggAcf(diff(d_vacc_ts))+ggtitle(\"ACF Plot for Differented Daily Vaccinations per Million\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nd_vacc_pacf &lt;- ggPacf(d_vacc_ts)+ggtitle(\"PACF Plot for Daily Vaccinations per Million\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nd_vacc_pacf1 &lt;- ggPacf(diff(d_vacc_ts))+ggtitle(\"PACF Plot for Differented Daily Vaccinations per Million\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(d_vacc_acf, d_vacc_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(d_vacc_acf1, d_vacc_pacf1, nrow=2)\n\n\n\n\n\nCode\np_vacc_acf &lt;- ggAcf(p_vacc_ts)+ggtitle(\"ACF Plot for People Vaccinated per Hundred\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \np_vacc_acf1 &lt;- ggAcf(diff(p_vacc_ts))+ggtitle(\"ACF Plot for Differented People Vaccinated per Hundred\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \np_vacc_pacf &lt;- ggPacf(p_vacc_ts)+ggtitle(\"PACF Plot for People Vaccinated per Hundred\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \np_vacc_pacf1 &lt;- ggPacf(diff(p_vacc_ts))+ggtitle(\"PACF Plot for Differented People Vaccinated per Hundred\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(p_vacc_acf, p_vacc_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(p_vacc_acf1, p_vacc_pacf1, nrow=2)\n\n\n\n\n\nCode\npf_vacc_acf &lt;- ggAcf(pf_vacc_ts)+ggtitle(\"ACF Plot for People Fully Vaccinated per Hundred\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \npf_vacc_acf1 &lt;- ggAcf(diff(pf_vacc_ts))+ggtitle(\"ACF Plot for Differented People Fully Vaccinated per Hundred\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \npf_vacc_pacf &lt;- ggPacf(pf_vacc_ts)+ggtitle(\"PACF Plot for People Fully Vaccinated per Hundred\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \npf_vacc_pacf1 &lt;- ggPacf(diff(pf_vacc_ts))+ggtitle(\"PACF Plot for Differented People Fully Vaccinated per Hundred\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\")\ngrid.arrange(pf_vacc_acf, pf_vacc_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(pf_vacc_acf1, pf_vacc_pacf1, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\ncase_acf &lt;- ggAcf(case_ts)+ggtitle(\"ACF Plot for Newly Confirmed Cases\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ncase_acf1 &lt;- ggAcf(diff(case_ts))+ggtitle(\"ACF Plot for Differented Newly Confirmed Cases\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ncase_pacf &lt;- ggPacf(case_ts)+ggtitle(\"PACF Plot for Newly Confirmed Cases\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ncase_pacf1 &lt;- ggPacf(diff(case_ts))+ggtitle(\"PACF Plot for Differented Newly Confirmed Cases\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(case_acf, case_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(case_acf1, case_pacf1, nrow=2)\n\n\n\n\n\nCode\ndead_acf &lt;- ggAcf(dead_ts)+ggtitle(\"ACF Plot for Dead Cases\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ndead_acf1 &lt;- ggAcf(diff(dead_ts))+ggtitle(\"ACF Plot for Differented Dead Cases\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ndead_pacf &lt;- ggPacf(dead_ts)+ggtitle(\"PACF Plot for Dead Cases\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ndead_pacf1 &lt;- ggPacf(diff(dead_ts))+ggtitle(\"PACF Plot for Differented Dead Cases\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(dead_acf, dead_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(dead_acf1, dead_pacf1, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nhos1_acf &lt;- ggAcf(hos_ts1)+ggtitle(\"ACF Plot for Number of Inpatient Beds\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos1_acf1 &lt;- ggAcf(diff(hos_ts1))+ggtitle(\"ACF Plot for Differented Number of Inpatient Beds\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos1_pacf &lt;- ggPacf(hos_ts1)+ggtitle(\"PACF Plot for Number of Inpatient Beds\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos1_pacf1 &lt;- ggPacf(diff(hos_ts1))+ggtitle(\"PACF Plot for Differented Number of Inpatient Beds\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(hos1_acf, hos1_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(hos1_acf1, hos1_pacf1, nrow=2)\n\n\n\n\n\nCode\nhos2_acf &lt;- ggAcf(hos_ts2)+ggtitle(\"ACF Plot for Number of Inpatient Beds Used for COVID\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos2_acf1 &lt;- ggAcf(diff(hos_ts2))+ggtitle(\"ACF Plot for Differented Number of Inpatient Beds Used for COVID\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos2_pacf &lt;- ggPacf(hos_ts2)+ggtitle(\"PACF Plot for Number of Inpatient Beds Used for COVID\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos2_pacf1 &lt;- ggPacf(diff(hos_ts2))+ggtitle(\"PACF Plot for Differented Number of Inpatient Beds Used for COVID\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(hos2_acf, hos2_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(hos2_acf1, hos2_pacf1, nrow=2)\n\n\n\n\n\nCode\nhos3_acf &lt;- ggAcf(hos_ts3)+ggtitle(\"ACF Plot for Utilization Rate of Inpatient Beds for COVID\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos3_acf1 &lt;- ggAcf(diff(hos_ts3))+ggtitle(\"ACF Plot for Differented Utilization Rate of Inpatient Beds for COVID\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos3_pacf &lt;- ggPacf(hos_ts3)+ggtitle(\"PACF Plot for Utilization Rate of Inpatient Beds for COVID\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nhos3_pacf1 &lt;- ggPacf(diff(hos_ts3))+ggtitle(\"PACF Plot for Differented Utilization Rate of Inpatient Beds for COVID\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(hos3_acf, hos3_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(hos3_acf1, hos3_pacf1, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nemp_acf &lt;- ggAcf(unemploy_ts)+ggtitle(\"ACF Plot for Unemployment Rate\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nemp_acf1 &lt;- ggAcf(diff(unemploy_ts))+ggtitle(\"ACF Plot for Differented Unemployment Rate\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nemp_pacf &lt;- ggPacf(unemploy_ts)+ggtitle(\"PACF Plot for Unemployment Rate\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nemp_pacf1 &lt;- ggPacf(diff(unemploy_ts))+ggtitle(\"PACF Plot for Differented Unemployment Rate\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(emp_acf, emp_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(emp_acf1, emp_pacf1, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nstock_acf &lt;- ggAcf(stock_ts)+ggtitle(\"ACF Plot for Pfizer Stock Price\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nstock_acf1 &lt;- ggAcf(diff(stock_ts))+ggtitle(\"ACF Plot for Differented Pfizer Stock Price\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nstock_pacf &lt;- ggPacf(stock_ts)+ggtitle(\"PACF Plot for Pfizer Stock Price\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nstock_pacf1 &lt;- ggPacf(diff(stock_ts))+ggtitle(\"PACF Plot for Differented Pfizer Stock Price\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(stock_acf, stock_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(stock_acf1, stock_pacf1, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\ndemo_acf &lt;- ggAcf(demo_ts)+ggtitle(\"ACF Plot for Support Rate for Democratic\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ndemo_acf1 &lt;- ggAcf(diff(demo_ts))+ggtitle(\"ACF Plot for Differented Support Rate for Democratic\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ndemo_pacf &lt;- ggPacf(demo_ts)+ggtitle(\"PACF Plot for Support Rate for Democratic\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ndemo_pacf1 &lt;- ggPacf(diff(demo_ts))+ggtitle(\"PACF Plot for Differented Support Rate for Democratic\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(demo_acf, demo_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(demo_acf1, demo_pacf1, nrow=2)\n\n\n\n\n\nCode\ninde_acf &lt;- ggAcf(inde_ts)+ggtitle(\"ACF Plot for Support Rate for Independent\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ninde_acf1 &lt;- ggAcf(diff(inde_ts))+ggtitle(\"ACF Plot for Differented Support Rate for Independent\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ninde_pacf &lt;- ggPacf(inde_ts)+ggtitle(\"PACF Plot for Support Rate for Independent\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ninde_pacf1 &lt;- ggPacf(diff(inde_ts))+ggtitle(\"PACF Plot for Differented Support Rate for Independent\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\")\ngrid.arrange(inde_acf, inde_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(inde_acf1, inde_pacf1, nrow=2)\n\n\n\n\n\nCode\nrep_acf &lt;- ggAcf(rep_ts)+ggtitle(\"ACF Plot for Support Rate for Republican\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nrep_acf1 &lt;- ggAcf(diff(rep_ts))+ggtitle(\"ACF Plot for Differented Support Rate for Republican\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nrep_pacf &lt;- ggPacf(rep_ts)+ggtitle(\"PACF Plot for Support Rate for Republican\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \nrep_pacf1 &lt;- ggPacf(diff(rep_ts))+ggtitle(\"PACF Plot for Differented Support Rate for Republican\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#5a3196\") +\n    geom_hline(yintercept = 0, color = \"#5a3196\") \ngrid.arrange(rep_acf, rep_pacf, nrow=2)\n\n\n\n\n\nCode\ngrid.arrange(rep_acf1, rep_pacf1, nrow=2)\n\n\n\n\n\n\n\n\nNumber of Daily Vaccinations Per Million: ACF Plot has significant lags at 1 and 2 so p = 1, 2. PACF Plot has significant lags at 1 and 2 so q = 1, 2. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nNumber of People Vaccinated Per Hundred: ACF Plot has significant lags at 1-3 so p = 1, 2, 3. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line.Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nNumber of People Fully Vaccinated Per Hundred: ACF Plot has significant lags at 1-3 so p = 1, 2, 3. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nNumber of Newly Confirmed Cases: ACF Plot has significant lags at 1-10 so p = 1, 2, 3，4, 5, 6, 7, 8, 9, 10. However, in general we care about only the first couple of lags, in this case the first 3. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line.Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nNumber of Death Cases: ACF Plot has significant lags at 1-10 so p = 1, 2, 3，4, 5, 6, 7, 8, 9, 10. However, in general we care about only the first couple of lags, in this case the first 3. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nNumber of Inpatient Beds: ACF Plot has significant lags at 1 and 2 so p = 1, 2. PACF Plot has significant lags at 1 and 4 so q = 1, 4. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line.Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nNumber of Inpatient Beds Used for COVID: ACF Plot has significant lags at 1 and 2 so p = 1, 2. PACF Plot has significant lags at 1 and 2 so q = 1, 2. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nUtilization Rate for Inpatient Beds Used for COVID: ACF Plot has significant lags at 1 so p = 1. PACF Plot has significant lags at 1-3 so q = 1, 2, 3. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nUnemployment Rate: ACF Plot has significant lags at 1 so p = 1. PACF Plot has significant lags at 1 so q = 1. Regarding stationarity, the ACF plot reveals autocorrelation values surpassing the threshold represented by the dashed line. Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nPfizer Stock Price:  The ACF and PACF plots provide critical insights for determining the parameters of our time series model. The ACF plot shows a significant lag at 1-10, suggesting a p-value of 1-10 for the AR component. Similarly, the PACF plot shows a significant lag at 1, indicating a q-value of 1 for the MA component. Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\nSupport Rate for Democratic: There are no lags over the dashed line in the ACF plot, which indicates that there is no significant autocorrelation in the series beyond the lag indicated by the highest peak. In this cases, the ACF plot suggests that there is no systematic relationship between the observations at different time points. This lack of autocorrelation implies that the series is likely stationary, as there is no discernible pattern of dependence between consecutive observations.\nSupport Rate for Independent: There are no lags over the dashed line in the ACF plot, which indicates that there is no significant autocorrelation in the series beyond the lag indicated by the highest peak. In this cases, the ACF plot suggests that there is no systematic relationship between the observations at different time points. This lack of autocorrelation implies that the series is likely stationary, as there is no discernible pattern of dependence between consecutive observations.\nSupport Rate for Republican: The ACF and PACF plots provide critical insights for determining the parameters of our time series model. The ACF plot shows a significant lag at 1, suggesting a p-value of 1 for the AR component. Similarly, the PACF plot shows a significant lag at 1, indicating a q-value of 1 for the MA component. Additionally, the presence of autocorrelation values above the threshold in the ACF plot indicates stationarity; this suggests that the time series has consistent, predictable patterns over time, confirmed by significant autocorrelation at multiple lags. This stationary behavior is crucial for the effective modeling and forecasting of the series.\n\n\n2. Dickey-Fuller Test\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID-19 Hospitalization NumberEconomic Indicators (Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\ntseries::adf.test(d_vacc_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  d_vacc_ts\nDickey-Fuller = -4.1125, Lag order = 3, p-value = 0.0183\nalternative hypothesis: stationary\n\n\nCode\np_vacc_ts1 &lt;- na.omit(p_vacc_ts)\ntseries::adf.test(p_vacc_ts1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  p_vacc_ts1\nDickey-Fuller = -3.3701, Lag order = 3, p-value = 0.08067\nalternative hypothesis: stationary\n\n\nCode\npf_vacc_ts1 &lt;- na.omit(pf_vacc_ts)\ntseries::adf.test(pf_vacc_ts1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  pf_vacc_ts1\nDickey-Fuller = -7.4927, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(case_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  case_ts\nDickey-Fuller = -1.8178, Lag order = 3, p-value = 0.6457\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(dead_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dead_ts\nDickey-Fuller = -0.54605, Lag order = 3, p-value = 0.9754\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(hos_ts1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  hos_ts1\nDickey-Fuller = -12.058, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(hos_ts2)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  hos_ts2\nDickey-Fuller = -2.9428, Lag order = 3, p-value = 0.1968\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(hos_ts3)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  hos_ts3\nDickey-Fuller = -2.8936, Lag order = 3, p-value = 0.2165\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(unemploy_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  unemploy_ts\nDickey-Fuller = -8.8759, Lag order = 2, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(stock_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  stock_ts\nDickey-Fuller = -3.0918, Lag order = 3, p-value = 0.1381\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(demo_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  demo_ts\nDickey-Fuller = -3.4688, Lag order = 3, p-value = 0.05607\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(inde_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  inde_ts\nDickey-Fuller = -4.2027, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(rep_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  rep_ts\nDickey-Fuller = -3.4937, Lag order = 3, p-value = 0.05222\nalternative hypothesis: stationary\n\n\n\n\n\nIn our project, we delve into an array of statistical series to discern patterns and ascertain stationarity, crucial for understanding sentiment impacts on the stock prices of leading tech companies and broader socio-economic indicators. Our methodology employs rigorous statistical tests, complemented by Autocorrelation Function (ACF) plots, to scrutinize the data’s behavior over time.\nNumber of Daily Vaccinations Per Million: A p-value below 0.05 signals sufficient grounds to reject the null hypothesis at a 5% significance level, indicating stationarity in our series. This finding, however, contrasts with prior conclusions, suggesting the ACF plot’s superior accuracy, which points toward non-stationarity.\nNumber of People Vaccinated Per Hundred: The p-value, exceeding 0.05, reveals an insufficient basis to reject the null hypothesis, indicating a non-stationary series. This necessitates further modifications for stationarity, reinforcing conclusions from earlier analyses, including a significant lag order of 3.\nNumber of People Fully Vaccinated Per Hundred: With a p-value below 0.05, we find adequate evidence to reject the null hypothesis, suggesting stationarity. Yet, this contradicts previous findings, with the ACF plot indicating non-stationarity, challenging our initial conclusion.\nNumber of Newly Confirmed Cases: A p-value above 0.05 indicates a lack of sufficient evidence to dismiss the null hypothesis, suggesting non-stationarity. This aligns with earlier observations, necessitating adjustments for stationarity, including a noted lag order of 3.\nNumber of Death Cases: The p-value, again above 0.05, underscores a lack of adequate evidence to reject the null hypothesis, signaling a non-stationary series and the need for further data adjustments. This finding is consistent with prior analyses.\nNumber of Inpatient Beds: Here, a p-value below 0.05 provides enough justification to reject the null hypothesis, suggesting a stationary series. Nevertheless, this result is at odds with previous analyses, indicating non-stationarity based on the ACF plot.\nNumber of Inpatient Beds Used for COVID: The p-value surpassing 0.05 suggests insufficient evidence to reject the null hypothesis, pointing to a non-stationary series that requires adjustments, corroborating earlier findings and the significance of a lag order of 3.\nUtilization Rate for Inpatient Beds Used for COVID: A high p-value indicates the series’ non-stationarity, echoing the need for adjustments to achieve stationarity and supporting earlier conclusions, including a lag order of 3.\nUnemployment Rate: A low p-value indicates sufficient evidence to reject the null hypothesis, suggesting stationarity. However, this contrasts with previous examples, with the ACF plot indicating non-stationarity.\nPfizer Stock Price:  With a p-value exceeding 0.05, there’s insufficient evidence to reject the null hypothesis, indicating a non-stationary series requiring adjustments, consistent with earlier findings, including a lag order of 3.\nSupport Rate for Democratic: A high p-value reveals a lack of evidence to reject the null hypothesis, suggesting non-stationarity and the need for adjustments, contradicting earlier conclusions of stationarity.\nSupport Rate for Independent: A low p-value provides ample evidence to reject the null hypothesis, indicating a stationary series. This finding aligns with prior conclusions, affirming the series’ stationarity.\nSupport Rate for Republican: The p-value, exceeding 0.05, indicates insufficient evidence to reject the null hypothesis, suggesting a non-stationary series that necessitates adjustments, in line with earlier analyses.\nThrough this detailed exploration, we meticulously gauge the stationarity of diverse series, juxtaposing statistical test results against ACF plot insights to draw nuanced conclusions on the dynamic interplay between sentiment, stock price movements, and broader socio-economic indicators.\n\n\n3. Detrend VS Difference\n\nVaccination RateNewly Confirmed Cases & Death CasesCOVID-19 Hospitalization NumberEconomic Indicators (Unemployment Rate)Medical Corporation (Pfizer) Stock PriceParty Support Rate\n\n\n\n\nCode\nfit_d_vacc = lm(d_vacc_ts~time(d_vacc_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_d_vacc), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(d_vacc_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_p_vacc = lm(p_vacc_ts1~time(p_vacc_ts1), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_p_vacc), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(p_vacc_ts1), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_pf_vacc = lm(pf_vacc_ts1~time(pf_vacc_ts1), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_pf_vacc), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(pf_vacc_ts1), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_case = lm(case_ts~time(case_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_case), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(case_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_dead = lm(dead_ts~time(dead_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_dead), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(dead_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_hos1 = lm(hos_ts1~time(hos_ts1), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_hos1), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(hos_ts1), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_hos2 = lm(hos_ts2~time(hos_ts2), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_hos2), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(hos_ts2), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_hos3 = lm(hos_ts3~time(hos_ts3), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_hos3), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(hos_ts3), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_emp = lm(unemploy_ts~time(unemploy_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_emp), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(unemploy_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_stock = lm(stock_ts~time(stock_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_stock), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(stock_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_demo = lm(demo_ts~time(demo_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_demo), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(demo_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_inde = lm(inde_ts~time(inde_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_inde), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(inde_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nCode\nfit_rep = lm(rep_ts~time(rep_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_rep), main=\"Detrended\", colour = \"#5a3196\") + theme_bw()\nplot2&lt;-autoplot(diff(rep_ts), main=\"First Difference\", colour = \"#5a3196\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\nDetrending and differencing stand as pivotal techniques in the realm of time series analysis, each aimed at achieving the crucial condition of stationarity within a dataset. While navigating the same goal of trend elimination, these methodologies diverge in their approach and application nuances.\nDetrending is a targeted process aimed squarely at eradicating the underlying trend from the dataset. This is accomplished by first meticulously estimating the trend component that permeates the time series and then subtracting this estimated trend from the original dataset. The outcome is a transformed series where the original mean has been adjusted to center around zero, effectively neutralizing the trend influence. However, this transformation is not a panacea; detrended data can still exhibit non-stationary characteristics, such as seasonality or variance instabilities, that require further intervention.\nConversely, differencing operates under a broader scope, addressing stationarity by focusing on the differences between consecutive observations. This method is encapsulated by the formula:\n\\[\\Delta y_t = y_t - y_{t-1}\\]\nwhere \\(\\Delta y_t\\) represents the difference between the current observation \\(y_t\\) and its predecessor \\(y_{t-1}\\). Through this simple yet effective mechanism, differencing excels at mitigating linear trends and highlighting the dynamic changes between data points. Its strength lies particularly in contexts where the time series displays a consistent directional trend, making it a robust choice for such scenarios.\nHowever, it’s worth noting that while differencing is adept at ironing out linear trends, it may falter when faced with nonlinear trends or pronounced seasonal fluctuations. The essence of differencing lies in its ability to simplify the series to a form where patterns and structures become more discernable, albeit at the potential cost of oversimplification in certain complex scenarios.\nThe decision to employ detrending or differencing hinges on a thorough examination of the time series at hand. The specific characteristics of the dataset, including the nature of its trends and seasonalities, dictate the most appropriate method for achieving stationarity. This choice is not merely technical but strategic, laying the foundation for deeper insights and more accurate forecasts in the pursuit of time series analysis.\n\n\n4. ARIMA(p,d,q)\nIn this section, our aim is to identify all potential values for the autoregressive (AR) parameter (p), the moving average (MA) parameter (q), and the differencing parameter (d) based on the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots of the original data.\nTo determine the value of p, we examine the most significant lags from the PACF plot, which helps us identify the lag orders where the correlation is not accounted for by previous lags.\nConversely, for the value of q, we focus on the most significant lags from the ACF plot, indicating the correlation between observations at different time lags, which informs us about the lag orders that may require inclusion in the moving average model.\nGiven that we have differenced all series once, the d value is consistently set to 1. However, when evaluating the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for model selection, we explore both d=0 and d=1 to ensure comprehensive assessment and comparison of model performance.\nDaily vaccinations time series: - q = 0,1,2 - d = 0,1 - p = 0,1,2\nPeople vaccinated time series: - q = 0,1,2,3 - d = 0,1 - p = 0,1\nPeople fully vaccinated time series: - q = 0,1,2,3 - d = 0,1 - p = 0,1\nNewly confirmed case time series: - q = 0,1,2,3,4,5,6,7,8,9,10 - d = 0,1 - p = 0,1\nDeath case time series: - q = 0,1,2,3,4,5,6,7,8,9,10 - d = 0,1 - p = 0,1\nInpatient bed time series: - q = 0,1,2 - d = 0,1 - p = 0,1\nInpatient bed used for COVID time series: - q = 0,1,2 - d = 0,1 - p = 0,1,2\nUtilization rate for inpatient bed used for COVID time series: - q = 0,1 - d = 0,1 - p = 0,1,2,3\nUnemployment rate time series: - q = 0,1 - d = 0,1 - p = 0,1\nPfizer stock price time series: - q = 0,1,2,3,4,5,6,7,8,9,10 - d = 0,1 - p = 0,1\nSupport rate for democratic time series: - q = 0,1 - d = 0,1 - p = 0,1\nSupport rate for independent time series: - q = 0,1 - d = 0,1 - p = 0,1\nSupport rate for republican time series: - q = 0,1 - d = 0,1 - p = 0,1\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesNewly confirmed case time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesUnemployment rate time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*18),nrow=18) \n\n\nfor (p in 0:2)\n{\n  for(q in 0:2)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(d_vacc_ts),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n738.4524\n742.5543\n739.4124\n\n\n0\n1\n0\n708.6048\n711.2692\n709.0848\n\n\n0\n0\n1\n729.7130\n735.1822\n731.3797\n\n\n0\n1\n1\n710.4991\n714.4957\n711.4991\n\n\n0\n0\n2\n727.4084\n734.2449\n730.0171\n\n\n0\n1\n2\n708.3318\n713.6606\n710.0709\n\n\n1\n0\n0\n730.5130\n735.9822\n732.1797\n\n\n1\n1\n0\n710.5352\n714.5318\n711.5352\n\n\n1\n0\n1\n730.2149\n737.0513\n732.8236\n\n\n1\n1\n1\n712.4472\n717.7760\n714.1863\n\n\n1\n0\n2\n730.9408\n739.1446\n734.7590\n\n\n1\n1\n2\n708.3072\n714.9682\n711.0345\n\n\n2\n0\n0\n727.3140\n734.1505\n729.9227\n\n\n2\n1\n0\n711.4293\n716.7581\n713.1684\n\n\n2\n0\n1\n718.6656\n726.8694\n722.4838\n\n\n2\n1\n1\n706.1272\n712.7882\n708.8544\n\n\n2\n0\n2\n730.3437\n739.9148\n735.6771\n\n\n2\n1\n2\n701.6110\n709.6042\n705.6110\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n   p d q     AIC      BIC    AICc\n18 2 1 2 701.611 709.6042 705.611\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n   p d q     AIC      BIC    AICc\n18 2 1 2 701.611 709.6042 705.611\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n   p d q     AIC      BIC    AICc\n18 2 1 2 701.611 709.6042 705.611\n\n\nThe model with the lowest AIC, BIC, AICc is ARIMA(2,1,2). So the best model is ARIMA(2,1,2).\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*16),nrow=16) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:3)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(p_vacc_ts),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n128.2556\n132.2522\n129.2556\n\n\n0\n1\n0\n111.3475\n113.9392\n111.8475\n\n\n0\n0\n1\n111.4001\n116.7289\n113.1393\n\n\n0\n1\n1\n108.9110\n112.7985\n109.9545\n\n\n0\n0\n2\n103.5130\n110.1741\n106.2403\n\n\n0\n1\n2\n108.8841\n114.0674\n110.7023\n\n\n0\n0\n3\n105.4507\n113.4439\n109.4507\n\n\n0\n1\n3\n102.3835\n108.8627\n105.2407\n\n\n1\n0\n0\n114.2362\n119.5650\n115.9753\n\n\n1\n1\n0\n111.2512\n115.1387\n112.2947\n\n\n1\n0\n1\n109.0870\n115.7480\n111.8143\n\n\n1\n1\n1\n110.8864\n116.0697\n112.7046\n\n\n1\n0\n2\n105.4663\n113.4596\n109.4663\n\n\n1\n1\n2\n107.7595\n114.2387\n110.6167\n\n\n1\n0\n3\n105.7102\n115.0357\n111.3102\n\n\n1\n1\n3\n104.2137\n111.9888\n108.4137\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n8 0 1 3 102.3835 108.8627 105.2407\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n8 0 1 3 102.3835 108.8627 105.2407\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n8 0 1 3 102.3835 108.8627 105.2407\n\n\nThe model with the lowest AIC, BIC, AICc is ARIMA(0,1,3). So the best model is ARIMA(0,1,3).\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*16),nrow=16) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:3)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(pf_vacc_ts),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n126.67506\n130.6717\n127.67506\n\n\n0\n1\n0\n106.80977\n109.4014\n107.30977\n\n\n0\n0\n1\n107.40914\n112.7380\n109.14827\n\n\n0\n1\n1\n99.34580\n103.2333\n100.38927\n\n\n0\n0\n2\n96.18947\n102.8505\n98.91674\n\n\n0\n1\n2\n97.58093\n102.7643\n99.39911\n\n\n0\n0\n3\n95.60878\n103.6020\n99.60878\n\n\n0\n1\n3\n95.34163\n101.8208\n98.19878\n\n\n1\n0\n0\n110.72871\n116.0575\n112.46784\n\n\n1\n1\n0\n101.27973\n105.1672\n102.32320\n\n\n1\n0\n1\n101.89615\n108.5572\n104.62342\n\n\n1\n1\n1\n100.28735\n105.4707\n102.10553\n\n\n1\n0\n2\n96.71949\n104.7127\n100.71949\n\n\n1\n1\n2\n99.41388\n105.8931\n102.27102\n\n\n1\n0\n3\n97.32260\n106.6480\n102.92260\n\n\n1\n1\n3\n95.30117\n103.0762\n99.50117\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n16 1 1 3 95.30117 103.0762 99.50117\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n8 0 1 3 95.34163 101.8208 98.19878\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n8 0 1 3 95.34163 101.8208 98.19878\n\n\nThe model with the lowest BIC, AICc is ARIMA(0,1,3). While the model with the lowest AIC is ARIMA(1,1,3), the significantly lower BIC and AICc values of ARIMA(0,1,3) underscore its stronger performance. Therefore, based on the evaluation metrics, ARIMA(0,1,3) emerges as the optimal model.\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*40),nrow=40) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:10)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(case_ts),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n1372.358\n1377.571\n1372.989\n\n\n0\n1\n0\n1329.589\n1333.016\n1329.904\n\n\n0\n0\n1\n1346.930\n1353.881\n1348.011\n\n\n0\n1\n1\n1328.008\n1333.149\n1328.657\n\n\n0\n0\n2\n1346.896\n1355.584\n1348.562\n\n\n0\n1\n2\n1317.681\n1324.535\n1318.792\n\n\n0\n0\n3\n1348.882\n1359.308\n1351.282\n\n\n0\n1\n3\n1318.245\n1326.813\n1319.959\n\n\n0\n0\n4\n1350.638\n1362.801\n1353.932\n\n\n0\n1\n4\n1320.171\n1330.452\n1322.642\n\n\n0\n0\n5\n1352.273\n1366.174\n1356.637\n\n\n0\n1\n5\n1322.083\n1334.078\n1325.477\n\n\n0\n0\n6\n1353.981\n1369.620\n1359.606\n\n\n0\n1\n6\n1323.493\n1337.202\n1327.993\n\n\n0\n0\n7\n1355.958\n1373.334\n1363.054\n\n\n0\n1\n7\n1324.984\n1340.406\n1330.791\n\n\n0\n0\n8\n1357.880\n1376.995\n1366.680\n\n\n0\n1\n8\n1326.827\n1343.963\n1334.161\n\n\n0\n0\n9\n1359.015\n1379.867\n1369.774\n\n\n0\n1\n9\n1328.814\n1347.664\n1337.918\n\n\n0\n0\n10\n1359.727\n1382.317\n1372.727\n\n\n1\n0\n0\n1356.483\n1363.434\n1357.564\n\n\n1\n1\n0\n1331.098\n1336.238\n1331.746\n\n\n1\n0\n1\n1347.042\n1355.730\n1348.708\n\n\n1\n1\n1\n1326.755\n1333.610\n1327.866\n\n\n1\n0\n2\n1348.887\n1359.313\n1351.287\n\n\n1\n1\n2\n1318.278\n1326.846\n1319.992\n\n\n1\n0\n3\n1350.894\n1363.057\n1354.188\n\n\n1\n1\n3\n1320.189\n1330.471\n1322.660\n\n\n1\n0\n4\n1352.529\n1366.430\n1356.892\n\n\n1\n1\n4\n1321.988\n1333.983\n1325.382\n\n\n1\n0\n5\n1354.097\n1369.736\n1359.722\n\n\n1\n1\n5\n1323.990\n1337.699\n1328.490\n\n\n1\n0\n6\n1355.970\n1373.347\n1363.067\n\n\n1\n1\n6\n1325.066\n1340.489\n1330.873\n\n\n1\n0\n7\n1356.421\n1375.536\n1365.221\n\n\n1\n1\n7\n1326.889\n1344.025\n1334.222\n\n\n1\n0\n8\n1357.666\n1378.518\n1368.425\n\n\n1\n1\n8\n1328.824\n1347.673\n1337.927\n\n\n1\n0\n9\n1359.205\n1381.794\n1372.205\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 0 1 2 1317.681 1324.535 1318.792\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 0 1 2 1317.681 1324.535 1318.792\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n6 0 1 2 1317.681 1324.535 1318.792\n\n\nThe model with the lowest AIC, BIC, AICc is ARIMA(0,1,2). So the best model is ARIMA(0,1,2).\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*40),nrow=40) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:10)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(dead_ts),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n961.2278\n966.4408\n961.8594\n\n\n0\n1\n0\n909.6730\n913.1001\n909.9887\n\n\n0\n0\n1\n922.8001\n929.7507\n923.8811\n\n\n0\n1\n1\n891.0065\n896.1472\n891.6552\n\n\n0\n0\n2\n907.6571\n916.3455\n909.3238\n\n\n0\n1\n2\n892.9500\n899.8043\n894.0611\n\n\n0\n0\n3\n906.6789\n917.1049\n909.0789\n\n\n0\n1\n3\n888.5781\n897.1459\n890.2923\n\n\n0\n0\n4\n908.6724\n920.8360\n911.9665\n\n\n0\n1\n4\n887.4925\n897.7739\n889.9630\n\n\n0\n0\n5\n904.2750\n918.1763\n908.6386\n\n\n0\n1\n5\n889.4303\n901.4253\n892.8243\n\n\n0\n0\n6\n906.0467\n921.6857\n911.6717\n\n\n0\n1\n6\n885.7971\n899.5057\n890.2971\n\n\n0\n0\n7\n907.9810\n925.3577\n915.0777\n\n\n0\n1\n7\n887.2764\n902.6986\n893.0829\n\n\n0\n0\n8\n907.8380\n926.9523\n916.6380\n\n\n0\n1\n8\n889.0024\n906.1381\n896.3357\n\n\n0\n0\n9\n908.6709\n929.5230\n919.4296\n\n\n0\n1\n9\n889.1873\n908.0365\n898.2907\n\n\n0\n0\n10\n909.5623\n932.1520\n922.5623\n\n\n1\n0\n0\n931.3810\n938.3317\n932.4621\n\n\n1\n1\n0\n905.2606\n910.4013\n905.9092\n\n\n1\n0\n1\n908.9242\n917.6126\n910.5909\n\n\n1\n1\n1\n892.9864\n899.8407\n894.0975\n\n\n1\n0\n2\n907.2712\n917.6972\n909.6712\n\n\n1\n1\n2\n889.2626\n897.8304\n890.9768\n\n\n1\n0\n3\n908.6759\n920.8396\n911.9700\n\n\n1\n1\n3\n887.9449\n898.2263\n890.4155\n\n\n1\n0\n4\n908.7772\n922.6785\n913.1408\n\n\n1\n1\n4\n889.4598\n901.4548\n892.8537\n\n\n1\n0\n5\n910.2747\n925.9138\n915.8997\n\n\n1\n1\n5\n888.9612\n902.6698\n893.4612\n\n\n1\n0\n6\n911.7520\n929.1287\n918.8487\n\n\n1\n1\n6\n887.4447\n902.8669\n893.2512\n\n\n1\n0\n7\n910.3910\n929.5053\n919.1910\n\n\n1\n1\n7\n889.1595\n906.2952\n896.4928\n\n\n1\n0\n8\n909.0411\n929.8931\n919.7997\n\n\n1\n1\n8\n892.1919\n911.0412\n901.2953\n\n\n1\n0\n9\n910.6741\n933.2638\n923.6741\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n14 0 1 6 885.7971 899.5057 890.2971\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n4 0 1 1 891.0065 896.1472 891.6552\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n   p d q      AIC      BIC    AICc\n10 0 1 4 887.4925 897.7739 889.963\n\n\nAmong the ARIMA models tested, ARIMA(0,1,6) has the lowest AIC value, indicating its superior fit compared to the other models in terms of goodness of fit and complexity. Conversely, ARIMA(0,1,1) boasts the lowest BIC, while ARIMA(0,1,4) exhibits the lowest AICc. Despite these distinctions, a comprehensive evaluation considering all metrics suggests that ARIMA(0,1,6) is the optimal choice, as it strikes a balance between model complexity and performance. Therefore, ARIMA(0,1,6) emerges as the preferred model based on a holistic assessment of AIC, BIC, and AICc values.\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*12),nrow=12) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:2)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(hos_ts1),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n1201.369\n1206.983\n1201.915\n\n\n0\n1\n0\n1194.581\n1198.281\n1194.853\n\n\n0\n0\n1\n1200.550\n1208.034\n1201.480\n\n\n0\n1\n1\n1180.010\n1185.561\n1180.568\n\n\n0\n0\n2\n1199.638\n1208.994\n1201.067\n\n\n0\n1\n2\n1180.156\n1187.556\n1181.108\n\n\n1\n0\n0\n1200.572\n1208.057\n1201.502\n\n\n1\n1\n0\n1191.430\n1196.981\n1191.988\n\n\n1\n0\n1\n1202.437\n1211.793\n1203.866\n\n\n1\n1\n1\n1180.024\n1187.424\n1180.976\n\n\n1\n0\n2\n1183.551\n1194.779\n1185.600\n\n\n1\n1\n2\n1181.936\n1191.187\n1183.399\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n  p d q     AIC      BIC     AICc\n4 0 1 1 1180.01 1185.561 1180.568\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q     AIC      BIC     AICc\n4 0 1 1 1180.01 1185.561 1180.568\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n  p d q     AIC      BIC     AICc\n4 0 1 1 1180.01 1185.561 1180.568\n\n\nThe model with the lowest AIC, BIC, AICc is ARIMA(0,1,1). So the best model is ARIMA(0,1,1).\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*18),nrow=18) \n\n\nfor (p in 0:2)\n{\n  for(q in 0:2)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(hos_ts2),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n1111.642\n1117.255\n1112.187\n\n\n0\n1\n0\n1111.820\n1115.521\n1112.093\n\n\n0\n0\n1\n1109.362\n1116.847\n1110.292\n\n\n0\n1\n1\n1093.468\n1099.019\n1094.027\n\n\n0\n0\n2\n1099.705\n1109.061\n1101.133\n\n\n0\n1\n2\n1090.587\n1097.988\n1091.540\n\n\n1\n0\n0\n1112.394\n1119.879\n1113.324\n\n\n1\n1\n0\n1112.938\n1118.489\n1113.497\n\n\n1\n0\n1\n1109.563\n1118.919\n1110.992\n\n\n1\n1\n1\n1093.881\n1101.282\n1094.834\n\n\n1\n0\n2\n1098.786\n1110.013\n1100.835\n\n\n1\n1\n2\n1091.077\n1100.327\n1092.540\n\n\n2\n0\n0\n1102.004\n1111.360\n1103.433\n\n\n2\n1\n0\n1102.553\n1109.954\n1103.506\n\n\n2\n0\n1\n1098.674\n1109.902\n1100.723\n\n\n2\n1\n1\n1084.503\n1093.754\n1085.966\n\n\n2\n0\n2\n1099.303\n1112.401\n1102.103\n\n\n2\n1\n2\n1085.436\n1096.537\n1087.536\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n16 2 1 1 1084.503 1093.754 1085.966\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n   p d q      AIC      BIC     AICc\n16 2 1 1 1084.503 1093.754 1085.966\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n16 2 1 1 1084.503 1093.754 1085.966\n\n\nThe model with the lowest AIC, BIC, AICc is ARIMA(2,1,1). So the best model is ARIMA(2,1,1).\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*16),nrow=16) \n\n\nfor (p in 0:3)\n{\n  for(q in 0:1)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(hos_ts3),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-207.1751\n-201.5615\n-206.6296\n\n\n0\n1\n0\n-182.0062\n-178.3059\n-181.7335\n\n\n0\n0\n1\n-212.0688\n-204.5840\n-211.1386\n\n\n0\n1\n1\n-197.8732\n-192.3228\n-197.3151\n\n\n1\n0\n0\n-207.1813\n-199.6965\n-206.2511\n\n\n1\n1\n0\n-180.4098\n-174.8594\n-179.8517\n\n\n1\n0\n1\n-211.3958\n-202.0398\n-209.9673\n\n\n1\n1\n1\n-198.3048\n-190.9042\n-197.3524\n\n\n2\n0\n0\n-219.0286\n-209.6726\n-217.6000\n\n\n2\n1\n0\n-193.7762\n-186.3756\n-192.8238\n\n\n2\n0\n1\n-217.1926\n-205.9654\n-215.1438\n\n\n2\n1\n1\n-209.0758\n-199.8250\n-207.6124\n\n\n3\n0\n0\n-217.1759\n-205.9487\n-215.1272\n\n\n3\n1\n0\n-198.2588\n-189.0080\n-196.7953\n\n\n3\n0\n1\n-215.1803\n-202.0819\n-212.3803\n\n\n3\n1\n1\n-207.1298\n-196.0290\n-205.0298\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n  p d q       AIC       BIC   AICc\n9 2 0 0 -219.0286 -209.6726 -217.6\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q       AIC       BIC   AICc\n9 2 0 0 -219.0286 -209.6726 -217.6\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n  p d q       AIC       BIC   AICc\n9 2 0 0 -219.0286 -209.6726 -217.6\n\n\nThe model with the lowest AIC, BIC, AICc is ARIMA(2,0,0). So the best model is ARIMA(2,0,0).\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*8),nrow=8) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:1)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(unemploy_ts),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-120.7451\n-116.97081\n-119.65419\n\n\n0\n1\n0\n-99.4529\n-97.01515\n-98.90744\n\n\n0\n0\n1\n-118.7475\n-113.71510\n-116.84272\n\n\n0\n1\n1\n-111.6317\n-107.97505\n-110.48882\n\n\n1\n0\n0\n-118.7467\n-113.71428\n-116.84190\n\n\n1\n1\n0\n-101.9969\n-98.34025\n-100.85402\n\n\n1\n0\n1\n-119.6972\n-113.40668\n-116.69716\n\n\n1\n1\n1\n-109.6552\n-104.77973\n-107.65524\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n  p d q       AIC       BIC      AICc\n1 0 0 0 -120.7451 -116.9708 -119.6542\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n1 0 0 0 -120.7451 -116.9708 -119.6542\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n1 0 0 0 -120.7451 -116.9708 -119.6542\n\n\nThe model with the lowest AIC, AICc is ARIMA(1,0,1). While the model with the lowest BIC is ARIMA(0,0,0), the significantly lower AIC and AICc values of ARIMA(1,0,1) underscore its stronger performance. Therefore, based on the evaluation metrics, ARIMA(1,0,1) emerges as the optimal model.\n\n\n\n\nCode\nd = 1\ni = 1\ndvacc = data.frame()\nls = matrix(rep(NA, 6*39), nrow = 39)\n\nfor (p in 0:1) {\n  for (q in 0:10) {\n    for (d in 0:1) {\n      if (p - 1 + d + q - 1 &lt;= 8) { # Usual threshold\n        tryCatch({\n          model &lt;- Arima(diff(stock_ts), order = c(p, d, q), include.drift = TRUE) \n          ls[i, ] = c(p, d, q, model$aic, model$bic, model$aicc)\n          i = i + 1\n        }, error = function(e) {\n          cat(\"Error occurred for p =\", p, \", d =\", d, \", q =\", q, \":\", conditionMessage(e), \"\\n\")\n        })\n      }\n    }\n  }\n}\n\n\nError occurred for p = 1 , d = 0 , q = 1 : non-stationary AR part from CSS \n\n\nCode\ndvacc = as.data.frame(ls)\nnames(dvacc) = c(\"p\", \"d\", \"q\", \"AIC\", \"BIC\", \"AICc\")\n\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n91.69192\n97.24237\n92.25006\n\n\n0\n1\n0\n128.69892\n132.35620\n128.97799\n\n\n0\n0\n1\n90.91191\n98.31250\n91.86429\n\n\n0\n1\n1\n94.70812\n100.19405\n95.27955\n\n\n0\n0\n2\n89.33705\n98.58779\n90.80046\n\n\n0\n1\n2\n94.43328\n101.74784\n95.40889\n\n\n0\n0\n3\n90.24920\n101.35008\n92.34920\n\n\n0\n1\n3\n92.35282\n101.49603\n93.85282\n\n\n0\n0\n4\n89.17360\n102.12464\n92.04540\n\n\n0\n1\n4\n93.67619\n104.64804\n95.83004\n\n\n0\n0\n5\n89.61851\n104.41969\n93.40798\n\n\n0\n1\n5\n95.67588\n108.47637\n98.62325\n\n\n0\n0\n6\n90.89691\n107.54824\n95.76177\n\n\n0\n1\n6\n96.20192\n110.83105\n100.09381\n\n\n0\n0\n7\n89.42175\n107.92323\n95.53286\n\n\n0\n1\n7\n97.22498\n113.68275\n102.22498\n\n\n0\n0\n8\n90.76309\n111.11471\n98.30595\n\n\n0\n1\n8\n95.06742\n113.35384\n101.35314\n\n\n0\n0\n9\n92.66751\n114.86928\n101.84398\n\n\n0\n1\n9\n96.83202\n116.94708\n104.59673\n\n\n0\n0\n10\n91.09034\n115.14226\n102.12064\n\n\n1\n0\n0\n89.34164\n96.74223\n90.29402\n\n\n1\n1\n0\n98.25570\n103.74162\n98.82712\n\n\n1\n1\n1\n95.71995\n103.03452\n96.69556\n\n\n1\n0\n2\n90.44847\n101.54935\n92.54847\n\n\n1\n1\n2\n91.32693\n100.47013\n92.82693\n\n\n1\n0\n3\n87.11988\n100.07091\n89.99168\n\n\n1\n1\n3\n93.64810\n104.61995\n95.80195\n\n\n1\n0\n4\n89.08540\n103.88658\n92.87488\n\n\n1\n1\n4\n94.19910\n106.99959\n97.14647\n\n\n1\n0\n5\n90.56824\n107.21957\n95.43310\n\n\n1\n1\n5\n96.18703\n110.81617\n100.07893\n\n\n1\n0\n6\n91.95168\n110.45316\n98.06279\n\n\n1\n1\n6\n96.74788\n113.20565\n101.74788\n\n\n1\n0\n7\n90.73691\n111.08853\n98.27976\n\n\n1\n1\n7\n98.64202\n116.92843\n104.92773\n\n\n1\n0\n8\n92.68015\n114.88192\n101.85662\n\n\n1\n1\n8\n96.65387\n116.76892\n104.41857\n\n\n1\n0\n9\n94.65167\n118.70359\n105.68197\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n27 1 0 3 87.11988 100.0709 89.99168\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n   p d q      AIC      BIC     AICc\n22 1 0 0 89.34164 96.74223 90.29402\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n27 1 0 3 87.11988 100.0709 89.99168\n\n\nThe model with the lowest AIC, AICc is ARIMA(1,0,3). While the model with the lowest BIC is ARIMA(1,0,0), the significantly lower AIC and AICc values of ARIMA(1,0,3) underscore its stronger performance. Therefore, based on the evaluation metrics, ARIMA(1,0,3) emerges as the optimal model.\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*8),nrow=8) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:1)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(demo_ts),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-201.1013\n-195.4877\n-200.5559\n\n\n0\n1\n0\n-146.0575\n-142.3572\n-145.7848\n\n\n0\n0\n1\n-227.4413\n-219.9565\n-226.5110\n\n\n0\n1\n1\n-191.9260\n-186.3755\n-191.3678\n\n\n1\n0\n0\n-215.5599\n-208.0751\n-214.6297\n\n\n1\n1\n0\n-181.3931\n-175.8427\n-180.8350\n\n\n1\n0\n1\n-225.4535\n-216.0975\n-224.0249\n\n\n1\n1\n1\n-205.1833\n-197.7827\n-204.2309\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n  p d q       AIC       BIC     AICc\n3 0 0 1 -227.4413 -219.9565 -226.511\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q       AIC       BIC     AICc\n3 0 0 1 -227.4413 -219.9565 -226.511\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n  p d q       AIC       BIC     AICc\n3 0 0 1 -227.4413 -219.9565 -226.511\n\n\nThe model with the lowest AIC, BIC, AICc is ARIMA(0,0,1). So the best model is ARIMA(0,0,1).\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*8),nrow=8) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:1)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(inde_ts),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-145.71805\n-140.10445\n-145.17260\n\n\n0\n1\n0\n-90.35146\n-86.65116\n-90.07873\n\n\n0\n0\n1\n-170.47709\n-162.99229\n-169.54686\n\n\n0\n1\n1\n-137.69655\n-132.14611\n-137.13841\n\n\n1\n0\n0\n-162.58903\n-155.10423\n-161.65880\n\n\n1\n1\n0\n-130.38563\n-124.83519\n-129.82749\n\n\n1\n0\n1\n-168.66260\n-159.30660\n-167.23403\n\n\n1\n1\n1\n-153.28254\n-145.88195\n-152.33015\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n  p d q       AIC       BIC      AICc\n3 0 0 1 -170.4771 -162.9923 -169.5469\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n3 0 0 1 -170.4771 -162.9923 -169.5469\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n3 0 0 1 -170.4771 -162.9923 -169.5469\n\n\nThe model with the lowest AIC, BIC, AICc is ARIMA(0,0,1). So the best model is ARIMA(0,0,1).\n\n\n\n\nCode\nd=1\ni=1\ndvacc= data.frame()\nls=matrix(rep(NA,6*8),nrow=8) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:1)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(diff(rep_ts),order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ndvacc= as.data.frame(ls)\nnames(dvacc)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#dvacc\nknitr::kable(dvacc)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-219.1727\n-213.5591\n-218.6273\n\n\n0\n1\n0\n-175.8871\n-172.1868\n-175.6144\n\n\n0\n0\n1\n-223.3828\n-215.8980\n-222.4526\n\n\n0\n1\n1\n-209.6209\n-204.0705\n-209.0628\n\n\n1\n0\n0\n-218.4539\n-210.9691\n-217.5237\n\n\n1\n1\n0\n-186.6854\n-181.1350\n-186.1273\n\n\n1\n0\n1\n-229.4175\n-220.0615\n-227.9889\n\n\n1\n1\n1\n-208.5970\n-201.1964\n-207.6446\n\n\n\n\n\nCode\ndvacc[which.min(dvacc$AIC),]\n\n\n  p d q       AIC       BIC      AICc\n7 1 0 1 -229.4175 -220.0615 -227.9889\n\n\nCode\ndvacc[which.min(dvacc$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n7 1 0 1 -229.4175 -220.0615 -227.9889\n\n\nCode\ndvacc[which.min(dvacc$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n7 1 0 1 -229.4175 -220.0615 -227.9889\n\n\nThe model with the lowest AIC, BIC, AICc is ARIMA(1,0,1). So the best model is ARIMA(1,0,1).\n\n\n\nIn terms of AIC, BIC and AICc, we always want to choose the lowest values, however, it can happen that the same model won’t have the lowest value for AIC, BIC, and AICc at the same time. In that case we favor the results from AIC as that is a better estimator for autoregressive models. In the next section we can see the results from the AIC-BIC analysis.\nFinal Selection:\n\nDaily vaccinations time series: p=2, d=1, q=2\nPeople vaccinated time series: p=0, d=1, q=3\nPeople fully vaccinated time series: p=0, d=1, q=3\nNewly confirmed case time series: p=0, d=1, q=2\nDeath case time series: p=0, d=1, q=6\nInpatient bed time series: p=0, d=1, q=1\nInpatient bed used for COVID time series: p=2, d=1, q=1\nUtilization rate for inpatient bed used for COVID time series: p=2, d=0, q=0\nUnemployment rate time series: p=1, d=0, q=1\nPfizer stock price time series: p=1, d=0, q=3\nSupport rate for democratic time series: p=0, d=0, q=1\nSupport rate for independent time series: p=0, d=0, q=1\nSupport rate for republican time series: p=1, d=0, q=1\n\n\n\n5. Fitting the best model\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesNewly confirmed case time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesUnemployment rate time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\nfit_d_vacc_AR &lt;- Arima(diff(d_vacc_ts), order=c(2, 1, 2),include.drift = TRUE) \nsummary(fit_d_vacc_AR)\n\n\nSeries: diff(d_vacc_ts) \nARIMA(2,1,2) with drift \n\nCoefficients:\n         ar1      ar2      ma1     ma2     drift\n      1.2897  -0.8477  -1.9859  0.9999   76.8714\ns.e.  0.1167   0.1099   0.2042  0.2043  466.8502\n\nsigma^2 = 2.446e+09:  log likelihood = -344.81\nAIC=701.61   AICc=705.61   BIC=709.6\n\nTraining set error measures:\n                    ME    RMSE      MAE      MPE    MAPE      MASE     ACF1\nTraining set -12609.01 44043.5 29611.64 -124.755 294.076 0.3295796 -0.17266\n\n\nThe equation for the model is: \\[x_t = 1.2897x_{t-1} - 0.8476x_{t-2} + w_t - 1.9859w_{t-1} + 0.9999w_{t-2}\\]\n\n\n\n\nCode\nfit_p_vacc_AR &lt;- Arima(diff(p_vacc_ts), order=c(0, 1, 3),include.drift = TRUE) \nsummary(fit_p_vacc_AR)\n\n\nSeries: diff(p_vacc_ts) \nARIMA(0,1,3) with drift \n\nCoefficients:\n         ma1      ma2      ma3    drift\n      0.2969  -0.2969  -1.0000  -0.2897\ns.e.  0.3100   0.2357   0.4322   0.0827\n\nsigma^2 = 1.625:  log likelihood = -46.19\nAIC=102.38   AICc=105.24   BIC=108.86\n\nTraining set error measures:\n                      ME     RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.08268841 1.155475 0.7479236 18.18607 72.88163 0.1936982\n                    ACF1\nTraining set -0.03066154\n\n\nThe equation for the model is: \\[x_t = w_t + 0.2969w_{t-1} - 0.2969w_{t-2} - 1.0000w_{t-3} \\]\n\n\n\n\nCode\nfit_pf_vacc_AR &lt;- Arima(diff(pf_vacc_ts), order=c(0, 1, 3),include.drift = TRUE) \nsummary(fit_pf_vacc_AR)\n\n\nSeries: diff(pf_vacc_ts) \nARIMA(0,1,3) with drift \n\nCoefficients:\n         ma1      ma2      ma3    drift\n      0.2578  -0.4132  -0.8446  -0.2678\ns.e.  0.1986   0.2289   0.2101   0.0722\n\nsigma^2 = 1.385:  log likelihood = -42.67\nAIC=95.34   AICc=98.2   BIC=101.82\n\nTraining set error measures:\n                     ME     RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.01757527 1.066799 0.7665792 13.17391 135.8925 0.2175191\n                  ACF1\nTraining set 0.1836963\n\n\nThe equation for the model is: \\[x_t = w_t + 0.2578w_{t-1} - 0.4132w_{t-2} - 0.8446w_{t-3} \\]\n\n\n\n\nCode\nfit_case_AR &lt;- Arima(diff(case_ts), order=c(0, 1, 2),include.drift = TRUE) \nsummary(fit_case_AR)\n\n\nSeries: diff(case_ts) \nARIMA(0,1,2) with drift \n\nCoefficients:\n          ma1      ma2       drift\n      -0.0254  -0.6635   -5440.876\ns.e.   0.1347   0.1331  112821.232\n\nsigma^2 = 4.582e+12:  log likelihood = -654.84\nAIC=1317.68   AICc=1318.79   BIC=1324.53\n\nTraining set error measures:\n                   ME    RMSE     MAE       MPE     MAPE      MASE      ACF1\nTraining set 58874.29 2036015 1092948 -54.67793 86.34503 0.4079362 0.1041066\n\n\nThe equation for the model is: \\[x_t = w_t - 0.0254w_{t-1} - 0.6635w_{t-2} \\]\n\n\n\n\nCode\nfit_dead_AR &lt;- Arima(diff(dead_ts), order=c(0, 1, 6),include.drift = TRUE) \nsummary(fit_dead_AR)\n\n\nSeries: diff(dead_ts) \nARIMA(0,1,6) with drift \n\nCoefficients:\n         ma1      ma2      ma3      ma4     ma5     ma6      drift\n      0.8533  -0.3695  -1.0757  -1.0900  0.0148  0.6672  -864.6125\ns.e.  0.2409   0.3200   0.2219   0.2703  0.2351  0.2069   362.8723\n\nsigma^2 = 84619984:  log likelihood = -434.9\nAIC=885.8   AICc=890.3   BIC=899.51\n\nTraining set error measures:\n                   ME     RMSE      MAE     MPE    MAPE      MASE        ACF1\nTraining set 1373.716 8276.587 6716.487 -24.481 66.6359 0.2869461 -0.09912142\n\n\nThe equation for the model is: \\[x_t = w_t + 0.8533w_{t-1} - 0.3695w_{t-2} - 1.0757w_{t-3} - 1.0900w_{t-4} + 0.0148w_{t-5} + 0.6672w_{t-6} \\]\n\n\n\n\nCode\nfit_hos1_AR &lt;- Arima(diff(hos_ts1), order=c(0, 1, 1),include.drift = TRUE) \nsummary(fit_hos1_AR)\n\n\nSeries: diff(hos_ts1) \nARIMA(0,1,1) with drift \n\nCoefficients:\n          ma1      drift\n      -0.8352  -2100.377\ns.e.   0.0997   1731.589\n\nsigma^2 = 4.203e+09:  log likelihood = -587.01\nAIC=1180.01   AICc=1180.57   BIC=1185.56\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE     MAPE      MASE      ACF1\nTraining set 1488.251 62773.32 28941.07 231.9019 1121.634 0.9238171 0.2046115\n\n\nThe equation for the model is: \\[x_t = w_t - 0.8352w_{t-1} \\]\n\n\n\n\nCode\nfit_hos2_AR &lt;- Arima(diff(hos_ts2), order=c(2, 1, 1),include.drift = TRUE) \nsummary(fit_hos2_AR)\n\n\nSeries: diff(hos_ts2) \nARIMA(2,1,1) with drift \n\nCoefficients:\n         ar1      ar2      ma1      drift\n      0.2460  -0.4570  -1.0000  -184.7783\ns.e.  0.1276   0.1247   0.0625   189.0308\n\nsigma^2 = 491138696:  log likelihood = -537.25\nAIC=1084.5   AICc=1085.97   BIC=1093.75\n\nTraining set error measures:\n                    ME     RMSE      MAE      MPE     MAPE      MASE\nTraining set -1297.816 20975.66 13210.73 -135.526 364.1159 0.5416423\n                    ACF1\nTraining set -0.06551829\n\n\nThe equation for the model is: \\[x_t = 0.2460x_{t-1} - 0.4570x_{t-2} + w_t - 1.0000w_{t-1} \\]\n\n\n\n\nCode\nfit_hos3_AR &lt;- Arima(diff(hos_ts3), order=c(2, 0, 0),include.drift = TRUE) \nsummary(fit_hos3_AR)\n\n\nSeries: diff(hos_ts3) \nARIMA(2,0,0) with drift \n\nCoefficients:\n        ar1      ar2  intercept   drift\n      0.305  -0.4961     0.0038  -1e-04\ns.e.  0.124   0.1220     0.0056   2e-04\n\nsigma^2 = 0.0005341:  log likelihood = 114.51\nAIC=-219.03   AICc=-217.6   BIC=-209.67\n\nTraining set error measures:\n                       ME       RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.0001589413 0.02212643 0.0145853 73.24355 106.4801 0.5439367\n                    ACF1\nTraining set -0.02088889\n\n\nThe equation for the model is: \\[x_t = 0.305x_{t-1} - 0.4961x_{t-2} \\]\n\n\n\n\nCode\nfit_unemploy_AR &lt;- Arima(diff(unemploy_ts), order=c(1, 0, 1),include.drift = TRUE) \nsummary(fit_unemploy_AR)\n\n\nSeries: diff(unemploy_ts) \nARIMA(1,0,1) with drift \n\nCoefficients:\n         ar1      ma1  intercept   drift\n      0.6683  -1.0000     0.0039  -4e-04\ns.e.  0.1782   0.1018     0.0054   6e-04\n\nsigma^2 = 0.0004398:  log likelihood = 64.85\nAIC=-119.7   AICc=-116.7   BIC=-113.41\n\nTraining set error measures:\n                      ME       RMSE         MAE  MPE MAPE      MASE      ACF1\nTraining set 0.001267183 0.01929114 0.008045807 -Inf  Inf 0.6023599 0.1298093\n\n\nThe equation for the model is: \\[x_t = -0.7636x_{t-1} + w_t + 0.8226w_{t-1} \\]\n\n\n\n\nCode\nfit_stock_AR &lt;- Arima(diff(stock_ts), order=c(1, 0, 3),include.drift = TRUE) \nsummary(fit_stock_AR)\n\n\nSeries: diff(stock_ts) \nARIMA(1,0,3) with drift \n\nCoefficients:\n         ar1      ma1     ma2      ma3  intercept    drift\n      0.5911  -1.0179  0.4384  -0.4204     0.0292  -0.0057\ns.e.  0.2132   0.2324  0.1997   0.1771     0.0608   0.0025\n\nsigma^2 = 0.3012:  log likelihood = -36.56\nAIC=87.12   AICc=89.99   BIC=100.07\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.01245506 0.5125679 0.3725768 53.53454 101.9605 0.6408168\n                    ACF1\nTraining set -0.01878968\n\n\nThe equation for the model is: \\[x_t = 0.5911x_{t-1} + w_t - 1.0179w_{t-1} + 0.4384w_{t-2} - 0.4204w_{t-3} \\]\n\n\n\n\nCode\nfit_demo_AR &lt;- Arima(diff(demo_ts), order=c(0, 0, 1),include.drift = TRUE) \nsummary(fit_demo_AR)\n\n\nSeries: diff(demo_ts) \nARIMA(0,0,1) with drift \n\nCoefficients:\n          ma1  intercept  drift\n      -1.0000     -9e-04      0\ns.e.   0.0577        NaN    NaN\n\nsigma^2 = 0.0004267:  log likelihood = 117.72\nAIC=-227.44   AICc=-226.51   BIC=-219.96\n\nTraining set error measures:\n                        ME       RMSE       MAE MPE MAPE      MASE        ACF1\nTraining set -0.0002909499 0.02000063 0.0157191 NaN  Inf 0.5052568 0.002088654\n\n\nThe equation for the model is: \\[x_t = w_t - 1.0000w_{t-1} \\]\n\n\n\n\nCode\nfit_inde_AR &lt;- Arima(diff(inde_ts), order=c(0, 0, 1),include.drift = TRUE) \nsummary(fit_inde_AR)\n\n\nSeries: diff(inde_ts) \nARIMA(0,0,1) with drift \n\nCoefficients:\n          ma1  intercept  drift\n      -1.0000     0.0011      0\ns.e.   0.0591        NaN    NaN\n\nsigma^2 = 0.001398:  log likelihood = 89.24\nAIC=-170.48   AICc=-169.55   BIC=-162.99\n\nTraining set error measures:\n                       ME       RMSE        MAE  MPE MAPE      MASE       ACF1\nTraining set -0.001342649 0.03620296 0.02893673 -Inf  Inf 0.4451804 0.06199612\n\n\nThe equation for the model is: \\[x_t = w_t - 1.0000w_{t-1} \\]\n\n\n\n\nCode\nfit_rep_AR &lt;- Arima(diff(rep_ts), order=c(1, 0, 1),include.drift = TRUE) \nsummary(fit_rep_AR)\n\n\nSeries: diff(rep_ts) \nARIMA(1,0,1) with drift \n\nCoefficients:\n         ar1      ma1  intercept  drift\n      0.4095  -1.0000      1e-04      0\ns.e.  0.1345   0.0627        NaN    NaN\n\nsigma^2 = 0.0004088:  log likelihood = 119.71\nAIC=-229.42   AICc=-227.99   BIC=-220.06\n\nTraining set error measures:\n                       ME       RMSE        MAE MPE MAPE      MASE       ACF1\nTraining set 0.0007249485 0.01935823 0.01596133 NaN  Inf 0.6245738 0.06783213\n\n\nThe equation for the model is: \\[x_t = 0.4095x_{t-1} + w_t - 1.0000w_{t-1} \\]\n\n\n\n\n\n6. Fully Model Diagnostics\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesNewly confirmed case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesUnemployment rate time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\nd_vacc_full &lt;- capture.output(sarima(diff(d_vacc_ts),2, 1, 2))\n\n\n\n\n\nCode\ncat(d_vacc_full[58:72], d_vacc_full[length(d_vacc_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate       SE t.value p.value\nar1        1.2897   0.1167 11.0487  0.0000\nar2       -0.8477   0.1099 -7.7104  0.0000\nma1       -1.9859   0.2042 -9.7273  0.0000\nma2        0.9999   0.2043  4.8941  0.0001\nconstant  76.8714 466.8502  0.1647  0.8707\n\nsigma^2 estimated as 2009108752 on 23 degrees of freedom \n \nAIC = 25.05754  AICc = 25.15494  BIC = 25.34301 \n \n \n\n\nThe Standard Residual Plot presents an encouraging picture, exhibiting characteristics of good stationarity with a relatively constant mean and variance. This stability is a positive sign for the model’s accuracy. Furthermore, the Autocorrelation Function (ACF) plot reinforces this positive assessment by showing a lack of significant correlation among the residuals, suggesting that the model has effectively captured the underlying patterns in the data, leaving behind what appears to be mere white noise. This is indicative of an exceptionally well-fitted model.\nThe Quantile-Quantile (Q-Q) Plot also leans towards a favorable evaluation, demonstrating a reasonable approximation of normality, though with some variability. This slight deviation does not detract from the overall model’s effectiveness.\nHowever, the Ljung-Box test introduces a layer of complexity with its results. Despite observing variations, the test values exceed the 0.05 threshold (aligned with a 5% significance level), implying a lack of significant autocorrelation. This outcome, coupled with all p-values falling below the 0.05 mark, further validates the model’s robustness, suggesting a commendable fit to the observed data.\n\n\n\n\nCode\np_vacc_full &lt;- capture.output(sarima(diff(p_vacc_ts),0, 1, 3))\n\n\n\n\n\nCode\ncat(p_vacc_full[21:34], p_vacc_full[length(p_vacc_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nma1        0.2969 0.3100  0.9578  0.3481\nma2       -0.2969 0.2357 -1.2599  0.2203\nma3       -1.0000 0.4322 -2.3138  0.0300\nconstant  -0.2897 0.0827 -3.5037  0.0019\n\nsigma^2 estimated as 1.38457 on 23 degrees of freedom \n \nAIC = 3.791982  AICc = 3.859322  BIC = 4.031952 \n \n \n\n\nThe Standard Residual Plot demonstrates commendable stationarity, with a consistently constant mean and variance, suggesting the model’s robustness in capturing the data’s central tendencies and spread. The Autocorrelation Function (ACF) plot further reinforces the model’s efficacy, showing negligible correlation among residuals and implying that the model has adeptly isolated and left behind only white noise. This is indicative of an exceptionally well-fitted model.\nIn the realm of normality assessment, the Quantile-Quantile (Q-Q) Plot exhibits a satisfactory alignment with normality, although there is room for improvement in mirroring the ideal normal distribution curve more closely. The Ljung-Box test results introduce a nuanced perspective, with values straddling the 0.05 (5% significance) threshold. This indicates a lack of substantial autocorrelation, underscoring the model’s aptitude in fitting the data without overlooking significant patterns.\nThe analysis of Moving Average parameters reveals a differentiated significance; while the p-values for ma1 and ma2 do not denote statistical significance, falling above the 0.05 threshold, the p-value for ma3 stands out by being less than 0.05. This suggests that while the first two parameters may not contribute significantly to the model, ma3 plays a crucial role, offering insights into the subtleties of the model’s fit and the dynamics captured by this specific parameter.\n\n\n\n\nCode\npf_vacc_full &lt;- capture.output(sarima(diff(pf_vacc_ts),0, 1, 3))\n\n\n\n\n\nCode\ncat(pf_vacc_full[20:33], pf_vacc_full[length(pf_vacc_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n         Estimate     SE t.value p.value\nma1        0.2578 0.1986  1.2980  0.2071\nma2       -0.4132 0.2289 -1.8054  0.0841\nma3       -0.8446 0.2101 -4.0202  0.0005\nconstant  -0.2678 0.0722 -3.7112  0.0011\n\nsigma^2 estimated as 1.180211 on 23 degrees of freedom \n \nAIC = 3.531172  AICc = 3.598512  BIC = 3.771141 \n \n \n\n\nThe Standard Residual Plot presents an encouraging picture of stationarity, characterized by a largely constant mean and variation, indicative of a robust model. The Autocorrelation Function (ACF) plot further bolsters this assessment, displaying an absence of correlation and suggesting that the model has effectively captured the underlying process, leaving only white noise. This is a strong marker of an excellently fitted model. While the Quantile-Quantile (Q-Q) Plot demonstrates a commendable degree of normality, minor deviations are observable, pointing towards an area for potential refinement.\nThe Ljung-Box test results introduce an element of variability, with values crossing the 0.05 (5% significance) threshold, yet this does not denote significant autocorrelation, reinforcing the model’s adequacy. Although the p-values for ma1 and ma2 slightly exceed 0.05, the p-value for ma3 falls below this mark, suggesting that while certain model parameters may edge towards marginal significance, the overall model integrity remains intact, pointing towards a well-specified model.\n\n\n\n\nCode\ncase_full &lt;- capture.output(sarima(diff(case_ts),0, 1, 2))\n\n\n\n\n\nCode\ncat(case_full[20:32], case_full[length(case_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n           Estimate          SE t.value p.value\nma1         -0.0254      0.1347 -0.1884  0.8515\nma2         -0.6635      0.1331 -4.9840  0.0000\nconstant -5440.8755 112821.2319 -0.0482  0.9618\n\nsigma^2 estimated as 4.246465e+12 on 38 degrees of freedom \n \nAIC = 32.13855  AICc = 32.15437  BIC = 32.30573 \n \n \n\n\nThe Standard Residual Plot presents a promising depiction of stationarity, characterized by a consistent mean and variation across the board. The Autocorrelation Function (ACF) plot reinforces this positive assessment, showing no discernible correlation and implying that all residual patterns have been effectively captured by the model, leaving behind only white noise. This is a strong indicator of an excellent model fit.\nFurther analysis through the Quantile-Quantile (Q-Q) Plot suggests a satisfactory alignment with normality, although there is room for slight improvement. The Ljung-Box test results introduce some variability, with values surpassing the 0.05 threshold (indicative of a 5% significance level). This outcome points to the lack of substantial correlation, further affirming the model’s aptness.\nRegarding the moving average parameters, the p-values associated with ma1 exceed the 0.05 mark, contrasting with ma2’s p-value, which falls below this threshold. This differential suggests a nuanced interplay within the model’s components, highlighting areas of both strength and potential refinement. ### Death case time series\n\n\nCode\ndead_full &lt;- capture.output(sarima(diff(dead_ts),0, 1, 6))\n\n\n\n\n\nCode\ncat(dead_full[43:59], dead_full[length(dead_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n          Estimate       SE t.value p.value\nma1         0.8533   0.2409  3.5425  0.0012\nma2        -0.3695   0.3200 -1.1546  0.2563\nma3        -1.0757   0.2219 -4.8470  0.0000\nma4        -1.0900   0.2703 -4.0323  0.0003\nma5         0.0148   0.2351  0.0631  0.9501\nma6         0.6672   0.2069  3.2254  0.0028\nconstant -864.6125 362.8723 -2.3827  0.0229\n\nsigma^2 estimated as 70172669 on 34 degrees of freedom \n \nAIC = 21.60481  AICc = 21.68759  BIC = 21.93916 \n \n \n\n\nThe Standard Residual Plot presents a promising outlook, showcasing good signs of stationarity with a consistent mean and variation over time. In the Autocorrelation Function (ACF) plot, the absence of significant correlation further supports the efficacy of our model, suggesting it has successfully captured the underlying patterns in the data, leaving only white noise behind. This is indicative of an excellent model fit. While the Quantile-Quantile (Q-Q) Plot largely aligns with expectations of normality, displaying satisfactory adherence, there is still some variation observed.\nDiving deeper into the diagnostic checks, the Ljung-Box test yields intriguing results, with values surpassing the 0.05 threshold (5% significance level). This indicates a lack of significant autocorrelation, reinforcing the model’s adequacy. However, an analysis of the Moving Average (MA) parameters reveals a nuanced picture: while the p-values for ma2 and ma5 exceed the 0.05 mark, suggesting these terms may not contribute significantly to the model, the p-values for ma1, ma3, ma4, and ma6 fall below this threshold, indicating their importance in the model’s structure. This mixed outcome highlights areas for potential refinement and underscores the model’s overall robustness.\n\n\n\n\nCode\nhos1_full &lt;- capture.output(sarima(diff(hos_ts1),0, 1, 1))\n\n\n\n\n\nCode\ncat(hos1_full[22:33], hos1_full[length(hos1_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n           Estimate        SE t.value p.value\nma1         -0.8352    0.0997 -8.3758  0.0000\nconstant -2100.3772 1731.5890 -1.2130  0.2315\n\nsigma^2 estimated as 4024330014 on 45 degrees of freedom \n \nAIC = 25.1066  AICc = 25.11241  BIC = 25.2247 \n \n \n\n\nThe Standard Residual Plot presents a positive indication of stationarity, characterized by a consistent mean and variance throughout. This suggests a stable model performance over time. The Autocorrelation Function (ACF) plot reinforces this assessment, showing an absence of correlation among residuals and implying that the model has effectively captured the underlying pattern, leaving only white noise behind. This is a hallmark of an excellently fitted model.\nFurthermore, the Quantile-Quantile (Q-Q) Plot demonstrates commendable adherence to normality, albeit with some minor deviations. The consistency in the plot underscores the model’s reliability in normal distribution assumptions. However, the Ljung-Box test introduces a nuanced perspective, displaying values that surpass the 0.05 threshold (5% significance level). This indicates a lack of significant autocorrelation, reinforcing the model’s adeptness at fitting the data effectively, as further evidenced by a p-value below 0.05.\n\n\n\n\nCode\nhos2_full &lt;- capture.output(sarima(diff(hos_ts2),2, 1, 1))\n\n\n\n\n\nCode\ncat(hos2_full[31:44], hos2_full[length(hos2_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n          Estimate       SE  t.value p.value\nar1         0.2460   0.1276   1.9271  0.0606\nar2        -0.4570   0.1247  -3.6649  0.0007\nma1        -1.0000   0.0625 -15.9981  0.0000\nconstant -184.7783 189.0308  -0.9775  0.3338\n\nsigma^2 estimated as 449339658 on 43 degrees of freedom \n \nAIC = 23.07453  AICc = 23.09479  BIC = 23.27135 \n \n \n\n\nThe Standard Residual Plot presents a promising picture, showcasing characteristics of good stationarity with a largely consistent mean and variance across the series. The Autocorrelation Function (ACF) plot further strengthens our confidence in the model’s robustness by displaying negligible correlation, which implies that the model has successfully captured the underlying pattern, leaving only white noise behind. This is indicative of an exceptionally well-fitted model. Meanwhile, the Quantile-Quantile (Q-Q) Plot also leans towards demonstrating commendable normality, albeit with some deviations. The Ljung-Box test results introduce a slight variance, displaying values surpassing the 0.05 threshold (at a 5% significance level), which denotes the lack of substantial correlation—another hallmark of a model that is fitting well. Although the p-value for ar1 marginally exceeds 0.05, the p-values for ar2 and ma1 are below this threshold, further underscoring the model’s efficacy and the accuracy of its fit.\n\n\n\n\nCode\nhos3_full &lt;- capture.output(sarima(diff(hos_ts3),2, 0, 0))\n\n\n\n\n\nCode\ncat(hos3_full[20:32], hos3_full[length(hos3_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE t.value p.value\nar1     0.3115 0.1242  2.5083  0.0158\nar2    -0.4916 0.1224 -4.0165  0.0002\nxmean   0.0005 0.0028  0.1899  0.8502\n\nsigma^2 estimated as 0.000494291 on 45 degrees of freedom \n \nAIC = -4.595389  AICc = -4.584025  BIC = -4.439455 \n \n \n\n\nThe Standard Residual Plot presents a promising depiction of stationarity, characterized by a largely constant mean and variance, suggesting the model’s effectiveness in capturing the data’s essence. The Autocorrelation Function (ACF) plot further strengthens this assessment, displaying negligible correlation among residuals and implying that the model residuals resemble white noise—a hallmark of an excellent model fit. Meanwhile, the Quantile-Quantile (Q-Q) Plot offers substantial evidence of normality, albeit with some variability. This is complemented by the outcomes of the Ljung-Box test, which, despite variations, predominantly reports p-values below the 0.05 mark (5% significance level). Such results underscore a lack of significant autocorrelation within the residuals, affirming the model’s robustness and precision in fitting the data.\n\n\n\n\nCode\nunemploy_full &lt;- capture.output(sarima(diff(diff(unemploy_ts)),1, 0, 1))\n\n\n\n\n\nCode\ncat(unemploy_full[135:147], unemploy_full[length(unemploy_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE t.value p.value\nar1     0.0308 0.2012  0.1533  0.8796\nma1    -1.0000 0.1216 -8.2224  0.0000\nxmean  -0.0005 0.0006 -0.8403  0.4098\n\nsigma^2 estimated as 0.0004656674 on 22 degrees of freedom \n \nAIC = -4.386209  AICc = -4.340495  BIC = -4.191189 \n \n \n\n\nAfter implementing first-order differencing, the unemployment rate time series continued to exhibit non-stationary characteristics, prompting the necessity for a second differencing step to attain stationarity. Post-differencing, the Standard Residual Plot demonstrated commendable stationarity, characterized by a mostly constant mean and variance, indicative of a well-adjusted series. The Autocorrelation Function (ACF) plot, revealing no significant correlations, suggests that the model has effectively captured the underlying patterns within the data, leaving behind what appears to be purely white noise. This is a strong indicator of an excellently fitted model.\nAdditionally, the Quantile-Quantile (Q-Q) Plot shows a satisfactory alignment with normality, though with some deviations. The results from the Ljung-Box test varied, presenting values surpassing the 0.05 threshold (5% significance level), which points to the absence of significant autocorrelations and underscores the model’s adequacy. The analysis of parameter significance revealed that the p-value for the autoregressive term (ar1) exceeded 0.05, suggesting it might not contribute significantly to the model, whereas the moving average term (ma1), with a p-value below 0.05, indicates a meaningful contribution. This nuanced understanding of the model’s components further attests to its robustness in capturing the dynamics of the unemployment rate time series.\n\n\n\n\nCode\nstock_full &lt;- capture.output(sarima(diff(stock_ts),1, 0, 3))\n\n\n\n\n\nCode\ncat(stock_full[57:71], stock_full[length(stock_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE t.value p.value\nar1     0.7390 0.1659  4.4548  0.0001\nma1    -1.0930 0.2018 -5.4163  0.0000\nma2     0.4961 0.1968  2.5211  0.0156\nma3    -0.4032 0.1575 -2.5594  0.0142\nxmean  -0.1074 0.0232 -4.6303  0.0000\n\nsigma^2 estimated as 0.2899104 on 42 degrees of freedom \n \nAIC = 1.901971  AICc = 1.933108  BIC = 2.13816 \n \n \n\n\nThe Standard Residual Plot presents a promising indication of stationarity, with the mean and variance appearing mostly constant throughout. This uniformity in the residuals suggests a stable model performance over time. Meanwhile, the Autocorrelation Function (ACF) plot reveals an absence of correlation among the residuals, implying that the model has effectively captured the underlying patterns in the data, leaving behind what appears to be pure white noise. Such an outcome is indicative of an excellently fitted model.\nOn the other hand, the Quantile-Quantile (Q-Q) Plot showcases a decent approximation to normality, although there is some variability. This suggests that while the model’s residuals closely follow a normal distribution, there are areas of deviation worth noting.\nMoreover, the results from the Ljung-Box test introduce some variability, with values surpassing the 0.05 threshold (5% significance level). This would typically indicate potential correlation; however, in this context, it signifies the absence of significant autocorrelation, further affirming the model’s adequacy. Notably, all p-values fall below the 0.05 mark, reinforcing the statistical strength and the well-fitted nature of the model.\n\n\n\n\nCode\ndemo_full &lt;- capture.output(sarima(diff(demo_ts),0, 0, 1))\n\n\n\n\n\nCode\ncat(demo_full[145:156], demo_full[length(demo_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE  t.value p.value\nma1    -0.9999 0.0732 -13.6622  0.0000\nxmean   0.0002 0.0003   0.8221  0.4153\n\nsigma^2 estimated as 0.0004178928 on 46 degrees of freedom \n \nAIC = -4.736419  AICc = -4.730863  BIC = -4.619469 \n \n \n\n\nThe Standard Residual Plot exhibits commendable stationarity, characterized by a consistent mean and variance throughout, indicative of a robust model performance. The Autocorrelation Function (ACF) plot further reinforces this by demonstrating an absence of correlation among residuals, thereby suggesting that the model has effectively captured the underlying data patterns, leaving behind only white noise. This is a hallmark of an excellently fitted model. Meanwhile, the Quantile-Quantile (Q-Q) Plot generally aligns with expectations of normality, although minor deviations are observed, which is typical in practical scenarios. The Ljung-Box test results introduce some variability, with certain values crossing the 0.05 (5% significance) threshold. However, the predominance of p-values below this threshold underscores the model’s ability to adequately represent the data without significant autocorrelation among residuals, cementing its status as well-calibrated and fitting.\n\n\n\n\nCode\ninde_full &lt;- capture.output(sarima(diff(inde_ts),0, 0, 1))\n\n\n\n\n\nCode\ncat(inde_full[27:38], inde_full[length(inde_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE  t.value p.value\nma1     -1e+00 0.0635 -15.7571  0.0000\nxmean    3e-04 0.0004   0.7764  0.4415\n\nsigma^2 estimated as 0.001318368 on 46 degrees of freedom \n \nAIC = -3.587405  AICc = -3.581849  BIC = -3.470454 \n \n \n\n\nThe Standard Residual Plot presents a commendable depiction of stationarity, with the mean and variance remaining mostly constant throughout, suggesting that the data points fluctuate around a steady level. The Autocorrelation Function (ACF) plot further reinforces the model’s efficacy by exhibiting negligible correlation among residuals, indicating that the model has successfully captured the underlying pattern in the data, leaving behind what appears to be mere white noise. This observation underscores the model’s robust fit to the data.\nIn the Quantile-Quantile (Q-Q) Plot, we observe a satisfactory alignment with normality, although there are minor deviations. Such variations are typical and do not significantly detract from the model’s overall performance.\nHowever, the results from the Ljung-Box test introduce a layer of complexity, displaying values that occasionally surpass the 0.05 threshold, which typically denotes a 5% significance level. Despite these variations, the predominance of p-values less than 0.05 throughout our analysis provides strong evidence against significant autocorrelation among residuals, further affirming the model’s aptitude in capturing the essence of the dataset without overfitting.\nCollectively, these diagnostic tools paint a picture of a well-adjusted model, adept at navigating through the intricacies of the data to offer valuable insights, albeit with room for minor improvements as indicated by the Q-Q plot and Ljung-Box test variations.\n\n\n\n\nCode\nrep_full &lt;- capture.output(sarima(diff(rep_ts),1, 0, 1))\n\n\n\n\n\nCode\ncat(rep_full[126:138], rep_full[length(rep_full)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE  t.value p.value\nar1       0.41 0.1351   3.0350  0.0040\nma1      -1.00 0.0628 -15.9210  0.0000\nxmean     0.00 0.0003   0.0824  0.9347\n\nsigma^2 estimated as 0.0003748071 on 45 degrees of freedom \n \nAIC = -4.821046  AICc = -4.809682  BIC = -4.665112 \n \n \n\n\nThe Standard Residual Plot presents a promising outlook, showcasing robust stationarity characterized by a largely constant mean and variance, indicative of a well-behaved model. The Autocorrelation Function (ACF) plot further corroborates this by revealing negligible correlation, implying that the residuals amount to white noise and underscoring the model’s comprehensive capture of underlying patterns—a hallmark of excellent model fit. Meanwhile, the Quantile-Quantile (Q-Q) Plot demonstrates commendable adherence to normality, albeit with minor deviations. The Ljung-Box test results introduce a nuance, exhibiting values surpassing the 0.05 threshold (at a 5% significance level), thereby negating the presence of substantial autocorrelation and endorsing the model’s aptness. Crucially, all observed p-values fall below the 0.05 mark, reinforcing the statistical soundness of our model.\n\n\n\n\n\n7. Auto.Arima()\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesNewly confirmed case time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesUnemployment rate time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\nauto.arima(diff(d_vacc_ts))\n\n\nSeries: diff(d_vacc_ts) \nARIMA(0,0,2) with zero mean \n\nCoefficients:\n         ma1     ma2\n      0.8344  0.5402\ns.e.  0.1780  0.2384\n\nsigma^2 = 3.458e+09:  log likelihood = -359.11\nAIC=724.22   AICc=725.18   BIC=728.32\n\n\nThe best model from the step above was ARIMA(2,1,2), while the best model Auto ARIMA gave me is ARIMA(0,0,2) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(p_vacc_ts))\n\n\nSeries: diff(p_vacc_ts) \nARIMA(0,1,1) \n\nCoefficients:\n         ma1\n      0.4527\ns.e.  0.1518\n\nsigma^2 = 2.746:  log likelihood = -51.55\nAIC=107.11   AICc=107.61   BIC=109.7\n\n\nThe best model from the step above was ARIMA(0,1,3), while the best model Auto ARIMA gave me is ARIMA(0,1,1) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(pf_vacc_ts))\n\n\nSeries: diff(pf_vacc_ts) \nARIMA(0,1,2) \n\nCoefficients:\n         ma1     ma2\n      0.8498  0.5394\ns.e.  0.1663  0.2047\n\nsigma^2 = 1.679:  log likelihood = -44.79\nAIC=95.59   AICc=96.63   BIC=99.47\n\n\nThe best model from the step above was ARIMA(0,1,3), while the best model Auto ARIMA gave me is ARIMA(0,1,2) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(case_ts))\n\n\nSeries: diff(case_ts) \nARIMA(0,0,1) with non-zero mean \n\nCoefficients:\n         ma1     mean\n      0.8182  2324827\ns.e.  0.0780   554675\n\nsigma^2 = 4.192e+12:  log likelihood = -669.47\nAIC=1344.95   AICc=1345.58   BIC=1350.16\n\n\nThe best model from the step above was ARIMA(0,1,2), while the best model Auto ARIMA gave me is ARIMA(0,0,1) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(dead_ts))\n\n\nSeries: diff(dead_ts) \nARIMA(0,1,1) \n\nCoefficients:\n         ma1\n      0.7989\ns.e.  0.0782\n\nsigma^2 = 138666540:  log likelihood = -442.5\nAIC=889.01   AICc=889.32   BIC=892.44\n\n\nThe best model from the step above was ARIMA(0,1,6), while the best model Auto ARIMA gave me is ARIMA(0,1,1) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(hos_ts1))\n\n\nSeries: diff(hos_ts1) \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.7905\ns.e.   0.0944\n\nsigma^2 = 4.238e+09:  log likelihood = -587.61\nAIC=1179.22   AICc=1179.49   BIC=1182.92\n\n\nThe best model from the step above and from the Auto ARIMA was all ARIMA(0,1,1), which means it is the best model.\n\n\n\n\nCode\nauto.arima(diff(hos_ts2))\n\n\nSeries: diff(hos_ts2) \nARIMA(0,0,3) with zero mean \n\nCoefficients:\n         ma1      ma2      ma3\n      0.0996  -0.4661  -0.2804\ns.e.  0.1375   0.1643   0.1693\n\nsigma^2 = 462716317:  log likelihood = -545.8\nAIC=1099.61   AICc=1100.54   BIC=1107.09\n\n\nThe best model from the step above was ARIMA(2,1,1), while the best model Auto ARIMA gave me is ARIMA(0,0,3) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(hos_ts3))\n\n\nSeries: diff(hos_ts3) \nARIMA(0,0,3) with zero mean \n\nCoefficients:\n         ma1      ma2      ma3\n      0.1479  -0.5111  -0.3791\ns.e.  0.1330   0.1547   0.1596\n\nsigma^2 = 0.0005008:  log likelihood = 115.25\nAIC=-222.51   AICc=-221.58   BIC=-215.02\n\n\nThe best model from the step above was ARIMA(2,0,0), while the best model Auto ARIMA gave me is ARIMA(0,0,3) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(diff(unemploy_ts)))\n\n\nSeries: diff(diff(unemploy_ts)) \nARIMA(0,0,0) with zero mean \n\nsigma^2 = 0.000934:  log likelihood = 51.73\nAIC=-101.45   AICc=-101.28   BIC=-100.23\n\n\nThe best model from the step above was ARIMA(1,0,1), while the best model Auto ARIMA gave me is ARIMA(0,0,0) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(stock_ts))\n\n\nSeries: diff(stock_ts) \nARIMA(2,0,0) with zero mean \n\nCoefficients:\n          ar1     ar2\n      -0.2074  0.3307\ns.e.   0.1500  0.1513\n\nsigma^2 = 0.3424:  log likelihood = -40.65\nAIC=87.29   AICc=87.85   BIC=92.84\n\n\nThe best model from the step above was ARIMA(1,0,3), while the best model Auto ARIMA gave me is ARIMA(2,0,0) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(demo_ts))\n\n\nSeries: diff(demo_ts) \nARIMA(0,0,1) with zero mean \n\nCoefficients:\n          ma1\n      -0.9552\ns.e.   0.1012\n\nsigma^2 = 0.0004485:  log likelihood = 116.21\nAIC=-228.43   AICc=-228.16   BIC=-224.69\n\n\nThe best model from the step above and from the Auto ARIMA was all ARIMA(0,0,1), which means it is the best model.\n\n\n\n\nCode\nauto.arima(diff(inde_ts))\n\n\nSeries: diff(inde_ts) \nARIMA(1,0,0) with zero mean \n\nCoefficients:\n          ar1\n      -0.5722\ns.e.   0.1176\n\nsigma^2 = 0.001697:  log likelihood = 85.29\nAIC=-166.58   AICc=-166.31   BIC=-162.83\n\n\nThe best model from the step above was ARIMA(0,0,1), while the best model Auto ARIMA gave me is ARIMA(1,0,0) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\n\nCode\nauto.arima(diff(rep_ts))\n\n\nSeries: diff(rep_ts) \nARIMA(0,0,1) with zero mean \n\nCoefficients:\n          ma1\n      -0.8013\ns.e.   0.1445\n\nsigma^2 = 0.0004739:  log likelihood = 115.59\nAIC=-227.18   AICc=-226.92   BIC=-223.44\n\n\nThe best model from the step above was ARIMA(1,0,1), while the best model Auto ARIMA gave me is ARIMA(0,0,1) with drift. This discrepancy raises concerns about reliability, as Auto ARIMA tends to overlook instances of significant lag correlation, as evidenced by the ACF/PACF plots. Instead, it prioritizes minimizing AIC/BIC values without considering the full spectrum of model dynamics. This narrow focus risks recommending a model prone to overfitting, lacking in the comprehensive assessment necessary for accurate forecasting.\n\n\n\nAuto ARIMA may not always serve as the most dependable model for forecasting, for several reasons. Firstly, its reliance on predefined criteria for model selection can sometimes overlook subtle nuances within the data, which might be crucial for accurate predictions. Additionally, the automated nature of Auto ARIMA increases the risk of overfitting or selecting a model that is less than optimal. Thus, although Auto ARIMA is an incredibly potent analytical tool, it is essential to approach its projected results with caution and not rely on them unquestioningly.\n\n\n8. Forecasting\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesNewly confirmed case time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesUnemployment rate time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\npred_techmu=forecast(fit_d_vacc_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_p_vacc_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_pf_vacc_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_case_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_dead_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_hos1_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_hos2_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_hos3_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_unemploy_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_stock_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_demo_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_inde_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\n\nCode\npred_techmu=forecast(fit_rep_AR,50)\nautoplot(pred_techmu) + theme_bw() \n\n\n\n\n\n\n\n\nIn the presented forecast graphs, the predictive trajectory is depicted by a blue line, surrounded by a confidence band in two shades of purple. The darker purple represents the 95% confidence interval, indicating a high level of certainty, while the lighter purple corresponds to the 5% interval, denoting lower confidence levels. Notably, as the forecast extends into the future, the confidence band expands, signifying a widening interval. This expansion reflects an increase in forecast uncertainty—the further we project into the future, the more variable and less certain the predictions become. This pattern is consistently observed across all plots, underscoring the inherent challenge of forecasting over extended periods.\n\n\n9. ARIMA vs. Benchmarks\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesNewly confirmed case time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesUnemployment rate time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(d_vacc_ts, order=c(2, 1, 2),include.drift = FALSE) \nautoplot(d_vacc_ts) +\n  autolayer(meanf(d_vacc_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(d_vacc_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(d_vacc_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(d_vacc_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Daily Vaccinations\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot showcases a comparison of different forecasting methods for daily vaccination numbers. The black line represents the actual historical data for daily vaccinations, displaying a sharp peak in early 2021 and another in early 2022, followed by a decline. The forecasts from ARIMA, Drift, Mean, and Naïve models are depicted as flat lines beyond the historical data, indicating their prediction for future values. The ARIMA forecast appears slightly above zero, suggesting minimal change in future vaccination numbers, which could imply a matured vaccination campaign. Drift and Mean models predict a very slight downward and upward trend, respectively, while the Naïve model, often used as a baseline comparison, suggests no change, simply carrying the last observed data point forward. The stability in these predictions may reflect an anticipation that vaccination rates will level off, having addressed the immediate demand.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(p_vacc_ts, order=c(0, 1, 3),include.drift = FALSE) \nautoplot(p_vacc_ts) +\n  autolayer(meanf(p_vacc_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(p_vacc_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(p_vacc_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(p_vacc_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with People Vaccinated\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot is a visual representation of the comparison between different forecasting methods for the number of people vaccinated over time. The historical data is shown by the black line, indicating the growth trend of vaccinations until the current period. The projections made by the ARIMA, Drift, Mean, and Naïve methods are depicted as flat lines extending from the last historical point into the future (2023 and beyond).\nThe ARIMA model predicts a slight increase in vaccination numbers, while the Drift method suggests a more optimistic steady rise. The Mean model forecasts a constant rate, and the Naïve method, which carries the last observed value forward, also indicates no change. It is clear that these models have varying degrees of optimism regarding the future trend of vaccination numbers, with ARIMA and Drift expecting growth, while Mean and Naïve forecasts imply stabilization.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(pf_vacc_ts, order=c(0, 1, 3),include.drift = FALSE) \nautoplot(pf_vacc_ts) +\n  autolayer(meanf(pf_vacc_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(pf_vacc_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(pf_vacc_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(pf_vacc_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with People Fully Vaccinated\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThis plot compares the forecasts of fully vaccinated individuals using several time series models. The black line represents the actual number of people fully vaccinated over time, showing an initial steep increase that plateaus as it moves into 2022. Predictions from the ARIMA, Drift, Mean, and Naïve models extend from the last data point. The ARIMA model projects a continued but slowing increase in fully vaccinated people, while the Drift model shows a steeper increase, suggesting higher future vaccination rates. The Mean model predicts a flat trend, indicating no significant change moving forward, and the Naïve model simply extends the last known value into the future, suggesting a static forecast. The models reflect different assumptions about the continuation of vaccination efforts and possible changes in public health policy or vaccine uptake.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(case_ts, order=c(0, 1, 2),include.drift = FALSE) \nautoplot(case_ts) +\n  autolayer(meanf(case_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(case_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(case_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(case_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Newly Confirmed Case\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe graph presents a forecast comparison for newly confirmed COVID-19 cases using various time series models. The historical data, illustrated by the black line, shows an increasing trend through 2020 and 2021, with a plateau into 2022. The forecast models—ARIMA, Drift, Mean, and Naïve—are indicated by different colored lines beyond the last historical data point. The ARIMA model predicts a steady upward trend, suggesting an increase in cases. In contrast, the Drift model shows a flat forecast, indicating little change. The Mean model also forecasts a constant trend, and the Naïve model projects a continuation of the last observed value. This graphical representation provides an outlook on potential future case trends based on different modeling approaches.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(dead_ts, order=c(0, 1, 6),include.drift = FALSE) \nautoplot(dead_ts) +\n  autolayer(meanf(dead_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(dead_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(dead_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(dead_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Death Case\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot illustrates a forecast comparison for COVID-19 death cases using various time series models. The black line represents the historical number of deaths, increasing initially and then plateauing into 2022. Forecasts by ARIMA, Drift, Mean, and Naïve models are shown as colored lines projecting beyond the last data point. The ARIMA model predicts an upward trend, possibly anticipating a rise in death cases. The Drift model’s forecast remains constant, while the Mean model suggests slight growth. The Naïve model extends the last value into the future, implying no immediate change. This analysis might help in understanding and preparing for potential future scenarios in public health planning.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(hos_ts1, order=c(0, 1, 1),include.drift = FALSE) \nautoplot(hos_ts1) +\n  autolayer(meanf(hos_ts1, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(hos_ts1, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(hos_ts1, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(hos_ts1 ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Inpatient Bed Number\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot compares different time series forecasting methods for the number of inpatient hospital beds occupied over time. The black line indicates the actual historical data, which shows a rapid increase at the beginning, stabilizing as it moves into the latter part of 2021 and remains relatively flat into 2022. Forecasts by the ARIMA, Drift, Mean, and Naïve methods are represented as colored lines extending from the end of the actual data into future years. The ARIMA model shows an optimistic continuous increase in bed occupancy, while the Drift model forecasts a flat trend. The Mean model predicts a very slight increase, suggesting stability, and the Naïve model extends the last known value, implying no expected change. Each model’s prediction reflects different assumptions and interpretations of the historical data’s underlying patterns.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(hos_ts2, order=c(2, 1, 1),include.drift = FALSE) \nautoplot(hos_ts2) +\n  autolayer(meanf(hos_ts2, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(hos_ts2, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(hos_ts2, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(hos_ts2 ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Inpatient Bed Number Used for COVID\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot illustrates a forecast comparison using different models for the number of inpatient hospital beds utilized for COVID-19 patients. The actual historical data, represented by the black line, shows several spikes, indicating surges in hospital bed usage at different times, presumably correlating with waves of the pandemic. Moving into the future, forecasts from ARIMA, Drift, Mean, and Naïve methods show diverging trends. ARIMA expects an increase, while Drift indicates a flat future trend. Mean and Naïve models suggest a slight increase and no change, respectively. The models likely reflect different assumptions about pandemic progression and healthcare needs.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(hos_ts3, order=c(2, 0, 0),include.drift = FALSE) \nautoplot(hos_ts3) +\n  autolayer(meanf(hos_ts3, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(hos_ts3, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(hos_ts3, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(hos_ts3 ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Utilization Rate for Inpatient Bed Used for COVID\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThis plot appears to be a time series forecast comparing different methods (ARIMA, Drift, Mean, Naive) for predicting the utilization rate of inpatient beds used for COVID. The historical data shows significant fluctuations, which could correspond to various waves or surges in COVID cases. The forecast section shows that while some methods predict stability or a decline, the ARIMA model suggests a potential increase in bed utilization, which might anticipate a rise in cases or a change in hospitalization rates. The other methods seem to predict a relatively flat or stable future trend.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(unemploy_ts, order=c(1, 0, 1),include.drift = FALSE) \nautoplot(unemploy_ts) +\n  autolayer(meanf(unemploy_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(unemploy_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(unemploy_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(unemploy_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Unemployment Rate\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThis plot appears to be a graphical representation comparing different forecasting methods for unemployment rates over time. The black line represents historical data on unemployment rates. The colored lines, which represent forecasts from various methods such as ARIMA, Drift, Mean, and Naïve, start from where the historical data ends. The plot shows the unemployment rate sharply increasing in 2020, likely due to the COVID-19 pandemic, then gradually decreasing over time, indicating recovery. The ARIMA model forecast seems to indicate a slight increase in unemployment in the future, while the Drift, Mean, and Naïve forecasts suggest a stable or decreasing trend.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(stock_ts, order=c(1, 0, 3),include.drift = FALSE) \nautoplot(stock_ts) +\n  autolayer(meanf(stock_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(stock_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(stock_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(stock_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Pfizer Stock Price\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot shows the historical stock price for Pfizer, represented by the black line, along with forecasts from different models: ARIMA, Drift, Mean, and Naïve. The colored horizontal lines indicate the forecasted stock price level according to each model from the present to 2024. The ARIMA model forecasts a slight decline, while the Drift and Mean models predict a stabilization of the stock price. The Naïve forecast suggests a more significant decline. This visualization is used to compare how different statistical methods anticipate the stock price trend based on historical data.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(demo_ts, order=c(0, 0, 1),include.drift = FALSE) \nautoplot(demo_ts) +\n  autolayer(meanf(demo_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(demo_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(demo_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(demo_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Support Rate for Democratic\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe uploaded plot appears to compare the performance of various forecasting methods—Arima, Drift, Mean, and Naïve—on a particular dataset over time. The solid black line likely represents actual historical data, and the horizontal colored lines project future predictions according to each method. These predictions might illustrate expected trends or levels for a variable such as support rates for a political party, stock prices, healthcare metrics, or economic indicators. The Arima forecast line suggests changes over time, while Drift, Mean, and Naïve methods seem to predict a constant future value, likely based on different statistical assumptions or calculations. This visualization helps to evaluate the different approaches to forecasting and their potential accuracy in predicting future trends or values.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(inde_ts, order=c(0, 0, 1),include.drift = FALSE) \nautoplot(inde_ts) +\n  autolayer(meanf(inde_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(inde_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(inde_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(inde_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Support Rate for Independent\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot you’ve shared appears to depict a time series analysis using various benchmark methods such as ARIMA, Drift, Mean, and Naïve to forecast future values related to a specific metric. The actual historical data is shown by the solid black line, which seems to have a particular trend or pattern. The forecast lines for each method start from where the actual data ends and project into the future, displaying the predicted values according to each method.\nARIMA is showing a distinct upward or downward trend, suggesting a specific model-based prediction. The Drift method appears to forecast a linear trend that picks up from the last observed point. The Mean forecast suggests that future values will hover around the historical average, while the Naïve method seems to project that future values will remain constant at the last observed value. Each method offers a different perspective on future expectations based on past data.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(rep_ts, order=c(1, 0, 1),include.drift = FALSE) \nautoplot(rep_ts) +\n  autolayer(meanf(rep_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(rep_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(rep_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(rep_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Support Rate for Republican\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nIt seems there was an error processing the image you’ve uploaded. Could you provide a description of what the image contains or try uploading it again? If it’s a plot or a graph, details about the axes, any legends or keys, and the general trend or data points shown would be helpful for me to give an explanation.\n\n\n\n\n\n10. SARIMA ACF & PACF Plots\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesNewly confirmed case time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesUnemployment rate time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\nd_vacc_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe from the seasonal differenced dataset that the time series plot looks significantly different. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1,2 D: 1 Q: 1\n\n\n\n\nCode\np_vacc_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe that there is no obvious seasonal difference in the dataset. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1,2 Q: 1\n\n\n\n\nCode\npf_vacc_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe that there is no obvious seasonal difference in the dataset. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1,2 Q: 1\n\n\n\n\nCode\ncase_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe that there is no obvious seasonal difference in the dataset. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1,2,3,4 Q: 1\n\n\n\n\nCode\ndead_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe that there is no obvious seasonal difference in the dataset. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1,2,3,4,5 Q: 1\n\n\n\n\nCode\nhos_ts1 %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe that there is no obvious seasonal difference in the dataset. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1,2 Q: 1\n\n\n\n\nCode\nhos_ts2 %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe from the seasonal differenced dataset that the time series plot looks significantly different. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1 Q: 1\n\n\n\n\nCode\nhos_ts3 %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe from the seasonal differenced dataset that the time series plot looks significantly different. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1,2 D: 1 Q: 1\n\n\n\n\nCode\nunemploy_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe that there is no obvious seasonal difference in the dataset. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1 Q: 1\n\n\n\n\nCode\nstock_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe from the seasonal differenced dataset that the time series plot looks significantly different. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1,2 Q: 1\n\n\n\n\nCode\ndemo_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe from the seasonal differenced dataset that the time series plot looks significantly different. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1 Q: 1\n\n\n\n\nCode\ninde_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe from the seasonal differenced dataset that the time series plot looks significantly different. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1 Q: 1\n\n\n\n\nCode\nrep_ts %&gt;% diff(lag=12) %&gt;% ggtsdisplay()\n\n\n\n\n\nYou can observe from the seasonal differenced dataset that the time series plot looks significantly different. The seasonal cycles are removed, revealing the true trend of the data. Looking at the ACF and PACF plots, we can determine that:\nP: 1 D: 1 Q: 1\n\n\n\n\n\n11. SARIMA(p,d,q)\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesNewly confirmed case time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesUnemployment rate time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\nI will run different combinations of SARIMA with the values q = 1,2,3 d = 1,2 p = 1,2,5 P = 1,2 Q = 1 D = 1\n\n\nCode\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*30),nrow=30)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\noutput=SARIMA.c(p1=1,p2=5,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=diff(diff(d_vacc_ts)))\n\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n398.4760\n399.1840\n398.7837\n\n\n0\n1\n0\n0\n1\n1\n400.2716\n401.6877\n401.2716\n\n\n0\n1\n0\n1\n1\n0\n400.2716\n401.6877\n401.2716\n\n\n0\n1\n0\n1\n1\n1\n402.2716\n404.3957\n404.4534\n\n\n0\n1\n1\n0\n1\n0\n394.5558\n395.9719\n395.5558\n\n\n0\n1\n1\n0\n1\n1\n396.5371\n398.6613\n398.7189\n\n\n0\n1\n1\n1\n1\n0\n396.5371\n398.6613\n398.7189\n\n\n0\n1\n1\n1\n1\n1\n398.5371\n401.3693\n402.5371\n\n\n0\n1\n2\n0\n1\n0\n396.1091\n398.2333\n398.2910\n\n\n0\n1\n2\n0\n1\n1\n398.1065\n400.9387\n402.1065\n\n\n0\n1\n2\n1\n1\n0\n398.1065\n400.9387\n402.1065\n\n\n1\n1\n0\n0\n1\n0\n399.1057\n400.5218\n400.1057\n\n\n1\n1\n0\n0\n1\n1\n401.0156\n403.1398\n403.1974\n\n\n1\n1\n0\n1\n1\n0\n401.0156\n403.1398\n403.1974\n\n\n1\n1\n0\n1\n1\n1\n403.0156\n405.8478\n407.0156\n\n\n1\n1\n1\n0\n1\n0\n396.2122\n398.3364\n398.3940\n\n\n1\n1\n1\n0\n1\n1\n398.2114\n401.0436\n402.2114\n\n\n1\n1\n1\n1\n1\n0\n398.2114\n401.0436\n402.2114\n\n\n1\n1\n2\n0\n1\n0\n398.1029\n400.9351\n402.1029\n\n\n2\n1\n0\n0\n1\n0\n400.3463\n402.4704\n402.5281\n\n\n2\n1\n0\n0\n1\n1\n402.2578\n405.0900\n406.2578\n\n\n2\n1\n0\n1\n1\n0\n402.2578\n405.0900\n406.2578\n\n\n2\n1\n1\n0\n1\n0\n397.6840\n400.5162\n401.6840\n\n\n3\n1\n0\n0\n1\n0\n397.1631\n399.9953\n401.1631\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n  p d q P D Q      AIC      BIC     AICc\n5 0 1 1 0 1 0 394.5558 395.9719 395.5558\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q      AIC      BIC     AICc\n5 0 1 1 0 1 0 394.5558 395.9719 395.5558\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q P D Q      AIC      BIC     AICc\n5 0 1 1 0 1 0 394.5558 395.9719 395.5558\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(0,1,1)x(0,1,0)12.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=p_vacc_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n100.48114\n101.25372\n100.76685\n\n\n0\n1\n0\n0\n1\n1\n102.38215\n103.92733\n103.30523\n\n\n0\n1\n0\n1\n1\n0\n102.38215\n103.92733\n103.30523\n\n\n0\n1\n0\n1\n1\n1\n104.38215\n106.69991\n106.38215\n\n\n0\n1\n1\n0\n1\n0\n88.52817\n90.07335\n89.45125\n\n\n0\n1\n1\n0\n1\n1\n90.46696\n92.78473\n92.46696\n\n\n0\n1\n1\n1\n1\n0\n90.46696\n92.78473\n92.46696\n\n\n0\n1\n1\n1\n1\n1\n92.46696\n95.55732\n96.10333\n\n\n0\n1\n2\n0\n1\n0\n78.02611\n80.34388\n80.02611\n\n\n0\n1\n2\n0\n1\n1\n79.96905\n83.05941\n83.60542\n\n\n0\n1\n2\n1\n1\n0\n79.96906\n83.05941\n83.60542\n\n\n0\n1\n3\n0\n1\n0\n76.93242\n80.02277\n80.56878\n\n\n1\n1\n0\n0\n1\n0\n79.14012\n80.68530\n80.06320\n\n\n1\n1\n0\n0\n1\n1\n81.11958\n83.43735\n83.11958\n\n\n1\n1\n0\n1\n1\n0\n81.11953\n83.43730\n83.11953\n\n\n1\n1\n0\n1\n1\n1\n83.11956\n86.20991\n86.75592\n\n\n1\n1\n1\n0\n1\n0\n78.34921\n80.66698\n80.34921\n\n\n1\n1\n1\n0\n1\n1\n80.34859\n83.43894\n83.98495\n\n\n1\n1\n1\n1\n1\n0\n80.34858\n83.43894\n83.98495\n\n\n1\n1\n2\n0\n1\n0\n76.30351\n79.39387\n79.93988\n\n\n2\n1\n0\n0\n1\n0\n78.18141\n80.49917\n80.18141\n\n\n2\n1\n0\n0\n1\n1\n80.18140\n83.27176\n83.81777\n\n\n2\n1\n0\n1\n1\n0\n80.18140\n83.27176\n83.81777\n\n\n2\n1\n1\n0\n1\n0\n79.84174\n82.93210\n83.47811\n\n\n3\n1\n0\n0\n1\n0\n79.07736\n82.16771\n82.71372\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n20 1 1 2 0 1 0 76.30351 79.39387 79.93988\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n20 1 1 2 0 1 0 76.30351 79.39387 79.93988\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n20 1 1 2 0 1 0 76.30351 79.39387 79.93988\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(1,1,2)x(0,1,0)12.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=pf_vacc_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n98.00877\n98.78136\n98.29449\n\n\n0\n1\n0\n0\n1\n1\n99.89367\n101.43885\n100.81675\n\n\n0\n1\n0\n1\n1\n0\n99.89367\n101.43885\n100.81675\n\n\n0\n1\n0\n1\n1\n1\n101.89367\n104.21144\n103.89367\n\n\n0\n1\n1\n0\n1\n0\n82.78603\n84.33121\n83.70911\n\n\n0\n1\n1\n0\n1\n1\n84.72002\n87.03778\n86.72002\n\n\n0\n1\n1\n1\n1\n0\n84.72002\n87.03778\n86.72002\n\n\n0\n1\n1\n1\n1\n1\n86.72002\n89.81037\n90.35638\n\n\n0\n1\n2\n0\n1\n0\n73.76322\n76.08098\n75.76322\n\n\n0\n1\n2\n0\n1\n1\n75.63203\n78.72239\n79.26840\n\n\n0\n1\n2\n1\n1\n0\n75.63204\n78.72240\n79.26840\n\n\n0\n1\n3\n0\n1\n0\n69.84105\n72.93140\n73.47741\n\n\n1\n1\n0\n0\n1\n0\n75.13328\n76.67846\n76.05635\n\n\n1\n1\n0\n0\n1\n1\n77.12142\n79.43919\n79.12142\n\n\n1\n1\n0\n1\n1\n0\n77.12142\n79.43919\n79.12142\n\n\n1\n1\n0\n1\n1\n1\n79.12142\n82.21178\n82.75779\n\n\n1\n1\n1\n0\n1\n0\n70.10211\n72.41988\n72.10211\n\n\n1\n1\n1\n0\n1\n1\n72.10029\n75.19064\n75.73665\n\n\n1\n1\n1\n1\n1\n0\n72.10029\n75.19065\n75.73665\n\n\n1\n1\n2\n0\n1\n0\n68.96748\n72.05784\n72.60385\n\n\n2\n1\n0\n0\n1\n0\n69.08627\n71.40404\n71.08627\n\n\n2\n1\n0\n0\n1\n1\n71.07520\n74.16556\n74.71156\n\n\n2\n1\n0\n1\n1\n0\n71.07456\n74.16491\n74.71092\n\n\n2\n1\n1\n0\n1\n0\n69.61066\n72.70102\n73.24703\n\n\n3\n1\n0\n0\n1\n0\n68.91017\n72.00053\n72.54654\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n25 3 1 0 0 1 0 68.91017 72.00053 72.54654\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n21 2 1 0 0 1 0 69.08627 71.40404 71.08627\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n21 2 1 0 0 1 0 69.08627 71.40404 71.08627\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(2,1,0)x(0,1,0)12.\n\n\nDue to the presence of non-stationary seasonality in this time series data, we have opted to discontinue its use. Non-stationary seasonality implies that the patterns and trends within the data exhibit variations over time, without displaying a consistent and predictable behavior. As a result, attempting to model or analyze such data may lead to unreliable or inaccurate outcomes. Hence, to ensure the robustness and validity of our analyses, we have decided to cease utilizing this particular time series.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=dead_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n706.0650\n707.4662\n706.2078\n\n\n0\n1\n0\n0\n1\n1\n707.8100\n710.6124\n708.2544\n\n\n0\n1\n0\n1\n1\n0\n708.0441\n710.8465\n708.4886\n\n\n0\n1\n0\n1\n1\n1\n708.3570\n712.5606\n709.2801\n\n\n0\n1\n1\n0\n1\n0\n680.9710\n683.7734\n681.4154\n\n\n0\n1\n1\n0\n1\n1\n680.6631\n684.8667\n681.5862\n\n\n0\n1\n1\n1\n1\n0\n682.6574\n686.8610\n683.5805\n\n\n0\n1\n1\n1\n1\n1\n682.0388\n687.6436\n683.6388\n\n\n0\n1\n2\n0\n1\n0\n667.8860\n672.0896\n668.8091\n\n\n0\n1\n2\n0\n1\n1\n663.0587\n668.6635\n664.6587\n\n\n0\n1\n2\n1\n1\n0\n666.1448\n671.7496\n667.7448\n\n\n0\n1\n3\n0\n1\n0\n669.6639\n675.2687\n671.2639\n\n\n1\n1\n0\n0\n1\n0\n684.1800\n686.9824\n684.6244\n\n\n1\n1\n0\n0\n1\n1\n681.6694\n685.8730\n682.5925\n\n\n1\n1\n0\n1\n1\n0\n684.9236\n689.1272\n685.8467\n\n\n1\n1\n0\n1\n1\n1\n683.5516\n689.1564\n685.1516\n\n\n1\n1\n1\n0\n1\n0\n668.1578\n672.3614\n669.0809\n\n\n1\n1\n1\n0\n1\n1\n663.7715\n669.3763\n665.3715\n\n\n1\n1\n1\n1\n1\n0\n666.1812\n671.7860\n667.7812\n\n\n1\n1\n2\n0\n1\n0\n668.5934\n674.1982\n670.1934\n\n\n2\n1\n0\n0\n1\n0\n673.8361\n678.0397\n674.7592\n\n\n2\n1\n0\n0\n1\n1\n672.1399\n677.7447\n673.7399\n\n\n2\n1\n0\n1\n1\n0\n673.8880\n679.4928\n675.4880\n\n\n2\n1\n1\n0\n1\n0\n668.2286\n673.8334\n669.8286\n\n\n3\n1\n0\n0\n1\n0\n670.8526\n676.4574\n672.4526\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 1 2 0 1 1 663.0587 668.6635 664.6587\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 1 2 0 1 1 663.0587 668.6635 664.6587\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 1 2 0 1 1 663.0587 668.6635 664.6587\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(0,1,2)x(0,1,1)12.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=diff(hos_ts1))\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n898.3779\n899.9333\n898.4992\n\n\n0\n1\n0\n0\n1\n1\n900.3664\n903.4771\n900.7414\n\n\n0\n1\n0\n1\n1\n0\n900.3668\n903.4775\n900.7418\n\n\n0\n1\n0\n1\n1\n1\n902.3740\n907.0400\n903.1481\n\n\n0\n1\n1\n0\n1\n0\n889.3236\n892.4343\n889.6986\n\n\n0\n1\n1\n0\n1\n1\n891.2997\n895.9657\n892.0739\n\n\n0\n1\n1\n1\n1\n0\n891.3006\n895.9667\n892.0748\n\n\n0\n1\n1\n1\n1\n1\n893.3158\n899.5372\n894.6491\n\n\n0\n1\n2\n0\n1\n0\n890.2569\n894.9229\n891.0311\n\n\n0\n1\n2\n0\n1\n1\n892.2341\n898.4554\n893.5674\n\n\n0\n1\n2\n1\n1\n0\n892.2345\n898.4559\n893.5678\n\n\n0\n1\n3\n0\n1\n0\n887.8623\n894.0837\n889.1957\n\n\n1\n1\n0\n0\n1\n0\n896.5763\n899.6870\n896.9513\n\n\n1\n1\n0\n0\n1\n1\n898.5269\n903.1929\n899.3010\n\n\n1\n1\n0\n1\n1\n0\n898.5321\n903.1982\n899.3063\n\n\n1\n1\n0\n1\n1\n1\n900.5115\n906.7329\n901.8448\n\n\n1\n1\n1\n0\n1\n0\n890.3868\n895.0528\n891.1610\n\n\n1\n1\n1\n0\n1\n1\n892.3537\n898.5751\n893.6871\n\n\n1\n1\n1\n1\n1\n0\n892.3548\n898.5762\n893.6881\n\n\n1\n1\n2\n0\n1\n0\n892.2498\n898.4711\n893.5831\n\n\n2\n1\n0\n0\n1\n0\n896.7770\n901.4431\n897.5512\n\n\n2\n1\n0\n0\n1\n1\n898.7687\n904.9901\n900.1020\n\n\n2\n1\n0\n1\n1\n0\n898.7689\n904.9903\n900.1023\n\n\n2\n1\n1\n0\n1\n0\n891.5259\n897.7473\n892.8592\n\n\n3\n1\n0\n0\n1\n0\n874.9692\n881.1906\n876.3025\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n25 3 1 0 0 1 0 874.9692 881.1906 876.3025\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n25 3 1 0 0 1 0 874.9692 881.1906 876.3025\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n25 3 1 0 0 1 0 874.9692 881.1906 876.3025\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(3,1,0)x(0,1,0)12.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=diff(hos_ts2))\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n845.9925\n847.5478\n846.1137\n\n\n0\n1\n0\n0\n1\n1\n832.8488\n835.9595\n833.2238\n\n\n0\n1\n0\n1\n1\n0\n831.0523\n834.1630\n831.4273\n\n\n0\n1\n0\n1\n1\n1\n831.1107\n835.7767\n831.8849\n\n\n0\n1\n1\n0\n1\n0\n833.9298\n837.0405\n834.3048\n\n\n0\n1\n1\n0\n1\n1\n823.9534\n828.6195\n824.7276\n\n\n0\n1\n1\n1\n1\n0\n828.7642\n833.4302\n829.5384\n\n\n0\n1\n1\n1\n1\n1\n825.2345\n831.4559\n826.5678\n\n\n0\n1\n2\n0\n1\n0\n833.4111\n838.0771\n834.1853\n\n\n0\n1\n2\n0\n1\n1\n822.2415\n828.4629\n823.5748\n\n\n0\n1\n2\n1\n1\n0\n824.6883\n830.9097\n826.0216\n\n\n0\n1\n3\n0\n1\n0\n830.4527\n836.6741\n831.7860\n\n\n1\n1\n0\n0\n1\n0\n846.8351\n849.9458\n847.2101\n\n\n1\n1\n0\n0\n1\n1\n833.9114\n838.5775\n834.6856\n\n\n1\n1\n0\n1\n1\n0\n832.9176\n837.5837\n833.6918\n\n\n1\n1\n0\n1\n1\n1\n832.6582\n838.8796\n833.9915\n\n\n1\n1\n1\n0\n1\n0\n834.8774\n839.5434\n835.6515\n\n\n1\n1\n1\n0\n1\n1\n824.1566\n830.3780\n825.4899\n\n\n1\n1\n1\n1\n1\n0\n827.2692\n833.4906\n828.6025\n\n\n1\n1\n2\n0\n1\n0\n833.6106\n839.8320\n834.9440\n\n\n2\n1\n0\n0\n1\n0\n842.0853\n846.7514\n842.8595\n\n\n2\n1\n0\n0\n1\n1\n829.0510\n835.2724\n830.3844\n\n\n2\n1\n0\n1\n1\n0\n829.3747\n835.5961\n830.7080\n\n\n2\n1\n1\n0\n1\n0\n830.6702\n836.8916\n832.0036\n\n\n3\n1\n0\n0\n1\n0\n835.3499\n841.5713\n836.6832\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 1 2 0 1 1 822.2415 828.4629 823.5748\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 1 2 0 1 1 822.2415 828.4629 823.5748\n\n\nCode\noutput[which.min(output$AICc),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 1 2 0 1 1 822.2415 828.4629 823.5748\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(0,1,2)x(0,1,1)12.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=diff(hos_ts3))\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-114.0213\n-112.4660\n-113.9001\n\n\n0\n1\n0\n0\n1\n1\n-126.5420\n-123.4313\n-126.1670\n\n\n0\n1\n0\n1\n1\n0\n-128.0841\n-124.9734\n-127.7091\n\n\n0\n1\n0\n1\n1\n1\n-127.8453\n-123.1793\n-127.0711\n\n\n0\n1\n1\n0\n1\n0\n-124.8268\n-121.7162\n-124.4518\n\n\n0\n1\n1\n0\n1\n1\n-133.5901\n-128.9241\n-132.8159\n\n\n0\n1\n1\n1\n1\n0\n-129.0858\n-124.4198\n-128.3116\n\n\n0\n1\n1\n1\n1\n1\n-132.1038\n-125.8824\n-130.7705\n\n\n0\n1\n2\n0\n1\n0\n-127.2883\n-122.6222\n-126.5141\n\n\n0\n1\n2\n0\n1\n1\n-138.2548\n-132.0334\n-136.9214\n\n\n0\n1\n2\n1\n1\n0\n-136.5076\n-130.2863\n-135.1743\n\n\n0\n1\n3\n0\n1\n0\n-129.4209\n-123.1995\n-128.0876\n\n\n1\n1\n0\n0\n1\n0\n-112.5920\n-109.4813\n-112.2170\n\n\n1\n1\n0\n0\n1\n1\n-124.8998\n-120.2337\n-124.1256\n\n\n1\n1\n0\n1\n1\n0\n-126.0867\n-121.4207\n-125.3125\n\n\n1\n1\n0\n1\n1\n1\n-125.9527\n-119.7313\n-124.6193\n\n\n1\n1\n1\n0\n1\n0\n-124.3921\n-119.7261\n-123.6179\n\n\n1\n1\n1\n0\n1\n1\n-134.3442\n-128.1228\n-133.0108\n\n\n1\n1\n1\n1\n1\n0\n-127.0859\n-120.8645\n-125.7526\n\n\n1\n1\n2\n0\n1\n0\n-127.0127\n-120.7913\n-125.6794\n\n\n2\n1\n0\n0\n1\n0\n-120.4829\n-115.8168\n-119.7087\n\n\n2\n1\n0\n0\n1\n1\n-133.3714\n-127.1500\n-132.0381\n\n\n2\n1\n0\n1\n1\n0\n-133.1602\n-126.9388\n-131.8269\n\n\n2\n1\n1\n0\n1\n0\n-131.4478\n-125.2264\n-130.1145\n\n\n3\n1\n0\n0\n1\n0\n-126.1666\n-119.9452\n-124.8333\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q P D Q       AIC       BIC      AICc\n10 0 1 2 0 1 1 -138.2548 -132.0334 -136.9214\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q       AIC       BIC      AICc\n10 0 1 2 0 1 1 -138.2548 -132.0334 -136.9214\n\n\nCode\noutput[which.min(output$AICc),] \n\n\n   p d q P D Q       AIC       BIC      AICc\n10 0 1 2 0 1 1 -138.2548 -132.0334 -136.9214\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(0,1,2)x(0,1,1)12.\n\n\nDue to the presence of non-stationary seasonality in this time series data, we have opted to discontinue its use. Non-stationary seasonality implies that the patterns and trends within the data exhibit variations over time, without displaying a consistent and predictable behavior. As a result, attempting to model or analyze such data may lead to unreliable or inaccurate outcomes. Hence, to ensure the robustness and validity of our analyses, we have decided to cease utilizing this particular time series.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=stock_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n80.86001\n82.41536\n80.98122\n\n\n0\n1\n0\n0\n1\n1\n81.55934\n84.67004\n81.93434\n\n\n0\n1\n0\n1\n1\n0\n81.99495\n85.10564\n82.36995\n\n\n0\n1\n0\n1\n1\n1\n83.06165\n87.72769\n83.83584\n\n\n0\n1\n1\n0\n1\n0\n82.57429\n85.68499\n82.94929\n\n\n0\n1\n1\n0\n1\n1\n83.44977\n88.11581\n84.22396\n\n\n0\n1\n1\n1\n1\n0\n83.89083\n88.55687\n84.66502\n\n\n0\n1\n1\n1\n1\n1\n84.79458\n91.01598\n86.12792\n\n\n0\n1\n2\n0\n1\n0\n83.06013\n87.72618\n83.83433\n\n\n0\n1\n2\n0\n1\n1\n83.79587\n90.01727\n85.12921\n\n\n0\n1\n2\n1\n1\n0\n84.32594\n90.54733\n85.65927\n\n\n0\n1\n3\n0\n1\n0\n84.78325\n91.00465\n86.11659\n\n\n1\n1\n0\n0\n1\n0\n82.45053\n85.56123\n82.82553\n\n\n1\n1\n0\n0\n1\n1\n83.39813\n88.06418\n84.17233\n\n\n1\n1\n0\n1\n1\n0\n83.83987\n88.50591\n84.61406\n\n\n1\n1\n0\n1\n1\n1\n84.66777\n90.88916\n86.00111\n\n\n1\n1\n1\n0\n1\n0\n82.89926\n87.56530\n83.67345\n\n\n1\n1\n1\n0\n1\n1\n83.75634\n89.97773\n85.08967\n\n\n1\n1\n1\n1\n1\n0\n84.30468\n90.52607\n85.63802\n\n\n1\n1\n2\n0\n1\n0\n84.49396\n90.71535\n85.82729\n\n\n2\n1\n0\n0\n1\n0\n82.76803\n87.43407\n83.54222\n\n\n2\n1\n0\n0\n1\n1\n83.56090\n89.78229\n84.89423\n\n\n2\n1\n0\n1\n1\n0\n84.08517\n90.30657\n85.41851\n\n\n2\n1\n1\n0\n1\n0\n84.41491\n90.63631\n85.74825\n\n\n3\n1\n0\n0\n1\n0\n84.50528\n90.72667\n85.83861\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n  p d q P D Q      AIC      BIC     AICc\n1 0 1 0 0 1 0 80.86001 82.41536 80.98122\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q      AIC      BIC     AICc\n1 0 1 0 0 1 0 80.86001 82.41536 80.98122\n\n\nCode\noutput[which.min(output$AICc),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n1 0 1 0 0 1 0 80.86001 82.41536 80.98122\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(0,1,0)x(0,1,0)12.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=demo_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-129.3895\n-127.8059\n-129.2718\n\n\n0\n1\n0\n0\n1\n1\n-134.2483\n-131.0813\n-133.8847\n\n\n0\n1\n0\n1\n1\n0\n-135.0908\n-131.9238\n-134.7272\n\n\n0\n1\n0\n1\n1\n1\n-133.1372\n-128.3867\n-132.3872\n\n\n0\n1\n1\n0\n1\n0\n-144.8954\n-141.7283\n-144.5317\n\n\n0\n1\n1\n0\n1\n1\n-150.0106\n-145.2600\n-149.2606\n\n\n0\n1\n1\n1\n1\n0\n-150.8358\n-146.0853\n-150.0858\n\n\n0\n1\n1\n1\n1\n1\n-148.8645\n-142.5304\n-147.5741\n\n\n0\n1\n2\n0\n1\n0\n-143.2911\n-138.5405\n-142.5411\n\n\n0\n1\n2\n0\n1\n1\n-148.1194\n-141.7853\n-146.8291\n\n\n0\n1\n2\n1\n1\n0\n-148.8999\n-142.5658\n-147.6096\n\n\n0\n1\n3\n0\n1\n0\n-141.6947\n-135.3606\n-140.4043\n\n\n1\n1\n0\n0\n1\n0\n-135.8274\n-132.6603\n-135.4637\n\n\n1\n1\n0\n0\n1\n1\n-145.4978\n-140.7472\n-144.7478\n\n\n1\n1\n0\n1\n1\n0\n-148.0413\n-143.2908\n-147.2913\n\n\n1\n1\n0\n1\n1\n1\n-146.0553\n-139.7212\n-144.7649\n\n\n1\n1\n1\n0\n1\n0\n-143.3479\n-138.5974\n-142.5979\n\n\n1\n1\n1\n0\n1\n1\n-148.1797\n-141.8456\n-146.8894\n\n\n1\n1\n1\n1\n1\n0\n-148.9446\n-142.6105\n-147.6543\n\n\n1\n1\n2\n0\n1\n0\n-141.3706\n-135.0366\n-140.0803\n\n\n2\n1\n0\n0\n1\n0\n-134.8497\n-130.0992\n-134.0997\n\n\n2\n1\n0\n0\n1\n1\n-144.0340\n-137.7000\n-142.7437\n\n\n2\n1\n0\n1\n1\n0\n-146.6904\n-140.3563\n-145.4001\n\n\n2\n1\n1\n0\n1\n0\n-141.4355\n-135.1014\n-140.1452\n\n\n3\n1\n0\n0\n1\n0\n-135.3470\n-129.0129\n-134.0567\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n  p d q P D Q       AIC       BIC      AICc\n7 0 1 1 1 1 0 -150.8358 -146.0853 -150.0858\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q       AIC       BIC      AICc\n7 0 1 1 1 1 0 -150.8358 -146.0853 -150.0858\n\n\nCode\noutput[which.min(output$AICc),] \n\n\n  p d q P D Q       AIC       BIC      AICc\n7 0 1 1 1 1 0 -150.8358 -146.0853 -150.0858\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(0,1,1)x(1,1,0)12.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=inde_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-76.67213\n-75.08861\n-76.55449\n\n\n0\n1\n0\n0\n1\n1\n-86.88426\n-83.71722\n-86.52063\n\n\n0\n1\n0\n1\n1\n0\n-82.45823\n-79.29120\n-82.09460\n\n\n0\n1\n0\n1\n1\n1\n-84.93352\n-80.18297\n-84.18352\n\n\n0\n1\n1\n0\n1\n0\n-95.56052\n-92.39348\n-95.19688\n\n\n0\n1\n1\n0\n1\n1\n-103.76691\n-99.01636\n-103.01691\n\n\n0\n1\n1\n1\n1\n0\n-100.32623\n-95.57567\n-99.57623\n\n\n0\n1\n1\n1\n1\n1\n-101.77080\n-95.43673\n-100.48048\n\n\n0\n1\n2\n0\n1\n0\n-93.58167\n-88.83111\n-92.83167\n\n\n0\n1\n2\n0\n1\n1\n-101.87717\n-95.54309\n-100.58685\n\n\n0\n1\n2\n1\n1\n0\n-98.36084\n-92.02676\n-97.07052\n\n\n0\n1\n3\n0\n1\n0\n-93.12094\n-86.78686\n-91.83062\n\n\n1\n1\n0\n0\n1\n0\n-88.70640\n-85.53936\n-88.34277\n\n\n1\n1\n0\n0\n1\n1\n-98.25774\n-93.50719\n-97.50774\n\n\n1\n1\n0\n1\n1\n0\n-95.57919\n-90.82863\n-94.82919\n\n\n1\n1\n0\n1\n1\n1\n-96.32103\n-89.98695\n-95.03071\n\n\n1\n1\n1\n0\n1\n0\n-93.59064\n-88.84009\n-92.84064\n\n\n1\n1\n1\n0\n1\n1\n-101.91998\n-95.58591\n-100.62966\n\n\n1\n1\n1\n1\n1\n0\n-98.37787\n-92.04379\n-97.08755\n\n\n1\n1\n2\n0\n1\n0\n-91.99172\n-85.65764\n-90.70139\n\n\n2\n1\n0\n0\n1\n0\n-86.92499\n-82.17443\n-86.17499\n\n\n2\n1\n0\n0\n1\n1\n-96.45215\n-90.11807\n-95.16182\n\n\n2\n1\n0\n1\n1\n0\n-93.76546\n-87.43138\n-92.47513\n\n\n2\n1\n1\n0\n1\n0\n-92.77531\n-86.44123\n-91.48498\n\n\n3\n1\n0\n0\n1\n0\n-87.23273\n-80.89865\n-85.94241\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n  p d q P D Q       AIC       BIC      AICc\n6 0 1 1 0 1 1 -103.7669 -99.01636 -103.0169\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q P D Q       AIC       BIC      AICc\n6 0 1 1 0 1 1 -103.7669 -99.01636 -103.0169\n\n\nCode\noutput[which.min(output$AICc),] \n\n\n  p d q P D Q       AIC       BIC      AICc\n6 0 1 1 0 1 1 -103.7669 -99.01636 -103.0169\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(0,1,1)x(0,1,1)12.\n\n\n\n\nCode\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=2,Q1=1,Q2=2,data=rep_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-140.5431\n-138.9596\n-140.4254\n\n\n0\n1\n0\n0\n1\n1\n-147.6760\n-144.5090\n-147.3124\n\n\n0\n1\n0\n1\n1\n0\n-143.3595\n-140.1924\n-142.9958\n\n\n0\n1\n0\n1\n1\n1\n-145.7567\n-141.0062\n-145.0067\n\n\n0\n1\n1\n0\n1\n0\n-140.9421\n-137.7750\n-140.5784\n\n\n0\n1\n1\n0\n1\n1\n-148.9078\n-144.1573\n-148.1578\n\n\n0\n1\n1\n1\n1\n0\n-142.2549\n-137.5043\n-141.5049\n\n\n0\n1\n1\n1\n1\n1\n-147.0896\n-140.7555\n-145.7992\n\n\n0\n1\n2\n0\n1\n0\n-146.7962\n-142.0457\n-146.0462\n\n\n0\n1\n2\n0\n1\n1\n-152.7731\n-146.4390\n-151.4828\n\n\n0\n1\n2\n1\n1\n0\n-149.2569\n-142.9228\n-147.9665\n\n\n0\n1\n3\n0\n1\n0\n-144.8497\n-138.5156\n-143.5593\n\n\n1\n1\n0\n0\n1\n0\n-138.7525\n-135.5855\n-138.3889\n\n\n1\n1\n0\n0\n1\n1\n-146.5084\n-141.7579\n-145.7584\n\n\n1\n1\n0\n1\n1\n0\n-141.5539\n-136.8034\n-140.8039\n\n\n1\n1\n0\n1\n1\n1\n-144.7059\n-138.3718\n-143.4155\n\n\n1\n1\n1\n0\n1\n0\n-144.3857\n-139.6351\n-143.6357\n\n\n1\n1\n1\n0\n1\n1\n-152.0404\n-145.7063\n-150.7500\n\n\n1\n1\n1\n1\n1\n0\n-146.9071\n-140.5730\n-145.6167\n\n\n1\n1\n2\n0\n1\n0\n-144.8238\n-138.4898\n-143.5335\n\n\n2\n1\n0\n0\n1\n0\n-141.9897\n-137.2391\n-141.2397\n\n\n2\n1\n0\n0\n1\n1\n-146.8787\n-140.5447\n-145.5884\n\n\n2\n1\n0\n1\n1\n0\n-143.3547\n-137.0206\n-142.0644\n\n\n2\n1\n1\n0\n1\n0\n-145.9783\n-139.6443\n-144.6880\n\n\n3\n1\n0\n0\n1\n0\n-141.9418\n-135.6077\n-140.6514\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q P D Q       AIC      BIC      AICc\n10 0 1 2 0 1 1 -152.7731 -146.439 -151.4828\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q       AIC      BIC      AICc\n10 0 1 2 0 1 1 -152.7731 -146.439 -151.4828\n\n\nCode\noutput[which.min(output$AICc),] \n\n\n   p d q P D Q       AIC      BIC      AICc\n10 0 1 2 0 1 1 -152.7731 -146.439 -151.4828\n\n\nThe model with the lowest AIC, BIC amd AICc is ARIMA(0,1,2)x(0,1,1)12.\n\n\n\n\n\n12. Fitting Best SARIMA(p,d,q) & Diagnostics\nDue to the limitation where the number of lags exceeds the available number of observations in some of the time series data, we’ve adjusted these SARIMA model to utilize 4 lags instead of the initially intended 12. This adaptation ensures that the model remains feasible and effectively captures the temporal dependencies within the data, albeit with a reduced lag length. While this modification may slightly alter the model’s predictive capacity, it allows us to derive meaningful insights and forecasts while circumventing the constraint posed by the insufficient number of observations.\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(diff(diff(d_vacc_ts)), 0,1,1,0,1,0,4))\n\n\n\n\n\nCode\ncat(model_output[35:45], model_output[length(model_output)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n    Estimate     SE t.value p.value\nma1  -0.9153 0.2663 -3.4378  0.0023\n\nsigma^2 estimated as 8164902333 on 22 degrees of freedom \n \nAIC = 25.91335  AICc = 25.92164  BIC = 26.01209 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n$ttable: all coefficient is significant.\nThe equation for the model is: \\[x_t = w_t -0.9153w_{t-1} \\]\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(p_vacc_ts, 1,1,2,0,1,0,4))\n\n\n\n\n\nCode\ncat(model_output[14:26], model_output[length(model_output)], sep = \"\\n\") \n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n    Estimate     SE t.value p.value\nar1   0.4289 0.2617  1.6388  0.1161\nma1   0.9434 0.2609  3.6161  0.0016\nma2   0.3449 0.3130  1.1018  0.2830\n\nsigma^2 estimated as 2.270531 on 21 degrees of freedom \n \nAIC = 4.070457  AICc = 4.120457  BIC = 4.266799 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n$ttable: only ma1 is significant.\nThe equation for the model is: \\[x_t = 0.4289x_{t-1} + w_t + 0.9434w_{t-1} + 0.3449w_{t-2}\\]\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(pf_vacc_ts, 2,1,0,0,1,0,4))\n\n\n\n\n\nCode\ncat(model_output[19:30], model_output[length(model_output)], sep = \"\\n\")\n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n    Estimate     SE t.value p.value\nar1   1.4015 0.1538  9.1117       0\nar2  -0.8044 0.1419 -5.6687       0\n\nsigma^2 estimated as 2.090858 on 22 degrees of freedom \n \nAIC = 3.950764  AICc = 3.974574  BIC = 4.098021 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n$ttable: all coefficients are significant.\nThe equation for the model is: \\[x_t = 1.40159x_{t-1} - 0.8044x_{t-2}\\]\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(dead_ts, 0,1,2,0,1,1,12))\n\n\n\n\n\nCode\ncat(model_output[36:48], model_output[length(model_output)], sep = \"\\n\")\n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n     Estimate     SE t.value p.value\nma1    1.8515 0.2207  8.3910  0.0000\nma2    0.9997 0.2351  4.2512  0.0002\nsma1  -0.9977 0.5750 -1.7352  0.0941\n\nsigma^2 estimated as 85591993 on 27 degrees of freedom \n \nAIC = 22.10196  AICc = 22.13273  BIC = 22.28878 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n$ttable: ma1 and ma2 are significant.\nThe equation for the model is: \\[x_t = 1.8515x_{t-1} + 0.9997x_{t-2} - 0.9977 + a_{t}\\]\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(diff(hos_ts1), 3,1,0,0,1,0,12))\n\n\n\n\n\nCode\ncat(model_output[32:44], model_output[length(model_output)], sep = \"\\n\")\n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n    Estimate     SE  t.value p.value\nar1  -0.4824 0.1197  -4.0287  0.0003\nar2  -0.4614 0.1364  -3.3828  0.0019\nar3  -0.8923 0.0828 -10.7763  0.0000\n\nsigma^2 estimated as 2912500402 on 32 degrees of freedom \n \nAIC = 24.99912  AICc = 25.02124  BIC = 25.17687 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n$ttable: all coefficients are significant.\nThe equation for the model is: \\[x_t = -0.4824x_{t-1} - 0.4614x_{t-2} - 0.8923x_{t-2}\\]\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(diff(hos_ts2), 0,1,2,0,1,1,12))\n\n\n\n\n\nCode\ncat(model_output[22:34], model_output[length(model_output)], sep = \"\\n\")\n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n     Estimate     SE t.value p.value\nma1   -0.5988 0.2210 -2.7100  0.0107\nma2   -0.4011 0.1836 -2.1847  0.0363\nsma1  -0.9994 0.3640 -2.7454  0.0098\n\nsigma^2 estimated as 416745453 on 32 degrees of freedom \n \nAIC = 23.49261  AICc = 23.51473  BIC = 23.67037 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n$ttable: all coefficients are significant.\nThe equation for the model is: \\[x_t = -0.5988x_{t-1} - 0.4011x_{t-2} - 0.9994 + a_{t}\\]\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(diff(hos_ts3), 0,1,2,0,1,1,12))\n\n\n\n\n\nCode\ncat(model_output[22:34], model_output[length(model_output)], sep = \"\\n\")\n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n     Estimate     SE t.value p.value\nma1   -0.4717 0.2009 -2.3476  0.0252\nma2   -0.5283 0.1702 -3.1046  0.0040\nsma1  -0.9997 0.3681 -2.7156  0.0106\n\nsigma^2 estimated as 0.0005031713 on 32 degrees of freedom \n \nAIC = -3.950136  AICc = -3.928016  BIC = -3.772382 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n$ttable: all coefficients are significant.\nThe equation for the model is: \\[x_t = -0.4717x_{t-1} - 0.5283x_{t-2} - 0.9997 + a_{t}\\]\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(stock_ts, 0,1,0,0,1,0,12))\n\n\n\n\n\nCode\ncat(model_output[1:9], model_output[length(model_output)], sep = \"\\n\")\n\n\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n     Estimate p.value\n\nsigma^2 estimated as 0.5572542 on 35 degrees of freedom \n \nAIC = 2.310286  AICc = 2.310286  BIC = 2.354725 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(demo_ts, 0,1,1,1,1,0,12))\n\n\n\n\n\nCode\ncat(model_output[23:34], model_output[length(model_output)], sep = \"\\n\")\n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n     Estimate     SE t.value p.value\nma1   -0.8685 0.1540 -5.6389  0.0000\nsar1  -0.5087 0.1495 -3.4021  0.0017\n\nsigma^2 estimated as 0.0006502313 on 34 degrees of freedom \n \nAIC = -4.189884  AICc = -4.179783  BIC = -4.057924 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\nThe equation for the model is: \\[x_t = -0.8685x_{t-1} - 0.5087 + a_{t}\\]\n$ttable: all coefficients are significant.\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(inde_ts, 0,1,1,0,1,1,12))\n\n\n\n\n\nCode\ncat(model_output[28:39], model_output[length(model_output)], sep = \"\\n\")\n\n\nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n     Estimate     SE t.value p.value\nma1   -0.9999 0.2161 -4.6278  0.0001\nsma1  -0.9998 0.6468 -1.5458  0.1314\n\nsigma^2 estimated as 0.001547766 on 34 degrees of freedom \n \nAIC = -2.882414  AICc = -2.872313  BIC = -2.750454 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n$ttable: only ma1 are significant.\nThe equation for the model is: \\[x_t = -0.9999x_{t-1} - 0.9998 + a_{t}\\]\n\n\n\n\nCode\nset.seed(123)\nmodel_output &lt;- capture.output(sarima(rep_ts, 0,1,2,0,1,1,12))\n\n\n\n\n\nCode\ncat(model_output[25:34], model_output[length(model_output)], sep = \"\\n\")\n\n\nCoefficients: \n     Estimate     SE t.value p.value\nma1   -0.4338 0.1777 -2.4408  0.0202\nma2   -0.4032 0.1828 -2.2057  0.0345\nsma1  -0.9999 0.5380 -1.8587  0.0720\n\nsigma^2 estimated as 0.0004077668 on 33 degrees of freedom \n \nAIC = -4.243697  AICc = -4.222864  BIC = -4.06775 \n \n \n\n\nThe Standard Residual Plot appears good, displaying stationarity with a nearly constant mean and variation.\nThe Autocorrelation Function (ACF) Plot shows almost no correlation indicating that the model has harnessed everything and all that is left is white noise. This indicates a good model fit.\nThe Quantile-Quantile (Q-Q) Plot demonstrates near-normality.\nThe Ljung-Box test results reveal values above the 0.05 (5% significance) threshold, indicating a good fit.\n$ttable: all coefficients are significant.\nThe equation for the model is: \\[x_t = -0.4338x_{t-1} - 0.4032x_{t-2} - 0.9999 + a_{t}\\]\n\n\n\n\n\n13. Forecasting\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\ntime &lt;- Arima(diff(diff(d_vacc_ts)), order=c(0,1,1), seasonal=c(0,1,0))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(p_vacc_ts, order=c(1,1,2), seasonal=c(0,1,0))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(pf_vacc_ts, order=c(2,1,0), seasonal=c(0,1,0))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(dead_ts, order=c(0,1,2), seasonal=c(0,1,1))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(diff(hos_ts1), order=c(3,1,0), seasonal=c(0,1,0))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(diff(hos_ts2), order=c(0,1,2), seasonal=c(0,1,1))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(diff(hos_ts3), order=c(0,1,2), seasonal=c(0,1,1))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(stock_ts, order=c(0,1,0), seasonal=c(0,1,0))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(demo_ts, order=c(0,1,1), seasonal=c(1,1,0))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(inde_ts, order=c(0,1,1), seasonal=c(0,1,1))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\nCode\ntime &lt;- Arima(rep_ts, order=c(0,1,2), seasonal=c(0,1,1))\n\n# forecast next three years\ntime %&gt;% forecast(h=36) %&gt;% autoplot()+theme_bw() +\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\n\n\n\n\n\n14. SARIMA vs. Benchmarks\n\nDaily vaccinations time seriesPeople vaccinated time seriesPeople fully vaccinated time seriesNewly confirmed case time seriesDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesUnemployment rate time seriesPfizer stock price time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(d_vacc_ts, order=c(2, 1, 2),include.drift = FALSE) \nautoplot(d_vacc_ts) +\n  autolayer(meanf(d_vacc_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(d_vacc_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(d_vacc_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(d_vacc_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Daily Vaccinations\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot you’ve uploaded appears to be a time series chart comparing different forecasting methods against actual data. The actual data is represented by the black line, and it shows daily vaccinations up to a certain point in time. The colored lines represent forecasts from different models, including ARIMA, Drift, Mean, and Naïve methods, projected beyond the actual data into future dates. Each forecast method predicts a different outcome for future vaccination numbers, with the ARIMA model suggesting a continued steady rate, while other models predict various levels of change or stability. Unfortunately, I can’t display the plot here, but I can provide descriptions or summaries of visual data when you upload it.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(p_vacc_ts, order=c(0, 1, 3),include.drift = FALSE) \nautoplot(p_vacc_ts) +\n  autolayer(meanf(p_vacc_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(p_vacc_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(p_vacc_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(p_vacc_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with People Vaccinated\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThis plot appears to be comparing the forecasted number of people vaccinated using different benchmark methods against actual historical data. The black line represents the historical data of people vaccinated over time. The different colored lines at the end of the historical data represent forecasts made by different models for future vaccinations: Arima, Drift, Mean, and Naïve. The Arima model shows a sharp upward trend, suggesting a significant increase in vaccinations, while the other models forecast a relatively steady or only slightly increasing trend.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(pf_vacc_ts, order=c(0, 1, 3),include.drift = FALSE) \nautoplot(pf_vacc_ts) +\n  autolayer(meanf(pf_vacc_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(pf_vacc_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(pf_vacc_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(pf_vacc_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with People Fully Vaccinated\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot appears to be a time series graph that shows data on the number of people fully vaccinated over time. The black line represents the actual historical data, showing a steady increase in the number of fully vaccinated individuals over time. The colored lines represent different forecasting methods projected into the future (2024 and beyond), such as ARIMA (Autoregressive Integrated Moving Average), Drift, Mean, and Naïve forecasting. Each method provides a different projection, indicating varying expectations about future vaccination trends based on past data.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(case_ts, order=c(0, 1, 2),include.drift = FALSE) \nautoplot(case_ts) +\n  autolayer(meanf(case_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(case_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(case_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(case_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Newly Confirmed Case\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot appears to show a comparison of different forecasting methods for a time series data set related to newly confirmed cases of a condition, likely COVID-19, given the context. The black line represents the actual historical data, while the various colored lines project into the future with forecasts from different methods. The ARIMA forecast (red) predicts an increase, while the Drift (green), Mean (blue), and Naïve (purple) methods predict a flat or slightly varied continuation of the most recent data. This type of visualization is used to compare the predictive performance of different statistical or machine learning models.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(dead_ts, order=c(0, 1, 6),include.drift = FALSE) \nautoplot(dead_ts) +\n  autolayer(meanf(dead_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(dead_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(dead_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(dead_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Death Case\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot is a comparison of forecast methods for death cases over time, with historical data shown by the black line increasing from 2020 through 2023. On the y-axis, death cases are plotted on a logarithmic scale, allowing for a wide range of values. Past 2023, four different forecast methods are shown: ARIMA (red line), suggesting a continuous increase; Drift (green line), indicating a more moderate increase; Mean (blue line), projecting a flat trend indicating no change from the last actual data point; and Naive (purple line), also predicting no change moving forward. These forecasts are meant to predict future values based on the historical trend and their respective statistical assumptions. The plot serves as a visual assessment tool to compare how each method anticipates the future based on the given data.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(hos_ts1, order=c(0, 1, 1),include.drift = FALSE) \nautoplot(hos_ts1) +\n  autolayer(meanf(hos_ts1, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(hos_ts1, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(hos_ts1, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(hos_ts1 ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Inpatient Bed Number\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThis graph illustrates the comparison of various forecasting methods applied to the number of inpatient beds over time. The historical data, plotted as a black line, shows a sharp increase in the number of beds around 2020, followed by a stabilization around 6e+05 (600,000 beds). Post-2023, the graph features predictions from different statistical models: ARIMA (red line) predicts a significant increase in bed numbers; Drift (green line) forecasts a slight upward trend; Mean (blue line) suggests the number will remain constant, equal to the historical average; and Naive (purple line) assumes no change, extending the last data point forward. The plot serves as a tool to visualize and evaluate how these models anticipate changes in hospital bed availability, with each model’s forecast based on its specific methodological approach to the existing data.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(hos_ts2, order=c(2, 1, 1),include.drift = FALSE) \nautoplot(hos_ts2) +\n  autolayer(meanf(hos_ts2, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(hos_ts2, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(hos_ts2, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(hos_ts2 ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Inpatient Bed Number Used for COVID\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot depicts the number of inpatient hospital beds used for COVID-19 over time, showing a volatile history with several peaks, particularly notable in 2020 and 2021. The time series data, illustrated by the black line, exhibits sharp increases and decreases, suggesting waves or surges in hospital bed usage due to the pandemic. Looking into the future beyond the historical data, various forecasting methods have been applied, shown by the horizontal lines after 2023: ARIMA (red) forecasts a slight increase, Drift (green) indicates stability with a very mild upward trend, Mean (blue) predicts a constant number equal to the historical average, and Naive (purple) extends the last observed data point forward, assuming no change. These forecasts provide a range of potential future scenarios for hospital bed usage, reflecting the differing assumptions and calculations inherent to each forecasting model.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(hos_ts3, order=c(2, 0, 0),include.drift = FALSE) \nautoplot(hos_ts3) +\n  autolayer(meanf(hos_ts3, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(hos_ts3, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(hos_ts3, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(hos_ts3 ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Utilization Rate for Inpatient Bed Used for COVID\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThis plot compares different forecasting methods for the utilization rate of inpatient beds used for COVID-19, as indicated by the historical data (black line) from 2020 to 2023. The y-axis represents the utilization rate, which shows significant fluctuations, peaking notably at various points likely corresponding to waves of COVID-19 cases. The forecast methods, represented by colored lines beyond 2023, provide different projections: ARIMA (red line) shows a slight increasing trend, Drift (green line) indicates a marginal increase, Mean (blue line) suggests a flat trend at the historical average, and Naive (purple line) extends the last observed data point, assuming the rate will remain unchanged. These models are used to predict future bed utilization, offering a visual tool for comparing the potential accuracy and assumptions of each method against future real-world data.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(unemploy_ts, order=c(1, 0, 1),include.drift = FALSE) \nautoplot(unemploy_ts) +\n  autolayer(meanf(unemploy_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(unemploy_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(unemploy_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(unemploy_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Unemployment Rate\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThis plot displays the historical trend and forecasts of the unemployment rate from 2020 to beyond 2022. The black line represents the actual historical unemployment rate, which shows a sharp spike in 2020, followed by a general decline over the subsequent years. Looking to the future, the forecasts made by different models are represented by the horizontal lines: ARIMA (red line) predicts a downward trend, continuing the decline of the unemployment rate; Drift (green line) suggests a stable rate, maintaining the last observed rate; Mean (blue line) forecasts that the unemployment rate will average out to a steady state, ignoring the downward trend; and Naive (purple line) projects no change, extending the last observed point into the future. The plot serves to compare how these forecasting methods project the future of unemployment rates based on the past data and their respective algorithmic interpretations.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(stock_ts, order=c(1, 0, 3),include.drift = FALSE) \nautoplot(stock_ts) +\n  autolayer(meanf(stock_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(stock_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(stock_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(stock_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Pfizer Stock Price\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot shows the historical performance and future forecast of Pfizer’s stock price. The black line represents the stock price from 2020 through part of 2023, with the price experiencing volatility and an overall downward trend. Projected forecasts beyond the historical data are made using four methods: ARIMA (red line) predicts a continuing decline; Drift (green line) forecasts a slight increase; Mean (blue line) suggests the stock price will level off to the average of the historical data; and Naive (purple line) assumes the stock price will remain constant at the last observed value. These different forecasts highlight the variability in predicting stock prices depending on the modeling technique used, with each forecast method taking a unique approach to extrapolate future prices from the past data.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(demo_ts, order=c(0, 0, 1),include.drift = FALSE) \nautoplot(demo_ts) +\n  autolayer(meanf(demo_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(demo_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(demo_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(demo_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Support Rate for Democratic\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe graph illustrates the historical support rate for the Democratic Party (presumably in the United States) from 2020 to the latter part of 2023, along with projected forecasts using various methods. The support rate, shown by the black line, fluctuates over time with notable volatility but remains within a band between approximately 0.86 and 0.92. The future forecasts, indicated by the lines extending from the end of 2023 to 2025, predict the support rate using different statistical models: ARIMA (red) forecasts a stable support rate continuing from the last observed point; Drift (green) also suggests a stable but slightly declining trend; Mean (blue) projects that the support rate will level off to the historical mean, and Naive (purple) predicts no change, carrying the last observed support rate forward. These projections provide a range of scenarios for future party support, each based on different assumptions about the patterns in the historical data.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(inde_ts, order=c(0, 0, 1),include.drift = FALSE) \nautoplot(inde_ts) +\n  autolayer(meanf(inde_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(inde_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(inde_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(inde_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Support Rate for Independent\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThe plot displays the fluctuating support rate for Independents, presumably in a political context, from 2020 through 2023, and forecasts for this rate into 2025 using different statistical methods. The black line shows actual historical data, indicating that support for Independents has varied, with rates moving between just below 0.25 and around 0.35. Post-2023, the forecast lines suggest different future trends: the ARIMA model (red) predicts a very slight decline, the Drift method (green) suggests a constant rate with a small upward tendency, the Mean (blue) indicates a flat forecast at the historical average, and the Naive approach (purple) projects the rate will remain unchanged at the last observed point. These projections offer diverse perspectives on potential future support for Independents based on past patterns.\n\n\n\n\nCode\nfit_techmu_bench &lt;- Arima(rep_ts, order=c(1, 0, 1),include.drift = FALSE) \nautoplot(rep_ts) +\n  autolayer(meanf(rep_ts, h=10),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(rep_ts, h=10),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(rep_ts, drift=TRUE, h=10),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(forecast(rep_ts ,10), \n            series=\"Arima\",PI=FALSE) + theme_bw() + ggtitle(\"Benchmark Methods Comparison with Support Rate for Republican\")+\n  guides(colour=guide_legend(title=\"Forecast\")) \n\n\n\n\n\nThis plot presents the support rate for the Republican Party from 2020 through 2023 and includes projections to 2025 based on various forecasting models. The support rate, depicted by the black line, exhibits volatility with values oscillating primarily between 0.05 and 0.1. The forecast section post-2023 features predictions from different models: ARIMA (red line) suggests a decrease in support; Drift (green line) forecasts a small upward trend; Mean (blue line) indicates that support will stabilize at the historical average rate; and Naive (purple line) predicts the support rate will remain constant at the end of the observed data. These differing forecasts provide insights into possible future trends of Republican support, each based on distinct statistical assumptions and calculations.\n\n\n\n\n\n15. Cross Validation\nWe did cross validation to select the best performed model to those time series with over 40 samples. Since the Auto Arima function did not provide us with the best SARIMA model, we use the model with lowest AIC and second lowest AIC to compare their performance.\n\nDeath case time seriesInpatient bed time seriesInpatient bed used for COVID time seriesUtilization rate for inpatient bed used for COVID time seriesSupport rate for democratic time seriesSupport rate for independent time seriesSupport rate for republican time series\n\n\nIn the death case time series, we select model ARIMA(0,1,2)x(0,1,1)12(lowest AIC) and model ARIMA(0,1,2)x(1,1,0)12(second lowest AIC).\n\n\nCode\n#n=length(dead_ts)\n#n-k=24; 24/12=2; \nk=19\nmae1 &lt;- matrix(NA, 2,12) \nmae2 &lt;- matrix(NA, 2,12)\n\nst &lt;- tsp(dead_ts)[1]+(k-1)/12 #24 observations\n# put up to 2\nfor(i in 1:2)\n{\n  #xtrain &lt;- window(a10, start=st+(i-k+1)/12, end=st+i/12)\n  xtrain &lt;- window(dead_ts, end=st + i-1)\n  xtest &lt;- window(dead_ts, start=st + (i-1) + 1/12, end=st + i)\n  # 1st model\n  fit &lt;- Arima(xtrain, order=c(0,1,2), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast &lt;- forecast(fit, h=1)\n  # 2nd Arima\n  fit2 &lt;- Arima(xtrain, order=c(0,1,2), seasonal=list(order=c(1,1,0), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 &lt;- forecast(fit2, h=1)\n  \n  mae1[i,] &lt;- abs(fcast$mean-xtest)\n  mae2[i,] &lt;- abs(fcast2$mean-xtest)\n  \n}\n\nmax_mae &lt;- max(c(colMeans(mae1, na.rm = TRUE), colMeans(mae2, na.rm = TRUE)), na.rm = TRUE)\nylim_range &lt;- c(0, max_mae + max_mae * 0.3)\nplot(1:12, colMeans(mae1, na.rm = TRUE), type = \"l\", col = 2, xlab = \"Horizon\", ylab = \"MAE\", ylim = ylim_range)\nlines(1:12, colMeans(mae2, na.rm = TRUE), type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"Model with lowest AIC\", \"Model with second lowest AIC\"), col = 2:3, lty = 1)\n\n\n\n\n\nWe can see the model with the lowest AIC actually performs better!\n\n\nIn the inpatient bed time series, we select model ARIMA(3,1,0)x(0,1,0)12(lowest AIC) and model ARIMA(1,1,1)x(0,1,0)12(second lowest AIC).\n\n\nCode\n#n=length(hos_ts1) 49\n#n-k=25; 24/12=2; \nk=25\nmae1 &lt;- matrix(NA, 2,12) \nmae2 &lt;- matrix(NA, 2,12)\n\nst &lt;- tsp(hos_ts1)[1]+(k-1)/12 #24 observations\n# put up to 2\nfor(i in 1:2)\n{\n  #xtrain &lt;- window(a10, start=st+(i-k+1)/12, end=st+i/12)\n  xtrain &lt;- window(hos_ts1, end=st + i-1)\n  xtest &lt;- window(hos_ts1, start=st + (i-1) + 1/12, end=st + i)\n  # 1st model\n  fit &lt;- Arima(xtrain, order=c(3,1,0), seasonal=list(order=c(0,1,0), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast &lt;- forecast(fit, h=1)\n  # 2nd Arima\n  fit2 &lt;- Arima(xtrain, order=c(1,1,1), seasonal=list(order=c(0,1,0), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 &lt;- forecast(fit2, h=1)\n  \n  mae1[i,] &lt;- abs(fcast$mean-xtest)\n  mae2[i,] &lt;- abs(fcast2$mean-xtest)\n  \n}\n\nmax_mae &lt;- max(c(colMeans(mae1, na.rm = TRUE), colMeans(mae2, na.rm = TRUE)), na.rm = TRUE)\nylim_range &lt;- c(0, max_mae + max_mae * 0.3)\nplot(1:12, colMeans(mae1, na.rm = TRUE), type = \"l\", col = 2, xlab = \"Horizon\", ylab = \"MAE\", ylim = ylim_range)\nlines(1:12, colMeans(mae2, na.rm = TRUE), type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"Model with lowest AIC\", \"Model with second lowest AIC\"), col = 2:3, lty = 1)\n\n\n\n\n\nWe can see the model with the second lowest AIC actually performs better!\n\n\nIn the inpatient bed used for COVIS time series, we select model ARIMA(0,1,2)x(0,1,1)12(lowest AIC) and model ARIMA(1,1,1)x(0,1,1)12(second lowest AIC).\n\n\nCode\n#n=length(hos_ts2) 49\n#n-k=25; 24/12=2; \nk=25\nmae1 &lt;- matrix(NA, 2,12) \nmae2 &lt;- matrix(NA, 2,12)\n\nst &lt;- tsp(hos_ts2)[1]+(k-1)/12 #24 observations\n# put up to 2\nfor(i in 1:2)\n{\n  #xtrain &lt;- window(a10, start=st+(i-k+1)/12, end=st+i/12)\n  xtrain &lt;- window(hos_ts2, end=st + i-1)\n  xtest &lt;- window(hos_ts2, start=st + (i-1) + 1/12, end=st + i)\n  # 1st model\n  fit &lt;- Arima(xtrain, order=c(0,1,2), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast &lt;- forecast(fit, h=1)\n  # 2nd Arima\n  fit2 &lt;- Arima(xtrain, order=c(1,1,1), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 &lt;- forecast(fit2, h=1)\n  \n  mae1[i,] &lt;- abs(fcast$mean-xtest)\n  mae2[i,] &lt;- abs(fcast2$mean-xtest)\n  \n}\n\nmax_mae &lt;- max(c(colMeans(mae1, na.rm = TRUE), colMeans(mae2, na.rm = TRUE)), na.rm = TRUE)\nylim_range &lt;- c(0, max_mae + max_mae * 0.3)\nplot(1:12, colMeans(mae1, na.rm = TRUE), type = \"l\", col = 2, xlab = \"Horizon\", ylab = \"MAE\", ylim = ylim_range)\nlines(1:12, colMeans(mae2, na.rm = TRUE), type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"Model with lowest AIC\", \"Model with second lowest AIC\"), col = 2:3, lty = 1)\n\n\n\n\n\nWe can see the model with the lowest AIC actually performs better!\n\n\nIn the utilization rate for inpatient bed used for COVID time series, we select model ARIMA(0,1,2)x(0,1,1)12(lowest AIC) and model ARIMA(0,1,2)x(1,1,0)12(second lowest AIC).\n\n\nCode\n#n=length(hos_ts3) 49\n#n-k=25; 24/12=2; \nk=25\nmae1 &lt;- matrix(NA, 2,12) \nmae2 &lt;- matrix(NA, 2,12)\n\nst &lt;- tsp(hos_ts3)[1]+(k-1)/12 #24 observations\n# put up to 2\nfor(i in 1:2)\n{\n  #xtrain &lt;- window(a10, start=st+(i-k+1)/12, end=st+i/12)\n  xtrain &lt;- window(hos_ts3, end=st + i-1)\n  xtest &lt;- window(hos_ts3, start=st + (i-1) + 1/12, end=st + i)\n  # 1st model\n  fit &lt;- Arima(xtrain, order=c(0,1,2), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast &lt;- forecast(fit, h=1)\n  # 2nd Arima\n  fit2 &lt;- Arima(xtrain, order=c(0,1,2), seasonal=list(order=c(1,1,0), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 &lt;- forecast(fit2, h=1)\n  \n  mae1[i,] &lt;- abs(fcast$mean-xtest)\n  mae2[i,] &lt;- abs(fcast2$mean-xtest)\n  \n}\n\nmax_mae &lt;- max(c(colMeans(mae1, na.rm = TRUE), colMeans(mae2, na.rm = TRUE)), na.rm = TRUE)\nylim_range &lt;- c(0, max_mae + max_mae * 0.3)\nplot(1:12, colMeans(mae1, na.rm = TRUE), type = \"l\", col = 2, xlab = \"Horizon\", ylab = \"MAE\", ylim = ylim_range)\nlines(1:12, colMeans(mae2, na.rm = TRUE), type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"Model with lowest AIC\", \"Model with second lowest AIC\"), col = 2:3, lty = 1)\n\n\n\n\n\nWe can see the model with the second lowest AIC actually performs better!\n\n\nIn the support rate for democratic time series, we select model ARIMA(0,1,1)x(1,1,0)12(lowest AIC) and model ARIMA(1,1,1)x(1,1,0)12(second lowest AIC).\n\n\nCode\n#n=length(demo_ts) 49\n#n-k=25; 24/12=2; \nk=25\nmae1 &lt;- matrix(NA, 2,12) \nmae2 &lt;- matrix(NA, 2,12)\n\nst &lt;- tsp(demo_ts)[1]+(k-1)/12 #24 observations\n# put up to 2\nfor(i in 1:2)\n{\n  #xtrain &lt;- window(a10, start=st+(i-k+1)/12, end=st+i/12)\n  xtrain &lt;- window(demo_ts, end=st + i-1)\n  xtest &lt;- window(demo_ts, start=st + (i-1) + 1/12, end=st + i)\n  # 1st model\n  fit &lt;- Arima(xtrain, order=c(0,1,1), seasonal=list(order=c(1,1,0), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast &lt;- forecast(fit, h=1)\n  # 2nd Arima\n  fit2 &lt;- Arima(xtrain, order=c(1,1,1), seasonal=list(order=c(1,1,0), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 &lt;- forecast(fit2, h=1)\n  \n  mae1[i,] &lt;- abs(fcast$mean-xtest)\n  mae2[i,] &lt;- abs(fcast2$mean-xtest)\n  \n}\n\nmax_mae &lt;- max(c(colMeans(mae1, na.rm = TRUE), colMeans(mae2, na.rm = TRUE)), na.rm = TRUE)\nylim_range &lt;- c(0, max_mae + max_mae * 0.3)\nplot(1:12, colMeans(mae1, na.rm = TRUE), type = \"l\", col = 2, xlab = \"Horizon\", ylab = \"MAE\", ylim = ylim_range)\nlines(1:12, colMeans(mae2, na.rm = TRUE), type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"Model with lowest AIC\", \"Model with second lowest AIC\"), col = 2:3, lty = 1) \n\n\n\n\n\nWe can see the model with the second lowest AIC actually performs better!\n\n\nIn the support rate for independent time series, we select model ARIMA(0,1,1)x(0,1,1)12(lowest AIC) and model ARIMA(1,1,1)x(0,1,1)12(second lowest AIC).\n\n\nCode\n#n=length(inde_ts) 49\n#n-k=25; 24/12=2; \nk=25\nmae1 &lt;- matrix(NA, 2,12) \nmae2 &lt;- matrix(NA, 2,12)\n\nst &lt;- tsp(inde_ts)[1]+(k-1)/12 #24 observations\n# put up to 2\nfor(i in 1:2)\n{\n  #xtrain &lt;- window(a10, start=st+(i-k+1)/12, end=st+i/12)\n  xtrain &lt;- window(inde_ts, end=st + i-1)\n  xtest &lt;- window(inde_ts, start=st + (i-1) + 1/12, end=st + i)\n  # 1st model\n  fit &lt;- Arima(xtrain, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast &lt;- forecast(fit, h=1)\n  # 2nd Arima\n  fit2 &lt;- Arima(xtrain, order=c(1,1,1), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 &lt;- forecast(fit2, h=1)\n  \n  mae1[i,] &lt;- abs(fcast$mean-xtest)\n  mae2[i,] &lt;- abs(fcast2$mean-xtest)\n  \n}\n\nmax_mae &lt;- max(c(colMeans(mae1, na.rm = TRUE), colMeans(mae2, na.rm = TRUE)), na.rm = TRUE)\nylim_range &lt;- c(0, max_mae + max_mae * 0.3)\nplot(1:12, colMeans(mae1, na.rm = TRUE), type = \"l\", col = 2, xlab = \"Horizon\", ylab = \"MAE\", ylim = ylim_range)\nlines(1:12, colMeans(mae2, na.rm = TRUE), type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"Model with lowest AIC\", \"Model with second lowest AIC\"), col = 2:3, lty = 1) \n\n\n\n\n\nWe can see the model with the second lowest AIC actually performs better!\n\n\nIn the support rate for republican time series, we select model ARIMA(0,1,2)x(0,1,1)12(lowest AIC) and model ARIMA(1,1,1)x(0,1,1)12(second lowest AIC).\n\n\nCode\n#n=length(rep_ts) 49\n#n-k=25; 24/12=2; \nk=25\nmae1 &lt;- matrix(NA, 2,12) \nmae2 &lt;- matrix(NA, 2,12)\n\nst &lt;- tsp(rep_ts)[1]+(k-1)/12 #24 observations\n# put up to 2\nfor(i in 1:2)\n{\n  #xtrain &lt;- window(a10, start=st+(i-k+1)/12, end=st+i/12)\n  xtrain &lt;- window(rep_ts, end=st + i-1)\n  xtest &lt;- window(rep_ts, start=st + (i-1) + 1/12, end=st + i)\n  # 1st model\n  fit &lt;- Arima(xtrain, order=c(0,1,2), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast &lt;- forecast(fit, h=1)\n  # 2nd Arima\n  fit2 &lt;- Arima(xtrain, order=c(1,1,1), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 &lt;- forecast(fit2, h=1)\n  \n  mae1[i,] &lt;- abs(fcast$mean-xtest)\n  mae2[i,] &lt;- abs(fcast2$mean-xtest)\n  \n}\n\nmax_mae &lt;- max(c(colMeans(mae1, na.rm = TRUE), colMeans(mae2, na.rm = TRUE)), na.rm = TRUE)\nylim_range &lt;- c(0, max_mae + max_mae * 0.3)\nplot(1:12, colMeans(mae1, na.rm = TRUE), type = \"l\", col = 2, xlab = \"Horizon\", ylab = \"MAE\", ylim = ylim_range)\nlines(1:12, colMeans(mae2, na.rm = TRUE), type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"Model with lowest AIC\", \"Model with second lowest AIC\"), col = 2:3, lty = 1) \n\n\n\n\n\nWe can see the model with the second lowest AIC actually performs better!"
  },
  {
    "objectID": "MT.html",
    "href": "MT.html",
    "title": "Multivariate TS Models",
    "section": "",
    "text": "In this section, we delve into the application of ARIMAX, SARIMAX, and VAR modeling techniques across a range of time series datasets pertinent to our project’s scope. ARIMAX and SARIMAX models come into play when we observe a unidirectional influence from one variable onto another. Conversely, VAR models are more suited to scenarios where a set of variables exhibits mutual influences.\nInitiating this phase of the project involved a comprehensive literature review to identify the models previously employed to analyze COVID vaccination-related variables. The study by Yundari (2022) leverages the Vector Autoregressive (VAR) model to simultaneously analyze the impact of vaccination numbers (first dose) on new and recovered COVID-19 cases, utilizing daily data from January 13 to December 30, 2021, in West Kalimantan Province. Ye (2021) underscores the critical nature of escalating COVID-19 Daily Vaccination Numbers to curtail the pandemic, noting that achieving vaccination targets remains an uphill task. This study probes into how political partisanship might influence Daily Vaccination Numbers, drawing comparisons between Democratic and Republican counties. Asch (2024) delves into the correlation between political leanings and the reporting of vaccine adverse events (AEs), revealing that a 10% uptick in Republican votes at the state level correlates with a 5% increase in the likelihood of reporting a COVID-19 vaccine AE.\nReferences\n\nYundari, Y., & Huda, N. M. Analysis of the Impact of Vaccination on Daily New and Recovered COVID-19 Cases Using the Vector Autoregressive (VAR) Model: A Case Study of West Kalimantan. BAREKENG: Jurnal Ilmu Matematika dan Terapan. https://ojs3.unpatti.ac.id/index.php/barekeng/article/view/5266\nYe, X. (2021). Exploring the Relationship Between Political Partisanship and COVID-19 Vaccination Rate. https://academic.oup.com/jpubhealth/article/45/1/91/6409075\nAsch, D.A., Luo, C., & Chen, Y. (2024). Reports of COVID-19 Vaccine Adverse Events in Predominantly Republican vs Democratic States. JAMA Network Open, 7(3), e244177. https://doi.org/10.1001/jamanetworkopen.2024.4177"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "In the pursuit of a comprehensive understanding, our research embarks on a meticulous examination of public data sources, seeking to address pivotal questions that encapsulate the intricate interplay between COVID-19 vaccination rates and multifaceted dimensions. Through the lens of data-driven analysis, our endeavor unfolds as a systematic exploration into the evolving dynamics of vaccination and its cascading impact across diverse realms.\nOne of the central inquiries guiding this investigation is the temporal evolution of COVID-19 vaccination rates and their influence on various sectors. Delving into the data, we seek to discern patterns and shifts in vaccination rates over time, unraveling the nuanced story of how the collective endeavor to immunize populations unfolds. We examine the effectiveness of the vaccine from a quantitative perspective, scrutinizing its role in mitigating the spread of the virus and potentially altering the trajectory of the pandemic.\nHospitalization rates stand as a critical metric in gauging the efficacy of vaccination efforts. We probe into whether the vaccine, by conferring immunity, contributes to alleviating the burden on healthcare systems. Through rigorous data analysis, we aim to illuminate the extent to which vaccination rates correlate with fluctuations in hospitalization numbers, providing valuable insights into the broader public health landscape.\nThe economic ramifications of COVID-19 vaccination constitute another facet of our exploration. By scrutinizing the data, we seek to ascertain whether higher vaccination rates correspond to economic recovery. Unpacking the intricate relationship between vaccination efforts and economic indicators, our analysis endeavors to shed light on the potential role of vaccination campaigns in fostering economic resilience.\nIn the financial realm, we examine the impact of vaccination on medical corporations, particularly in the context of stock prices. This inquiry navigates the nexus between vaccination investments and market dynamics, unraveling the intricate dance between public health imperatives and corporate performance.\nBeyond these dimensions, our research extends its gaze into the political arena, exploring the relationship between vaccination rates and party support. By dissecting the data, we aim to uncover whether vaccination rates influence political sentiments, offering a nuanced understanding of how public health measures intertwine with political dynamics."
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Time Series Data Visualization",
    "section": "",
    "text": "Importing Libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(imputeTS)\nlibrary(gridExtra)\nlibrary(reticulate)\nlibrary(readxl)\n#use_python(\"/usr/local/bin/python3\", require = T)\n#knitr::knit_engines$set(python = reticulate::eng_python)\n#py_install(\"tensorflow\")\n\n\n\nTime Series Data Visualization\nIn time series data visualization, the importance lies in presenting temporal patterns and trends in a clear and comprehensible manner. Effective visualization allows analysts and decision-makers to extract meaningful insights from the data, aiding in better understanding the dynamics of a system over time. The choice of visualization techniques is crucial, as it directly influences the interpretation of patterns within the time series.\nThe ability to discern seasonality, identify anomalies, and recognize patterns is vital for making informed predictions and strategic decisions. Furthermore, interactive features in visualizations enable users to delve deeper into the data, offering a dynamic and exploratory experience.\nUltimately, the clarity and accuracy of time series data visualization contribute significantly to enhancing decision-making processes across various domains, such as finance, healthcare, environmental monitoring and many other areas.\n\nData Visualization with Stock DataInteractive PlotEthereum plot using ggplot2Ethereum plot using plotlyEthereum candlestick plotMacroeconomic indicators plot\n\n\nThe following graph shows overall trends in Google (GOOGL), Microsoft (MSFT), and META (META) stock prices.\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers &lt;- c(\"GOOGL\", \"MSFT\", \"META\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-10-01\",\n             to = \"2024-01-01\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(GOOGL = GOOGL$GOOGL.Adjusted,\n                    MSFT = MSFT$MSFT.Adjusted,\n                    META = META$META.Adjusted)\n\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\n# head(stock)\n\n################################################\n\nggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GOOGL, colour=\"GOOGL\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=META, colour=\"META\"))+\n   labs(\n    title = \"Stock Prices for the Tech Companies\",\n    subtitle = \"From 2012 October to 2023 December\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))+\n    guides(colour=guide_legend(title=\"Tech Companies\")) \n\n\n\n\n\nUpon analyzing the visualization, it becomes evident that META stands out as the most volatile stock among the three featured tech companies. Despite an overall upward trajectory in stock prices for these companies spanning from 2013 to 2022, a noteworthy trend emerges post-2022. Following this period, there is a distinctive and rapid decline in the stock prices of all three companies, persisting until 2023. However, a notable shift occurs thereafter, marking the commencement of a gradual recovery in stock prices from 2023 onwards. This intriguing pattern suggests a complex interplay of market dynamics, possibly influenced by external factors or industry-specific events, contributing to the nuanced behavior observed in the stock prices of these tech giants.\n\n\nHover over the plot to see the difference.\n\n\nCode\ng4&lt;- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GOOGL, colour=\"GOOGL\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=META, colour=\"META\"))+\n   labs(\n    title = \"Stock Prices for the Tech Companies\",\n    subtitle = \"From 2012 October to 2023 December\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))+\n    guides(colour=guide_legend(title=\"Tech Companies\")) \n\n\nggplotly(g4) %&gt;%\n  layout(hovermode = \"x\")\n\n\n\n\n\n\nThe visual representation mirrors the previous one; however, for a more detailed exploration of the specific distinctions, you can hover your mouse cursor over any position on the figure. This interactive feature allows you to gain precise insights into variations, providing a dynamic and hands-on experience with the data. By engaging with the figure through mouse-hover interactions, you can uncover nuanced details and gain a deeper understanding of the intricacies within each data point, enhancing the overall interpretative experience.\n\n\nIn this example, we use ggplot2 package present a visualization depicting the fluctuation in Ethereum’s adjusted price since September 15, 2021.\n\n\nCode\n#bitc_ALL &lt;- getSymbols(\"BTC\",auto.assign = FALSE, from = \"2020-10-01\",src=\"yahoo\")\n# Fetch Ethereum (ETH) data\neth &lt;- getSymbols(\"ETH-USD\", auto.assign = FALSE, from = \"2021-09-15\", src = \"yahoo\")\n\n# Convert data to a data frame\neth_df &lt;- data.frame(date = index(eth), ETH.Adjusted = Ad(eth))\n\n# Convert date to Date type\neth_df$date &lt;- as.Date(eth_df$date)\n\n# Print the structure of the data frame\n# str(eth_df)\n\n# Plot with ggplot\nggplot(eth_df, aes(x = date, y = ETH.USD.Adjusted)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"Ethereum Prices\",\n    subtitle = \"From September 15, 2021, to the present\",\n    x = \"Date\",\n    y = \"Adjusted Prices\"\n  ) +\n  theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))\n\n\n\n\n\nCode\n# Plot with plotly\nfig &lt;- plot_ly(eth_df, x = ~date, y = ~ETH.USD.Adjusted, type = 'scatter', mode = 'lines')\n\nfig &lt;- fig %&gt;% layout(title = \"Ethereum Prices\")\nfig\n\n\n\n\n\n\nFrom the visualization, it becomes evident that the fluctuation pattern in Ethereum’s price closely mirrors the trend observed in the stock prices of the tech companies highlighted in the previous example. To delve into specifics, Ethereum’s price exhibited a continuous upward trajectory until the year 2022. However, a distinct shift occurred post-2022, ushering in an overall decline in price. Notably, the downward trend persisted until 2023, after which a recovery phase ensued. It’s worth noting, however, that despite the recovery, the price of Ethereum by this point remains below the levels observed in 2022.\n\n\nIn this example, we use plotly package present a visualization depicting the fluctuation in Ethereum’s adjusted price since September 15, 2021.\n\n\nCode\n# Plot with plotly\nfig &lt;- plot_ly(eth_df, x = ~date, y = ~ETH.USD.Adjusted, type = 'scatter', mode = 'lines')\n\nfig &lt;- fig %&gt;% layout(title = \"Ethereum Prices\")\nfig\n\n\n\n\n\n\nThe plot shows the same pattern as the visualization created by ggplot2 package. But one advantage of plotly package is that the audience can check the detailed number by clicking the point within the figure to get exact data to analyze.\n\n\nIn this example, we use candlestick plot to depict the intraday price fluctuations, using Ethereum as an example.\n\n\nCode\n#plotly\n# candlestick plot\n\neth1 &lt;- data.frame(date = index(eth), coredata(eth))\ndf &lt;- tail(eth1, 30)\ncolnames(df) &lt;- make.names(colnames(df))\n\nfigc &lt;- df %&gt;% plot_ly(x = ~date, type=\"candlestick\",\n          open = ~ETH.USD.Open, close = ~ETH.USD.Close,\n          high = ~ETH.USD.High, low = ~ETH.USD.Low) \nfigc &lt;- figc %&gt;% layout(title = \"Ethereum Candlestick Plot\")\nfigc\n\n\n\n\n\n\nThrough a comprehensive analysis of the visualization, the intraday change trends in stock become distinctly discernible. The color scheme aids in this interpretation: the red candlesticks denote instances where the closing price is lower than the opening price, while the green counterparts signify a close higher than the open. The length of each candlestick serves as an insightful indicator, with longer candlesticks suggesting more pronounced price fluctuations on a given day, and smaller ones indicative of relative stability. Simultaneously, the placement of points along the line graph signifies the highest and lowest prices of the stock within a specific day, providing further depth for analytical considerations. This multifaceted representation empowers observers to glean valuable insights into the dynamic nature of intraday stock price movements.\n\n\nIn this illustrative scenario, we specifically curate data from the National Data section of the BEA Government Website. This dataset intricately captures the seasonally adjusted percentage change from the preceding period in real Gross Domestic Product (GDP) and Personal Consumption Expenditures. Leveraging the capabilities of the plotly package, we construct dynamic visualizations that articulate the trends inherent in these critical economic indicators over time.\n\n\nCode\ndata &lt;- read.csv('indicator.csv')\n\n# Create a line chart\nfig &lt;- plot_ly(data, x = ~paste(Year, Quarter, sep = \"-\")) %&gt;%\n  add_lines(y = ~GDP, name = \"GDP\") %&gt;%\n  add_lines(y = ~PCE, name = \"PCE\") %&gt;%\n  layout(\n    title = \"Percent Change From Preceding Period in GDP and Personal Consumption \\n\n    Expenditure Trends Over Quarters in the US\",\n    xaxis = list(title = \"Year-Quarter\"),\n    yaxis = list(title = \"Percentage\"),\n    showlegend = TRUE\n  )\n\n# Display the plot\nfig\n\n\n\n\n\n\nUpon close examination of this visualization, a recurring trend becomes apparent: both Gross Domestic Product (GDP) and personal consumption exhibit consistent increases throughout the depicted timeline, barring an exception in the first and second quarters of 2022. A noteworthy observation surfaces as well, unveiling an intriguing pattern: the percentage of increase observed before the year 2022 surpasses that witnessed after 2022. This disparity in growth rates delineates a distinctive shift in the economic dynamics, prompting further exploration into the factors influencing these fluctuations and their potential implications."
  },
  {
    "objectID": "dv.html#macroeconomic-indicators-plot",
    "href": "dv.html#macroeconomic-indicators-plot",
    "title": "Time Series Data Visualization",
    "section": "Macroeconomic indicators plot",
    "text": "Macroeconomic indicators plot\n\n# Read the file\ndata &lt;- read.csv('indicator.csv')\n\n# Create a line chart\nfig &lt;- plot_ly(data, x = ~paste(Year, Quarter, sep = \"-\")) %&gt;%\n  add_lines(y = ~GDP, name = \"GDP\") %&gt;%\n  add_lines(y = ~PCE, name = \"PCE\") %&gt;%\n  layout(\n    title = \"Percent Change From Preceding Period in GDP and Personal Consumption \\n\n    Expenditure Trends Over Quarters in the US\",\n    xaxis = list(title = \"Year-Quarter\"),\n    yaxis = list(title = \"Percentage\"),\n    showlegend = TRUE\n  )\n\n# Display the plot\nfig"
  },
  {
    "objectID": "aboutme.html#education",
    "href": "aboutme.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nGeorgetown University, McCourt School of Public Policy\nWashington, DC\n\nMaster of Science, Data Science for Public Policy\nExpected Graduation: May 2024 - GPA: 3.9\nSpecializations: Criminal Justice Policy, Data Analytics, Data Science, Housing & Neighborhoods, Public Health\n\nUniversity of International Business and Economics\nBeijing, China\n\nBachelor of Management, Customs Administration\nGraduated June 2022"
  },
  {
    "objectID": "aboutme.html#experience",
    "href": "aboutme.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nCenter for Global Health Practice and Impact, Georgetown University\nWashington, DC\nData Scientist Assistant\nMarch 2023– Present\n\nDeveloped pipeline for data cleaning and transformation using Pandas on 20K patients data from SQL.\nBuilt predictive models for HIV treatment status using techniques like Logistic Regression, Random Forest, XGBoost, SVM, and Neural Network algorithm, and achieved the highest accuracy rate of 91.12% with the XGBoost model.\nUtilized Multinomial Logistic Regression algorithm to investigate the factors contributing to the concordance or discordance of IPV behavior reports within couples.\nUtilized Logistic Regression algorithm to analyze factors impacting patient retention and virological status, visualizing correlations and offering strategic insights for policymakers.\nEnhanced the Haiti Team’s analytical capabilities by developing Dashboards to present the visualization of descriptive analyses and use machine learning model to predict patients’ treatment status, enabling the team to derive actionable insights and make informed decisions.\n\nCenter for International Private Enterprise\nWashington, DC\nMEL (Monitor, Evaluation, Learning) Intern\nSeptember 2023– December 2023\n\nDesigned and implemented a comprehensive pipeline for the CIPE database to efficiently manage and store project information.\nOrganized subsets of data, including organizations, activities, and individuals, streamlining data retrieval and enhancing accessibility.\nLeveraged Web Scraping to collect extensive resource information from CIPE websites, ensuring up-to-date data availability.\nConducted Keyword Extraction and performed in-depth Statistical Analysis at multiple levels, including topics, keywords, countries, and continents, providing valuable insights for strategic decision-making.\n\nInstitute for Policy Studies\nWashington, DC\nData Analyst Intern\nJune 2023 – August 2023\n\nUtilized Web Scraping techniques to extract housing sales data from the Massachusetts Land Record website, employed data science techniques to classify the housing into different neighborhoods to conduct in-depth neighborhood-level research for valuable insights and policy suggestions on the feasibility of implementing an additional transfer tax on housing over 2M.\nUtilized data science to research the feasibility of expansion of Hanscom Private Airport (Boston), employed API to access and analyze flight records, conducted comprehensive research to gauge private flight ratios and its impact on air pollution, and delivered policy recommendations to the Boston government."
  },
  {
    "objectID": "aboutme.html#relevant-projects",
    "href": "aboutme.html#relevant-projects",
    "title": "About Me",
    "section": "Relevant Projects",
    "text": "Relevant Projects\n\nProject Name: Incarceration Rate/ Sentence Length Prediction – Cook County\n\nUtilized county-level big data to conduct in-depth data exploration analysis and feature engineering.\nModeled the incarceration rate using KNN, Decision Tree, and Random Forest algorithms, and achieved the highest accuracy rate of 86.17% using the Random Forest Model.\nModeled the sentence length using KNN, Decision Tree, and Random Forest algorithms, and achieved the lowest RMSE of 1.84 using the Random Forest Model.\nGeographically categorized and mapped defendants at the neighborhood level based on their latitude and longitude coordinates to gain insights into their spatial distribution.\nResults: GitHub Repo"
  },
  {
    "objectID": "aboutme.html#technical-summary",
    "href": "aboutme.html#technical-summary",
    "title": "About Me",
    "section": "Technical Summary",
    "text": "Technical Summary\nTools/Software: R Studio, R, Visual Studio Code, SQL, Tableau, PostgreSQL, AWS, MS Office Suite, Command Line, MySQL, ggplot2, Leaflet\nProgramming Languages: R, Python, SQL, TensorFlow, Shell Scripting"
  },
  {
    "objectID": "aboutme.html#additional-information",
    "href": "aboutme.html#additional-information",
    "title": "About Me",
    "section": "Additional Information",
    "text": "Additional Information\nLeadership:\n\nCounselor to the Business and Management Department – Webster Vienna Private University - Vice President of Debate Club – Webster Vienna Private University\nGlobal View:\n\nLanguages: Czech – Native, English – Fluent, Slovak – Fluent, German – Intermediate\n\n\nCommunity Involvement:\n\nVolunteering at Data Kind DC, Beauty of Help Prague, and Ted Ex Youth Event\nAttended Warwick Economic Summit 2022"
  },
  {
    "objectID": "introduction.html#topic-explanation",
    "href": "introduction.html#topic-explanation",
    "title": "Introduction",
    "section": "",
    "text": "The emergence of the COVID-19 pandemic marked an unprecedented juncture in human history. As the virus proliferated, nations worldwide found themselves grappling with the need for swift and decisive action to curb its relentless spread. Soon after the outbreak of the pandemic, states in the U.S. began implementing various community mitigation strategies (e.g., mandatory stay at home orders and business closures) to curb the spread of COVID-19. In total, 42 U.S. states and territories issued mandatory stay-at-home orders, covering 73% of U.S. counties. In the United States, a paradigm shift unfolded, compelling individuals and organizations to reevaluate and adapt their daily routines. A notable consequence of this upheaval was the widespread adoption of remote work, a strategic response aimed at mitigating the transmission of the virus while ensuring the continuity of essential services. Internationally, countries like China responded to the crisis by enacting localized policies, effectively delineating geographic boundaries to staunch the virus’s advance. These measures, ranging from city-wide lockdowns to regional containment strategies, reflected the urgency and gravity of the situation. As societies navigated the uncharted territory of a novel and highly infectious pathogen, a collective sense of unfamiliarity and uncertainty permeated daily life. In the face of this existential threat, the global medical community rallied with unparalleled speed and collaboration. Scientific institutions, pharmaceutical companies, and researchers joined forces to develop and deploy effective vaccines.\nHowever, as these vaccines became available, a nuanced narrative unfolded, marked by public skepticism and questions about their efficacy, given that no vaccine could guarantee absolute immunity. This project endeavors to unravel the multifaceted impact of COVID-19 vaccination rates over time, exploring its influence across diverse realms. Beyond the standard metrics of confirmed cases, mortality rates, and hospitalizations caused by COVID-19, the analysis extends its gaze to encompass economic indicators, stock prices, and even political dynamics. The intricacies of vaccine acceptance and its dynamic impact will be dissected through our time series analysis, revealing the social influence shaped by the ongoing vaccination efforts."
  },
  {
    "objectID": "introduction.html#big-picture",
    "href": "introduction.html#big-picture",
    "title": "Introduction",
    "section": "",
    "text": "In the pursuit of a comprehensive understanding, our research embarks on a meticulous examination of public data sources, seeking to address pivotal questions that encapsulate the intricate interplay between COVID-19 vaccination rates and multifaceted dimensions. Through the lens of data-driven analysis, our endeavor unfolds as a systematic exploration into the evolving dynamics of vaccination and its cascading impact across diverse realms.\nOne of the central inquiries guiding this investigation is the temporal evolution of COVID-19 vaccination rates and their influence on various sectors. Delving into the data, we seek to discern patterns and shifts in vaccination rates over time, unraveling the nuanced story of how the collective endeavor to immunize populations unfolds. We examine the effectiveness of the vaccine from a quantitative perspective, scrutinizing its role in mitigating the spread of the virus and potentially altering the trajectory of the pandemic.\nHospitalization rates stand as a critical metric in gauging the efficacy of vaccination efforts. We probe into whether the vaccine, by conferring immunity, contributes to alleviating the burden on healthcare systems. Through rigorous data analysis, we aim to illuminate the extent to which vaccination rates correlate with fluctuations in hospitalization numbers, providing valuable insights into the broader public health landscape.\nThe economic ramifications of COVID-19 vaccination constitute another facet of our exploration. By scrutinizing the data, we seek to ascertain whether higher vaccination rates correspond to economic recovery. Unpacking the intricate relationship between vaccination efforts and economic indicators, our analysis endeavors to shed light on the potential role of vaccination campaigns in fostering economic resilience.\nIn the financial realm, we examine the impact of vaccination on medical corporations, particularly in the context of stock prices. This inquiry navigates the nexus between vaccination investments and market dynamics, unraveling the intricate dance between public health imperatives and corporate performance.\nBeyond these dimensions, our research extends its gaze into the political arena, exploring the relationship between vaccination rates and party support. By dissecting the data, we aim to uncover whether vaccination rates influence political sentiments, offering a nuanced understanding of how public health measures intertwine with political dynamics."
  },
  {
    "objectID": "introduction.html#literature-review",
    "href": "introduction.html#literature-review",
    "title": "Introduction",
    "section": "",
    "text": "In the post-pandemic landscape, scholars have delved into comprehensive research on COVID-19 and the ramifications of vaccination, yielding valuable insights. Moghadas et al. (2020) highlight the substantial benefits of COVID-19 vaccines, demonstrating their potential to significantly reduce future infection rates, hospitalizations, and deaths, even with limited protection against infection. Conversely, Barro’s (2022) findings suggest sizable negative effects of vaccination on critical metrics until early December 2021, indicating potential waning efficacy over time.\nFurther nuanced insights emerge from Guo et al. (2022), indicating complex associations between county-level socioeconomic factors and vaccination rates. Per capita income is negatively linked to vaccination rates in counties with higher proportions of BIPOC individuals, while the unemployment rate shows a negative association in counties with higher proportions of non-Hispanic White individuals.\nKhalfaoui et al. (2021) contribute to the literature by revealing a positive and significant influence of COVID-19 vaccination, infection rates, and case fatality ratios on S&P 500 returns at various business cycle frequencies, suggesting an interconnected relationship with financial markets.\nAdditionally, examining the intersection of vaccination and political dynamics, Galston et al. (2022) reveal a correlation between COVID-19 vaccination rates and party support. States with vaccination rates above the national average predominantly favored Joe Biden in the last November elections, while those below the average leaned towards Donald Trump. The vaccination-rate gap between counties won by Biden and Trump increased significantly, underscoring the evolving political landscape influenced by vaccination trends.\nReference\n\nMoghadas, S. M., Vilches, T. N., Zhang, K., Wells, C. R., Shoukat, A., Singer, B. H., Meyers, L. A., Neuzil, K. M., Langley, J. M., Fitzpatrick, M. C., & Galvani, A. P. (2021). The impact of vaccination on COVID-19 outbreaks in the United States. medRxiv : the preprint server for health sciences, 2020.11.27.20240051. https://doi.org/10.1101/2020.11.27.20240051\nGalston, W. A., Kamarck, E., West, D. M., & Cecilia Elena Rouse, A. P. (2022, March 9). For covid-19 vaccinations, party affiliation matters more than race and ethnicity. Brookings. https://www.brookings.edu/articles/for-covid-19-vaccinations-party-affiliation-matters-more-than-race-and-ethnicity/\nGuo Y, Kaniuka AR, Gao J, Sims OT. An Epidemiologic Analysis of Associations between County-Level Per Capita Income, Unemployment Rate, and COVID-19 Vaccination Rates in the United States. International Journal of Environmental Research and Public Health. 2022; 19(3):1755. https://doi.org/10.3390/ijerph19031755\nKhalfaoui, R., Nammouri, H., Labidi, O., & Ben Jabeur, S. (2021). Is the COVID-19 vaccine effective on the US financial market? Public Health, 198, 177–179. https://doi.org/10.1016/j.puhe.2021.07.026\nBarro, R. (2022). Vaccination Rates and Covid Outcomes across U.S. States. https://doi.org/10.3386/w29884"
  },
  {
    "objectID": "introduction.html#analytical-angles",
    "href": "introduction.html#analytical-angles",
    "title": "Introduction",
    "section": "",
    "text": "Our investigation employs five distinct analytical angles, each offering a unique lens through which we comprehensively address the data science question and navigate the intricate landscape of COVID-19 vaccination dynamics.\n1. Temporal Evolution of Vaccination Rates: We scrutinize patterns and shifts in vaccination rates over time, unraveling the nuanced story of the collective endeavor to immunize populations and its temporal evolution.\n2. Impact on Healthcare Systems: Hospitalization rates stand as a critical metric in gauging the efficacy of vaccination efforts. We probe into whether vaccination, by conferring immunity, contributes to alleviating the burden on healthcare systems.\n3. Economic Recovery: Scrutinizing the economic ramifications of COVID-19 vaccination, we aim to ascertain whether higher vaccination rates correspond to economic recovery, unpacking the intricate relationship between vaccination efforts and economic indicators.\n4. Financial Impact: In the financial realm, we examine the impact of vaccination on medical corporations, particularly in the context of stock prices. This inquiry navigates the nexus between vaccination investments and market dynamics.\n5. Political Dynamics: Our research extends into the political arena, exploring the nexus between vaccination rates and party support. By dissecting the data, we aim to uncover whether vaccination rates influence political sentiments, offering insights into the intersection of public health measures and political dynamics."
  },
  {
    "objectID": "introduction.html#guiding-question",
    "href": "introduction.html#guiding-question",
    "title": "Introduction",
    "section": "",
    "text": "How have COVID-19 vaccination rates evolved over time, and what patterns can be discerned?\nTo what extent has the vaccine proven effective in mitigating the spread of the virus, and how does this effectiveness manifest quantitatively?\nDoes higher vaccination rates correlate with a reduction in hospitalization numbers, contributing to a more resilient healthcare system?\nCan we observe a correlation between vaccination rates and economic recovery, and if so, what are the nuances of this relationship?\nHow do vaccination efforts impact the stock prices of medical corporations, and what are the broader financial implications?\nTo what extent do vaccination rates influence political sentiments and party support, unveiling the intersection of public health and political dynamics?\nAre there geographical variations in vaccination rates, and how do regional differences impact the overall efficacy of vaccination campaigns?\nHow do demographic factors contribute to vaccination disparities, and what are the implications for public health interventions?\nWhat lessons can be drawn from the global response to COVID-19 vaccination for future pandemics and public health emergencies?\nCan time series analysis and forecasting models predict the future impact of the COVID-19 vaccination in the post-pandemic era?"
  },
  {
    "objectID": "data_source.html#newly-confirmed-cases",
    "href": "data_source.html#newly-confirmed-cases",
    "title": "Data Sources",
    "section": "Newly Confirmed Cases",
    "text": "Newly Confirmed Cases\n\nData Overview\n\n\nCode\nwide_data &lt;- read_csv(\"Datasets/covid_confirmed_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ncon_case_df &lt;- long_data %&gt;%\n  group_by(State, date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Calculate the daily new cases\ncon_case_df1 &lt;- con_case_df %&gt;%\n  group_by(State) %&gt;%\n  mutate(new_cases = value_sum - lag(value_sum, default = 0))\n\n# data glimpse\nrbind(head(con_case_df1, 3), tail(con_case_df1, 3)) %&gt;%\n  kbl(row.names = FALSE) %&gt;%\n  kable_styling()\n\n\n\n\n\nState\ndate\nvalue_sum\nnew_cases\n\n\n\n\nAK\n2020-01-22\n0\n0\n\n\nAK\n2020-01-23\n0\n0\n\n\nAK\n2020-01-24\n0\n0\n\n\nWY\n2023-07-21\n187389\n0\n\n\nWY\n2023-07-22\n187389\n0\n\n\nWY\n2023-07-23\n187389\n0\n\n\n\n\n\n\n\n\n\nData dictionary\n\nState: abbreviate of the states.\nvalue_sum: total sum of confirmed cases.\nnew_cases: number of newly confirmed cases in each day."
  },
  {
    "objectID": "data_source.html#death-cases",
    "href": "data_source.html#death-cases",
    "title": "Data Sources",
    "section": "Death Cases",
    "text": "Death Cases\n\nData Overview\n\n\nCode\nwide_data &lt;- read_csv(\"Datasets/covid_deaths_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ndead_case_df &lt;- long_data %&gt;%\n  group_by(State, date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Calculate the daily new cases\ndead_case_df1 &lt;- dead_case_df %&gt;%\n  group_by(State) %&gt;%\n  mutate(new_cases = value_sum - lag(value_sum, default = 0))\n\n# data glimpse\nrbind(head(dead_case_df1, 3), tail(dead_case_df1, 3)) %&gt;%\n  kbl(row.names = FALSE) %&gt;%\n  kable_styling()\n\n\n\n\n\nState\ndate\nvalue_sum\nnew_cases\n\n\n\n\nAK\n2020-01-22\n0\n0\n\n\nAK\n2020-01-23\n0\n0\n\n\nAK\n2020-01-24\n0\n0\n\n\nWY\n2023-07-21\n2039\n0\n\n\nWY\n2023-07-22\n2039\n0\n\n\nWY\n2023-07-23\n2039\n0\n\n\n\n\n\n\n\n\n\nData dictionary\n\nState: abbreviate of the states.\nvalue_sum: total sum of death cases.\nnew_cases: number of newly death cases in each day."
  },
  {
    "objectID": "data_source.html#gdp-per-capita",
    "href": "data_source.html#gdp-per-capita",
    "title": "Data Sources",
    "section": "GDP Per Capita",
    "text": "GDP Per Capita\n\nData Overview\n\n\nCode\ngdp &lt;- read_csv('Datasets/gdp.csv')\n\n# data glimpse\nrbind(head(gdp, 3),  tail(gdp, 3)) %&gt;%\n  kbl(row.names = FALSE) %&gt;%\n  kable_styling()\n\n\n\n\n\nDATE\nGDP\n\n\n\n\n1/1/17\n19280.08\n\n\n4/1/17\n19438.64\n\n\n7/1/17\n19692.60\n\n\n4/1/23\n27063.01\n\n\n7/1/23\n27610.13\n\n\n10/1/23\n27944.63"
  },
  {
    "objectID": "data_source.html#unemployment-rate",
    "href": "data_source.html#unemployment-rate",
    "title": "Data Sources",
    "section": "Unemployment Rate",
    "text": "Unemployment Rate\n\nData Overview\n\n\nCode\nunemp &lt;- read_csv('Datasets/unemployment.csv')\nkey_cols &lt;- c(\"Location\")\nvalue_cols &lt;- setdiff(names(unemp), key_cols)\nunemp1 &lt;- pivot_longer(\n  unemp,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"Unemployment\"\n)\n\n# data glimpse\nrbind(head(unemp1, 3), tail(unemp1, 3)) %&gt;%\n  kbl(row.names = FALSE) %&gt;%\n  kable_styling()\n\n\n\n\n\nLocation\nTime\nUnemployment\n\n\n\n\nUnited States\n2020-01\n0.036\n\n\nUnited States\n2020-02\n0.035\n\n\nUnited States\n2020-03\n0.044\n\n\nPuerto Rico\n2022-01\n0.071\n\n\nPuerto Rico\n2022-02\n0.068\n\n\nPuerto Rico\n2022-03\n0.065"
  },
  {
    "objectID": "MT.html#confirmed-covid-19-cases-as-a-function-of-vaccination-rate",
    "href": "MT.html#confirmed-covid-19-cases-as-a-function-of-vaccination-rate",
    "title": "Multivariate TS Models",
    "section": "2.1 Confirmed COVID-19 Cases as a Function of Vaccination Rate:",
    "text": "2.1 Confirmed COVID-19 Cases as a Function of Vaccination Rate:\n(ARIMAX)Confirmed COVID-19 Cases ~ Vaccination Rate\nThis model aims to capture the impact of vaccination rates on the trend of newly confirmed COVID-19 cases.\n\n\nCode\n# Read the dataset\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million')\nt &lt;- vac_df[, cols_show]\n\n# Group by 'date' and summarize columns, ignoring NA values\nt1 &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    daily_vaccinations_per_million = sum(daily_vaccinations_per_million, na.rm = TRUE)\n  )\n\n# Convert date column to Date format\nt1$date &lt;- as.Date(t1$date)\n\n# Newly confirmed cases\nwide_data &lt;- read_csv(\"Datasets/covid_confirmed_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ncon_case_df &lt;- long_data %&gt;%\n  group_by(date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Convert date column to Date format\ncon_case_df$date &lt;- as.Date(con_case_df$date)\n\n# Merge them together\ncava_df &lt;- merge(t1, con_case_df, by = \"date\", all = FALSE)\nhead(cava_df)\n\n\n        date daily_vaccinations_per_million value_sum\n1 2020-12-20                              0  17848765\n2 2020-12-21                            174  18010051\n3 2020-12-22                            384  18204174\n4 2020-12-23                            454  18416088\n5 2020-12-24                            575  18609135\n6 2020-12-25                            648  18704673\n\n\nCode\n# Shape of the df\ndim(cava_df)\n\n\n[1] 872   3"
  },
  {
    "objectID": "MT.html#covid-19-death-cases-as-a-function-of-vaccination-rate",
    "href": "MT.html#covid-19-death-cases-as-a-function-of-vaccination-rate",
    "title": "Multivariate TS Models",
    "section": "2.2 COVID-19 Death Cases as a Function of Vaccination Rate:",
    "text": "2.2 COVID-19 Death Cases as a Function of Vaccination Rate:\n(ARIMAX)COVID-19 Death Cases ~ Vaccination Rate\nThrough this model, we explore how variations in vaccination rates influence the count of COVID-19-related fatalities.\n\n\nCode\n# Read the dataset\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million')\nt &lt;- vac_df[, cols_show]\n\n# Group by 'date' and summarize columns, ignoring NA values\nt1 &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    daily_vaccinations_per_million = sum(daily_vaccinations_per_million, na.rm = TRUE)\n  )\n\n# Convert date column to Date format\nt1$date &lt;- as.Date(t1$date)\n\n\n# Death cases\nwide_data &lt;- read_csv(\"Datasets/covid_deaths_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ndead_case_df &lt;- long_data %&gt;%\n  group_by(date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Convert date column to Date format\ndead_case_df$date &lt;- as.Date(dead_case_df$date)\n\n# Merge them together\ndeva_df &lt;- merge(t1, dead_case_df, by = \"date\", all = FALSE)\nhead(deva_df)\n\n\n        date daily_vaccinations_per_million value_sum\n1 2020-12-20                              0    324336\n2 2020-12-21                            174    326365\n3 2020-12-22                            384    328950\n4 2020-12-23                            454    332017\n5 2020-12-24                            575    335328\n6 2020-12-25                            648    336496\n\n\nCode\n# Shape of the df\ndim(deva_df)\n\n\n[1] 872   3"
  },
  {
    "objectID": "MT.html#unemployment-rate-as-influenced-by-vaccination-rate-and-confirmed-cases",
    "href": "MT.html#unemployment-rate-as-influenced-by-vaccination-rate-and-confirmed-cases",
    "title": "Multivariate TS Models",
    "section": "2.3 Unemployment Rate as Influenced by Vaccination Rate and Confirmed Cases:",
    "text": "2.3 Unemployment Rate as Influenced by Vaccination Rate and Confirmed Cases:\n(ARIMAX)Unemployment Rate ~ Vaccination Rate + Confirmed COVID-19 Cases\nThis model investigates the effect of vaccination rates and COVID-19 case counts on the unemployment rate, providing insights into the pandemic’s broader economic implications.\n\n\nCode\n# Read the dataset\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million')\nt &lt;- vac_df[, cols_show]\n\n# Group by 'date' and summarize columns, ignoring NA values\nt1 &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    daily_vaccinations_per_million = sum(daily_vaccinations_per_million, na.rm = TRUE)\n  )\n\n# Convert date column to Date format\nt1$date &lt;- as.Date(t1$date)\n\n# Aggregate data to monthly level using mean for each column\nt1 &lt;- t1 %&gt;%\n  group_by(date = format(date, \"%Y-%m\")) %&gt;%\n  summarize(daily_vaccinations_per_million = mean(daily_vaccinations_per_million, na.rm = TRUE))\n\n# Transform the date column type with specified format\nt1$date &lt;- as.Date(paste0(t1$date, \"-01-01\"))\n\n# Newly confirmed cases\nwide_data &lt;- read_csv(\"Datasets/covid_confirmed_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ncon_case_df &lt;- long_data %&gt;%\n  group_by(date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Convert date column to Date format\ncon_case_df$date &lt;- as.Date(con_case_df$date)\n\n# Aggregate data to monthly level using mean for each column\ncon_case_df &lt;- con_case_df %&gt;%\n  group_by(date = format(date, \"%Y-%m\")) %&gt;%\n  summarize(value_sum = mean(value_sum, na.rm = TRUE))\n\n# Transform the date column type with specified format\ncon_case_df$date &lt;- as.Date(paste0(con_case_df$date, \"-01-01\"))\n\n\nunemp &lt;- read_csv('Datasets/unemployment.csv')\nkey_cols &lt;- c(\"Location\")\nvalue_cols &lt;- setdiff(names(unemp), key_cols)\nunemp1 &lt;- pivot_longer(\n  unemp,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"Unemployment\"\n)\n\n# Convert Time column to Date format\nunemp1$date &lt;- as.Date(paste0(unemp1$Time, \"-01\"))\n\n# Convert Unemployment column to numeric (floating-point) format\nunemp1$Unemployment &lt;- as.numeric(unemp1$Unemployment)\n\n# Focus on US\nunemp2 &lt;- unemp1[unemp1$Location =='United States',]\nunemp3 &lt;- unemp2[,c('date', 'Unemployment')]\n\n# Merge them together\ndd_df &lt;- merge(t1, con_case_df, by = \"date\", all = FALSE)\nddd_df &lt;- merge(dd_df, unemp3, by = \"date\", all = FALSE)\n\nhead(ddd_df)\n\n\n        date daily_vaccinations_per_million value_sum Unemployment\n1 2020-12-01                       544.9167  16927034        0.067\n2 2021-01-01                    123266.4194  23365835        0.063\n3 2021-02-01                    304911.1429  27271116        0.062\n4 2021-03-01                    429380.9032  29098434        0.060\n5 2021-04-01                    514652.3333  30975088        0.061\n6 2021-05-01                    319535.1290  32356593        0.058\n\n\nCode\n# Shape of the df\ndim(ddd_df)\n\n\n[1] 16  4"
  },
  {
    "objectID": "MT.html#interdependence-between-vaccination-rate-and-independent-party-support-rate",
    "href": "MT.html#interdependence-between-vaccination-rate-and-independent-party-support-rate",
    "title": "Multivariate TS Models",
    "section": "2.4 Interdependence between Vaccination Rate and Independent Party Support Rate:",
    "text": "2.4 Interdependence between Vaccination Rate and Independent Party Support Rate:\n(VAR)Vaccination Rate ~ Independent Party Support Rate\nThis model evaluates how vaccination efforts and support for independent political parties influence each other over time.\n\n\nCode\n# Read the dataset\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million')\nt &lt;- vac_df[, cols_show]\n\n# Group by 'date' and summarize columns, ignoring NA values\nt1 &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    daily_vaccinations_per_million = sum(daily_vaccinations_per_million, na.rm = TRUE)\n  )\n\n# Convert date column to Date format\nt1$date &lt;- as.Date(t1$date)\n\ndemo &lt;- read_excel('Datasets/party.xlsx',sheet = 'Democrat')\ninde &lt;- read_excel('Datasets/party.xlsx',sheet = 'Independent')\nrep &lt;- read_excel('Datasets/party.xlsx',sheet = 'Republican')\n\n# Transform the wide dataframe into a long dataframe\nkey_cols &lt;- c(\"Attitude\")\nvalue_cols &lt;- setdiff(names(demo), key_cols)\ndemo1 &lt;- pivot_longer(\n  demo,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"democrat\"\n)\n\ninde1 &lt;- pivot_longer(\n  inde,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"independent\"\n)\n\nrep1 &lt;- pivot_longer(\n  rep,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"republican\"\n)\n\n# Combine these three datasets together\ncombined_data &lt;- full_join(demo1, inde1, by = c(\"date\", \"Attitude\")) %&gt;%\n  full_join(rep1, by = c(\"date\", \"Attitude\"))\ncombined_data1 &lt;- combined_data[combined_data$Attitude=='Favorable',]\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"Attitude\", \"date\")\nvalue_cols &lt;- setdiff(names(combined_data1), key_cols)\n\n# Pivot the data from wide to long\ncombined_data2 &lt;- pivot_longer(\n  combined_data1,\n  cols = value_cols,\n  names_to = \"Party\",\n  values_to = \"value\"\n)\n\n# Convert date column to Date format\ncombined_data2$date &lt;- as.Date(combined_data2$date)\n\n# Subset to each party\ndemo_data &lt;- combined_data2[combined_data2$Party=='democrat',][,c(\"date\",\"value\")]\ninde_data &lt;- combined_data2[combined_data2$Party=='independent',][,c(\"date\",\"value\")]\nrep_data &lt;- combined_data2[combined_data2$Party=='republican',][,c(\"date\",\"value\")]\n\ninde_df &lt;- merge(t1, inde_data, by = \"date\", all = FALSE)\nhead(inde_df)\n\n\n        date daily_vaccinations_per_million value\n1 2020-12-22                            384  0.28\n2 2021-01-05                           1022  0.30\n3 2021-01-12                           1932  0.30\n4 2021-01-19                         174090  0.32\n5 2021-01-26                         214206  0.29\n6 2021-02-02                         238622  0.30\n\n\nCode\n# Shape of the df\ndim(inde_df)\n\n\n[1] 117   3"
  },
  {
    "objectID": "MT.html#vaccination-rate-and-democratic-party-support-rate-dynamics",
    "href": "MT.html#vaccination-rate-and-democratic-party-support-rate-dynamics",
    "title": "Multivariate TS Models",
    "section": "2.5 Vaccination Rate and Democratic Party Support Rate Dynamics:",
    "text": "2.5 Vaccination Rate and Democratic Party Support Rate Dynamics:\n(VAR)Vaccination Rate ~ Democratic Party Support Rate\nHere, we assess the mutual influences between COVID-19 vaccination rates and support levels for the Democratic Party.\n\n\nCode\ndemo_df &lt;- merge(t1, demo_data, by = \"date\", all = FALSE)\nhead(demo_df)\n\n\n        date daily_vaccinations_per_million value\n1 2020-12-22                            384  0.86\n2 2021-01-05                           1022  0.86\n3 2021-01-12                           1932  0.90\n4 2021-01-19                         174090  0.88\n5 2021-01-26                         214206  0.91\n6 2021-02-02                         238622  0.88\n\n\nCode\n# Shape of the df\ndim(demo_df)\n\n\n[1] 117   3"
  },
  {
    "objectID": "MT.html#the-relationship-between-vaccination-rate-and-republican-party-support-rate",
    "href": "MT.html#the-relationship-between-vaccination-rate-and-republican-party-support-rate",
    "title": "Multivariate TS Models",
    "section": "2.6 The Relationship between Vaccination Rate and Republican Party Support Rate:",
    "text": "2.6 The Relationship between Vaccination Rate and Republican Party Support Rate:\n(VAR)Vaccination Rate ~ Republican Party Support Rate\nThis model explores the bidirectional influence between vaccination initiatives and Republican Party support rates.\n\n\nCode\nrep_df &lt;- merge(t1, rep_data, by = \"date\", all = FALSE)\nhead(rep_df)\n\n\n        date daily_vaccinations_per_million value\n1 2020-12-22                            384  0.05\n2 2021-01-05                           1022  0.09\n3 2021-01-12                           1932  0.10\n4 2021-01-19                         174090  0.09\n5 2021-01-26                         214206  0.09\n6 2021-02-02                         238622  0.07\n\n\nCode\n# Shape of the df\ndim(rep_df)\n\n\n[1] 117   3"
  },
  {
    "objectID": "MT.html#variable-selection",
    "href": "MT.html#variable-selection",
    "title": "Multivariate TS Models",
    "section": "3.1 Variable Selection",
    "text": "3.1 Variable Selection\nTo effectively construct an ARIMAX/SARIMAX model, it is crucial to select and define the response variable, which will serve as the primary focus of the analysis. Additionally, identifying relevant exogenous variables—those external predictor variables that are hypothesized to influence the response variable—is essential. This process involves a careful examination of potential predictors to ensure that they are not only relevant but also appropriately measured and free from collinearity issues. The selection of these variables should be guided by both theoretical considerations and empirical evidence, ensuring that they meaningfully contribute to the dynamics of the model. By integrating these exogenous variables, the ARIMAX/SARIMAX model can capture complex interactions and provide nuanced insights into how external factors may drive changes in the response variable over time.\n\n(ARIMAX)Confirmed COVID-19 Cases ~ Daily Vaccination Number(ARIMAX)COVID-19 Death Cases ~ Daily Vaccination Number(ARIMAX)Unemployment Rate ~ Daily Vaccination Number + Confirmed COVID-19 Cases\n\n\n\n\nCode\ncava_df.ts&lt;-ts(cava_df,star=decimal_date(as.Date(\"2020-12-20\",format = \"%Y-%m-%d\")),frequency = 365.25)\n\nautoplot(cava_df.ts[,c(2:3)], facets=TRUE, color=\"#5a3196\") +\n  xlab(\"Year\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Daily changes in new confirmed case and vaccination number\")\n\n\n\n\n\n\n\n\n\nCode\ndeva_df.ts&lt;-ts(deva_df,star=decimal_date(as.Date(\"2020-12-20\",format = \"%Y-%m-%d\")),frequency = 365.25)\n\nautoplot(deva_df.ts[,c(2:3)], facets=TRUE, color=\"#5a3196\") +\n  xlab(\"Year\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Daily changes in dead case and new vaccination number\")\n\n\n\n\n\n\n\n\n\nCode\nddd_df.ts&lt;-ts(ddd_df,star=decimal_date(as.Date(\"2020-12-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\nautoplot(ddd_df.ts[,c(2:4)], facets=TRUE, color=\"#5a3196\") +\n  xlab(\"Year\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Monthly changes in new vaccination number, confirm case, and unemployment rate\")"
  },
  {
    "objectID": "MT.html#fit-auto.arima",
    "href": "MT.html#fit-auto.arima",
    "title": "Multivariate TS Models",
    "section": "3.2 Fit Auto.Arima()",
    "text": "3.2 Fit Auto.Arima()\n\n(ARIMAX)Confirmed COVID-19 Cases ~ Daily Vaccination Number(ARIMAX)COVID-19 Death Cases ~ Daily Vaccination Number(ARIMAX)Unemployment Rate ~ Daily Vaccination Number + Confirmed COVID-19 Cases\n\n\n\n\nCode\nfit1 &lt;- auto.arima(cava_df.ts[,\"value_sum\"],\n  xreg=cava_df.ts[,\"daily_vaccinations_per_million\"])\n\nsummary(fit1)\n\n\nSeries: cava_df.ts[, \"value_sum\"] \nRegression with ARIMA(3,1,5) errors \n\nCoefficients:\n          ar1     ar2     ar3     ma1      ma2      ma3     ma4     ma5\n      -0.0972  0.5262  0.5203  0.2720  -0.6280  -0.6003  0.3334  0.5651\ns.e.   0.0599  0.0433  0.0669  0.0517   0.0372   0.0591  0.0281  0.0311\n         drift     xreg\n      93299.27  -0.0061\ns.e.  40830.89   0.1507\n\nsigma^2 = 4.598e+09:  log likelihood = -10922.34\nAIC=21866.69   AICc=21866.99   BIC=21919.15\n\nTraining set error measures:\n                    ME    RMSE      MAE          MPE       MAPE         MASE\nTraining set -435.6647 67380.3 36417.53 0.0009398995 0.06462941 0.0008688438\n                   ACF1\nTraining set 0.02955241\n\n\nCode\ncheckresiduals(fit1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(3,1,5) errors\nQ* = 1736.4, df = 166, p-value &lt; 2.2e-16\n\nModel df: 8.   Total lags used: 174\n\n\nCode\nmodel_output1 &lt;- capture.output(sarima(residuals(fit1), 3,1,5)) \n\n\n\n\n\nCode\ncat(model_output1[76:91], model_output1[length(model_output1)], sep = \"\\n\")\n\n\nCoefficients: \n         Estimate      SE t.value p.value\nar1       -0.5252  0.2344 -2.2404  0.0253\nar2       -0.6165  0.1185 -5.2028  0.0000\nar3       -0.2279  0.1662 -1.3718  0.1705\nma1       -0.4443  0.2282 -1.9468  0.0519\nma2        0.3707  0.2879  1.2873  0.1983\nma3       -0.3128  0.2967 -1.0544  0.2920\nma4       -0.4284  0.2246 -1.9073  0.0568\nma5       -0.1851  0.0858 -2.1568  0.0313\nconstant  -5.9105 11.6371 -0.5079  0.6117\n\nsigma^2 estimated as 3883502862 on 862 degrees of freedom \n \nAIC = 24.94926  AICc = 24.9495  BIC = 25.00402 \n \n \n\n\n\nThe Standard Residual Plot exhibits some inconsistencies in terms of mean and variance, suggesting potential issues with the model’s assumptions about homoscedasticity or linearity.\nThe Autocorrelation Function (ACF) plot reveals minimal correlation among residuals, suggesting that the model has effectively captured most of the underlying process; however, the presence of white noise indicates that the model may not be capturing all the data’s nuances, which could point to an inadequate model fit.\nThe Quantile-Quantile (Q-Q) Plot shows that the residuals are mostly aligned with a normal distribution, suggesting that the normality assumption for the errors may largely hold, which is a positive indicator for the model’s reliability.\nResults from the Ljung-Box test show p-values below the 0.05 significance level, which typically suggests a lack of fit as it indicates autocorrelation in the residuals at lag intervals that could influence the model’s accuracy.\nThe coefficient table ($ttable) shows that the parameters ar1, ar2, ma4, and ma5 are statistically significant, highlighting their critical influence on the model’s dynamics and underscoring the importance of these terms in explaining the time series variability.\n\n\n\n\n\nCode\nfit2 &lt;- auto.arima(deva_df.ts[,\"value_sum\"],\n  xreg=deva_df.ts[,\"daily_vaccinations_per_million\"])\n\nsummary(fit2)\n\n\nSeries: deva_df.ts[, \"value_sum\"] \nRegression with ARIMA(5,2,2) errors \n\nCoefficients:\n         ar1      ar2      ar3      ar4      ar5      ma1     ma2    xreg\n      0.1645  -0.5352  -0.3586  -0.3468  -0.4067  -1.0923  0.6907  0.0005\ns.e.  0.0447   0.0323   0.0329   0.0295   0.0348   0.0346  0.0517  0.0015\n\nsigma^2 = 297676:  log likelihood = -6714.64\nAIC=13447.28   AICc=13447.49   BIC=13490.2\n\nTraining set error measures:\n                    ME     RMSE      MAE          MPE       MAPE        MASE\nTraining set -10.73691 542.4596 352.5427 -0.001237311 0.04726475 0.001078151\n                   ACF1\nTraining set -0.0649723\n\n\nCode\ncheckresiduals(fit2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(5,2,2) errors\nQ* = 767.24, df = 167, p-value &lt; 2.2e-16\n\nModel df: 7.   Total lags used: 174\n\n\nCode\nmodel_output2 &lt;- capture.output(sarima(residuals(fit2), 5,2,2)) \n\n\n\n\n\nCode\ncat(model_output2[77:90], model_output2[length(model_output2)], sep = \"\\n\")\n\n\nCoefficients: \n    Estimate     SE  t.value p.value\nar1  -1.5360 0.0867 -17.7142  0.0000\nar2  -1.1448 0.0897 -12.7689  0.0000\nar3  -0.8029 0.0834  -9.6288  0.0000\nar4  -0.5359 0.0736  -7.2817  0.0000\nar5  -0.1235 0.0496  -2.4886  0.0130\nma1  -0.2246 0.0776  -2.8943  0.0039\nma2  -0.7754 0.0776  -9.9942  0.0000\n\nsigma^2 estimated as 350912.9 on 863 degrees of freedom \n \nAIC = 15.63613  AICc = 15.63628  BIC = 15.67997 \n \n \n\n\n\nThe Standard Residual Plot suggests some irregularities in both the mean and the variance of the residuals, pointing to potential issues in the homoscedasticity or distribution assumptions of the model.\nThe Autocorrelation Function (ACF) Plot reveals minimal correlation among residuals, suggesting that most of the systematic information in the data has been captured by the model. However, the presence of any remaining correlation, albeit slight, could indicate that the model might not be capturing all underlying patterns effectively, which is indicative of a suboptimal fit.\nThe Quantile-Quantile (Q-Q) Plot shows that the residuals are approximately normally distributed, with only minor deviations from normality. This aspect of the diagnostics is generally positive, indicating that the assumption of normality is reasonably satisfied.\nResults from the Ljung-Box Test show p-values below the 0.05 significance level, suggesting that there is still some autocorrelation present in the residuals at lagged intervals. This result contradicts the ideal scenario where p-values would significantly exceed the threshold, confirming the absence of autocorrelation and a good fit.\nCoefficient Significance Table: All the coefficients in the model are statistically significant, suggesting that each predictor contributes meaningfully to the model. However, significance of coefficients alone does not necessarily imply an overall effective model fit, as it does not account for potential issues in other diagnostic areas.\n\n\n\n\n\nCode\nfit3 &lt;- auto.arima(ddd_df.ts[,\"Unemployment\"],\n  xreg=ddd_df.ts[,2:3])\n\nsummary(fit3)\n\n\nSeries: ddd_df.ts[, \"Unemployment\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n      intercept  daily_vaccinations_per_million  value_sum\n         0.0732                           0e+00      0e+00\ns.e.     0.0033                           2e-04      2e-04\n\nsigma^2 = 1.838e-05:  log likelihood = 66.19\nAIC=-124.38   AICc=-120.75   BIC=-121.29\n\nTraining set error measures:\n                        ME        RMSE         MAE        MPE     MAPE\nTraining set -2.636216e-14 0.003864599 0.003345663 -0.4910086 7.187009\n                  MASE      ACF1\nTraining set 0.1351783 0.6529014\n\n\nCode\ncheckresiduals(fit3)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 9.5034, df = 3, p-value = 0.0233\n\nModel df: 0.   Total lags used: 3\n\n\nCode\nmodel_output3 &lt;- capture.output(sarima(residuals(fit3), 0,0,0)) \n\n\n\n\n\nCode\ncat(model_output3[11:18], model_output3[length(model_output3)], sep = \"\\n\")\n\n\nCoefficients: \n      Estimate    SE t.value p.value\nxmean        0 0.001       0       1\n\nsigma^2 estimated as 1.493512e-05 on 15 degrees of freedom \n \nAIC = -8.023918  AICc = -8.006061  BIC = -7.927344 \n \n \n\n\n\nThe Standard Residual Plot reveals some inconsistencies in the mean and variance, suggesting potential deviations from homoscedasticity or the presence of outliers that could affect the robustness of the model predictions.\nThe Autocorrelation Function (ACF) Plot exhibits minimal correlation among the residuals, which suggests that the model has effectively captured the underlying patterns in the data, leaving behind what appears to be white noise. This is a strong indicator of a well-fitting model.\nThe Quantile-Quantile (Q-Q) Plot closely aligns with the theoretical normal distribution, underscoring the assumption of normality in the model’s residuals, which is essential for the validity of many inferential statistics.\nThe Ljung-Box Test results are mixed, with half of the p-values falling below the 0.05 significance threshold. This inconsistency suggests that some autocorrelations at different lags are significantly different from zero, indicating potential issues with the model fit.\nSignificance Table: The statistical significance of all model coefficients is affirmed, implying that each predictor contributes meaningfully to the model. This significance is crucial for understanding the impact of each variable within the model and for making informed predictions or decisions based on the model’s outputs."
  },
  {
    "objectID": "MT.html#manually-fit",
    "href": "MT.html#manually-fit",
    "title": "Multivariate TS Models",
    "section": "3.3 Manually Fit",
    "text": "3.3 Manually Fit\n\n(ARIMAX)Confirmed COVID-19 Cases ~ Daily Vaccination Number(ARIMAX)COVID-19 Death Cases ~ Daily Vaccination Number(ARIMAX)Unemployment Rate ~ Daily Vaccination Number + Confirmed COVID-19 Cases\n\n\n\n\nCode\nfit.reg &lt;- lm(value_sum ~ daily_vaccinations_per_million, data=cava_df.ts)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = value_sum ~ daily_vaccinations_per_million, data = cava_df.ts)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-67783470 -11303790   6076610  15172166  31376996 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     8.563e+07  1.032e+06   83.01   &lt;2e-16 ***\ndaily_vaccinations_per_million -1.471e+02  5.458e+00  -26.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20530000 on 870 degrees of freedom\nMultiple R-squared:  0.4549,    Adjusted R-squared:  0.4543 \nF-statistic:   726 on 1 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nThe variables appear to be significant. Next, looking at the ACF/PACF plot of the residuals.\n\n\nCode\nres.fit1&lt;-ts(residuals(fit.reg),start = decimal_date(as.Date(\"2020-12-20\", format = \"%Y-%m-%d\")),frequency = 365.25)\n\nplot1&lt;-ggAcf(res.fit1,20) + theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \nplot2&lt;- ggPacf(res.fit1,20)+theme_bw()+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nThere appears to be seasonality still, so I will apply some differencing.\n\n\nCode\nplot1&lt;-ggAcf(diff(res.fit1),20) + theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \nplot2&lt;- ggPacf(diff(res.fit1),20)+theme_bw()+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nAccording to the plots, we have the following values: p = 1,2,3 q= 1,2,3.\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) \n\n\nfor (p in 0:3)\n{\n  for(q in 0:3)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(res.fit1/10000,order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n15190.02\n15204.33\n15190.05\n\n\n0\n1\n0\n11253.02\n11262.56\n11253.04\n\n\n0\n0\n1\n14123.64\n14142.72\n14123.68\n\n\n0\n1\n1\n11242.93\n11257.24\n11242.96\n\n\n0\n0\n2\n13285.16\n13309.01\n13285.23\n\n\n0\n1\n2\n11222.57\n11241.65\n11222.61\n\n\n0\n0\n3\n12809.76\n12838.38\n12809.86\n\n\n0\n1\n3\n11219.64\n11243.49\n11219.71\n\n\n1\n0\n0\n11273.37\n11292.46\n11273.42\n\n\n1\n1\n0\n11239.06\n11253.37\n11239.08\n\n\n1\n0\n1\n11263.17\n11287.03\n11263.24\n\n\n1\n1\n1\n11222.22\n11241.30\n11222.27\n\n\n1\n0\n2\n11242.16\n11270.78\n11242.25\n\n\n1\n1\n2\n11216.68\n11240.53\n11216.75\n\n\n1\n0\n3\n11238.93\n11272.32\n11239.05\n\n\n1\n1\n3\n11218.54\n11247.16\n11218.63\n\n\n2\n0\n0\n11258.99\n11282.84\n11259.06\n\n\n2\n1\n0\n11218.21\n11237.29\n11218.26\n\n\n2\n0\n1\n11240.73\n11269.36\n11240.83\n\n\n2\n1\n1\n11217.28\n11241.13\n11217.35\n\n\n2\n0\n2\n11235.32\n11268.71\n11235.45\n\n\n2\n1\n2\n11208.17\n11236.79\n11208.27\n\n\n2\n0\n3\n11237.17\n11275.33\n11237.33\n\n\n2\n1\n3\n11220.62\n11254.01\n11220.75\n\n\n3\n0\n0\n11237.31\n11265.93\n11237.41\n\n\n3\n1\n0\n11216.51\n11240.36\n11216.58\n\n\n3\n0\n1\n11235.72\n11269.12\n11235.85\n\n\n3\n1\n1\n11218.42\n11247.03\n11218.51\n\n\n3\n0\n2\n11228.83\n11267.00\n11229.00\n\n\n3\n1\n2\n11204.65\n11238.04\n11204.78\n\n\n3\n0\n3\n11229.98\n11272.92\n11230.19\n\n\n3\n1\n3\n11188.50\n11226.66\n11188.67\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),] \n\n\n   p d q     AIC      BIC     AICc\n32 3 1 3 11188.5 11226.66 11188.67\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n   p d q     AIC      BIC     AICc\n32 3 1 3 11188.5 11226.66 11188.67\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n   p d q     AIC      BIC     AICc\n32 3 1 3 11188.5 11226.66 11188.67\n\n\nThis gives me a Regression with model ARIMA(3,1,3).\n\n\nCode\nmodel_output1 &lt;- capture.output(sarima(res.fit1, 3,1,3)) \n\n\n\n\n\nCode\ncat(model_output1[73:86], model_output1[length(model_output1)], sep = \"\\n\")\n\n\nCoefficients: \n           Estimate         SE t.value p.value\nar1         -0.1960     0.0844 -2.3226  0.0204\nar2         -0.4063     0.0721 -5.6334  0.0000\nar3          0.6487     0.0821  7.8979  0.0000\nma1          0.3397     0.0975  3.4825  0.0005\nma2          0.5956     0.0829  7.1818  0.0000\nma3         -0.4884     0.0956 -5.1106  0.0000\nconstant 98615.9324 75690.4242  1.3029  0.1930\n\nsigma^2 estimated as 2.162845e+12 on 864 degrees of freedom \n \nAIC = 31.26626  AICc = 31.26641  BIC = 31.31007 \n \n \n\n\n\nThe Standard Residual Plot exhibits some inconsistencies in terms of mean and variance, suggesting potential issues with the model’s assumptions about homoscedasticity or linearity.\nThe Autocorrelation Function (ACF) Plot exhibits minimal correlation among the residuals, which suggests that the model has effectively captured the underlying patterns in the data, leaving behind what appears to be white noise. This is a strong indicator of a well-fitting model.\nThe Quantile-Quantile (Q-Q) Plot shows that the residuals are mostly aligned with a normal distribution, suggesting that the normality assumption for the errors may largely hold, which is a positive indicator for the model’s reliability.\nResults from the Ljung-Box test show p-values below the 0.05 significance level, which typically suggests a lack of fit as it indicates autocorrelation in the residuals at lag intervals that could influence the model’s accuracy.\nSignificance Table: The statistical significance of all model coefficients is affirmed, implying that each predictor contributes meaningfully to the model. This significance is crucial for understanding the impact of each variable within the model and for making informed predictions or decisions based on the model’s outputs.\n\n\n\n\n\nCode\nfit.reg &lt;- lm(value_sum ~ daily_vaccinations_per_million, data=deva_df.ts)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = value_sum ~ daily_vaccinations_per_million, data = deva_df.ts)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-673660  -81409   69407  113393  222586 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     9.980e+05  8.768e+03  113.82   &lt;2e-16 ***\ndaily_vaccinations_per_million -1.120e+00  4.639e-02  -24.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 174500 on 870 degrees of freedom\nMultiple R-squared:  0.4014,    Adjusted R-squared:  0.4007 \nF-statistic: 583.4 on 1 and 870 DF,  p-value: &lt; 2.2e-16\n\n\nThe variables appear to be significant. Next, looking at the ACF/PACF plot of the residuals.\n\n\nCode\nres.fit1&lt;-ts(residuals(fit.reg),start = decimal_date(as.Date(\"2020-12-20\", format = \"%Y-%m-%d\")),frequency = 365.25)\n\nplot1&lt;-ggAcf(res.fit1,20) + theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \nplot2&lt;- ggPacf(res.fit1,20)+theme_bw()+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nThere appears to be seasonality still, so I will apply some differencing.\n\n\nCode\nplot1&lt;-ggAcf(diff(res.fit1),20) + theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \nplot2&lt;- ggPacf(diff(res.fit1),20)+theme_bw()+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nAccording to the plots, we have the following values: p = 1,2,3 q= 1,2,3.\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) \n\n\nfor (p in 0:3)\n{\n  for(q in 0:3)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(res.fit1/10000,order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n6884.051\n6898.364\n6884.079\n\n\n0\n1\n0\n2772.386\n2781.925\n2772.400\n\n\n0\n0\n1\n5806.323\n5825.406\n5806.369\n\n\n0\n1\n1\n2760.379\n2774.688\n2760.407\n\n\n0\n0\n2\n4940.384\n4964.238\n4940.453\n\n\n0\n1\n2\n2738.328\n2757.406\n2738.374\n\n\n0\n0\n3\n4458.706\n4487.331\n4458.803\n\n\n0\n1\n3\n2734.816\n2758.664\n2734.885\n\n\n1\n0\n0\n2784.017\n2803.100\n2784.063\n\n\n1\n1\n0\n2755.694\n2770.003\n2755.722\n\n\n1\n0\n1\n2772.545\n2796.399\n2772.614\n\n\n1\n1\n1\n2735.762\n2754.840\n2735.808\n\n\n1\n0\n2\n2749.551\n2778.176\n2749.648\n\n\n1\n1\n2\n2730.846\n2754.694\n2730.915\n\n\n1\n0\n3\n2746.060\n2779.455\n2746.189\n\n\n1\n1\n3\n2732.765\n2761.383\n2732.862\n\n\n2\n0\n0\n2767.347\n2791.201\n2767.416\n\n\n2\n1\n0\n2732.869\n2751.948\n2732.916\n\n\n2\n0\n1\n2741.094\n2769.718\n2741.191\n\n\n2\n1\n1\n2731.293\n2755.141\n2731.363\n\n\n2\n0\n2\n2768.397\n2801.793\n2768.527\n\n\n2\n1\n2\n2725.615\n2754.233\n2725.712\n\n\n2\n0\n3\n2744.072\n2782.238\n2744.238\n\n\n2\n1\n3\n2734.797\n2768.185\n2734.927\n\n\n3\n0\n0\n2743.763\n2772.388\n2743.860\n\n\n3\n1\n0\n2730.685\n2754.533\n2730.754\n\n\n3\n0\n1\n2742.232\n2775.628\n2742.362\n\n\n3\n1\n1\n2732.659\n2761.277\n2732.757\n\n\n3\n0\n2\n2746.984\n2785.150\n2747.151\n\n\n3\n1\n2\n2734.658\n2768.046\n2734.788\n\n\n3\n0\n3\n2751.145\n2794.082\n2751.354\n\n\n3\n1\n3\n2702.731\n2740.888\n2702.898\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),] \n\n\n   p d q      AIC      BIC     AICc\n32 3 1 3 2702.731 2740.888 2702.898\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n   p d q      AIC      BIC     AICc\n32 3 1 3 2702.731 2740.888 2702.898\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n32 3 1 3 2702.731 2740.888 2702.898\n\n\nThis gives me a Regression with model ARIMA(3,1,3).\n\n\nCode\nmodel_output1 &lt;- capture.output(sarima(res.fit1, 3,1,3)) \n\n\n\n\n\nCode\ncat(model_output1[77:90], model_output1[length(model_output1)], sep = \"\\n\")\n\n\nCoefficients: \n         Estimate       SE t.value p.value\nar1       -0.1644   0.0803 -2.0482  0.0408\nar2       -0.3805   0.0685 -5.5579  0.0000\nar3        0.6786   0.0780  8.7049  0.0000\nma1        0.3127   0.0951  3.2866  0.0011\nma2        0.5740   0.0806  7.1185  0.0000\nma3       -0.5148   0.0933 -5.5190  0.0000\nconstant 947.2074 603.9139  1.5684  0.1171\n\nsigma^2 estimated as 127026096 on 864 degrees of freedom \n \nAIC = 21.5237  AICc = 21.52385  BIC = 21.56751 \n \n \n\n\n\nThe Standard Residual Plot exhibits some inconsistencies in terms of mean and variance, suggesting potential issues with the model’s assumptions about homoscedasticity or linearity.\nThe Autocorrelation Function (ACF) Plot exhibits minimal correlation among the residuals, which suggests that the model has effectively captured the underlying patterns in the data, leaving behind what appears to be white noise. This is a strong indicator of a well-fitting model.\nThe Quantile-Quantile (Q-Q) Plot shows that the residuals are mostly aligned with a normal distribution, suggesting that the normality assumption for the errors may largely hold, which is a positive indicator for the model’s reliability.\nResults from the Ljung-Box test show p-values below the 0.05 significance level, which typically suggests a lack of fit as it indicates autocorrelation in the residuals at lag intervals that could influence the model’s accuracy.\nSignificance Table: The statistical significance of all model coefficients is affirmed, implying that each predictor contributes meaningfully to the model. This significance is crucial for understanding the impact of each variable within the model and for making informed predictions or decisions based on the model’s outputs.\n\n\n\n\n\nCode\nfit.reg &lt;- lm(Unemployment ~ daily_vaccinations_per_million + value_sum, data=ddd_df.ts)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = Unemployment ~ daily_vaccinations_per_million + \n    value_sum, data = ddd_df.ts)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.008137 -0.002539  0.001572  0.002869  0.004882 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     7.323e-02  3.660e-03  20.008 3.78e-11 ***\ndaily_vaccinations_per_million  1.333e-09  8.421e-09   0.158    0.877    \nvalue_sum                      -5.306e-10  6.405e-11  -8.283 1.52e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.004287 on 13 degrees of freedom\nMultiple R-squared:  0.8501,    Adjusted R-squared:  0.827 \nF-statistic: 36.86 on 2 and 13 DF,  p-value: 4.396e-06\n\n\nThe confirmed COVID-19 case number appears to be significant. Next, looking at the ACF/PACF plot of the residuals.\n\n\nCode\nres.fit1&lt;-ts(residuals(fit.reg),start = decimal_date(as.Date(\"2020-12-01\", format = \"%Y-%m-%d\")),frequency = 12)\n\nplot1&lt;-ggAcf(res.fit1) + theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \nplot2&lt;- ggPacf(res.fit1)+theme_bw()+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\nAccording to the plots, we have the following values: p = 1 q= 1.\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*8),nrow=8) \n\n\nfor (p in 0:1)\n{\n  for(q in 0:1)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(res.fit1/10000,order=c(p,d,q),include.drift=TRUE) \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-422.7689\n-420.4511\n-420.7689\n\n\n0\n1\n0\n-403.1358\n-401.7197\n-402.1358\n\n\n0\n0\n1\n-427.6066\n-424.5162\n-423.9702\n\n\n0\n1\n1\n-401.2666\n-399.1424\n-399.0848\n\n\n1\n0\n0\n-428.8463\n-425.7560\n-425.2100\n\n\n1\n1\n0\n-401.2515\n-399.1273\n-399.0697\n\n\n1\n0\n1\n-427.8426\n-423.9797\n-421.8426\n\n\n1\n1\n1\n-399.2710\n-396.4388\n-395.2710\n\n\n\n\n\nCode\ntemp[which.min(temp$AIC),] \n\n\n  p d q       AIC      BIC    AICc\n5 1 0 0 -428.8463 -425.756 -425.21\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q       AIC      BIC    AICc\n5 1 0 0 -428.8463 -425.756 -425.21\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q       AIC      BIC    AICc\n5 1 0 0 -428.8463 -425.756 -425.21\n\n\nThis gives me a Regression with model ARIMA(1,0,0).\n\n\nCode\nmodel_output1 &lt;- capture.output(sarima(res.fit1, 1,0,0)) \n\n\n\n\n\nCode\ncat(model_output1[21:29], model_output1[length(model_output1)], sep = \"\\n\")\n\n\nCoefficients: \n      Estimate     SE t.value p.value\nar1     0.6799 0.1747  3.8916  0.0016\nxmean   0.0007 0.0020  0.3457  0.7347\n\nsigma^2 estimated as 7.862675e-06 on 14 degrees of freedom \n \nAIC = -8.501737  AICc = -8.444045  BIC = -8.356877 \n \n \n\n\n\nThe Standard Residual Plot exhibits some inconsistencies in terms of mean and variance, suggesting potential issues with the model’s assumptions about homoscedasticity or linearity.\nThe Autocorrelation Function (ACF) Plot exhibits minimal correlation among the residuals, which suggests that the model has effectively captured the underlying patterns in the data, leaving behind what appears to be white noise. This is a strong indicator of a well-fitting model.\nThe Quantile-Quantile (Q-Q) Plot shows that the residuals are mostly aligned with a normal distribution, suggesting that the normality assumption for the errors may largely hold, which is a positive indicator for the model’s reliability.\nResults from the Ljung-Box test show p-values above the 0.05 significance level, which typically suggests a good fit as it indicates autocorrelation in the residuals at lag intervals.\nSignificance Table: The statistical significance of all model coefficients is affirmed, implying that each predictor contributes meaningfully to the model. This significance is crucial for understanding the impact of each variable within the model and for making informed predictions or decisions based on the model’s outputs."
  },
  {
    "objectID": "MT.html#cross-validation",
    "href": "MT.html#cross-validation",
    "title": "Multivariate TS Models",
    "section": "3.4 Cross Validation",
    "text": "3.4 Cross Validation\nTo confirm our selection on the model, we choose to use cross validation to compare the RMSE values to help us make decision.\n\n(ARIMAX)Confirmed COVID-19 Cases ~ Daily Vaccination Number(ARIMAX)COVID-19 Death Cases ~ Daily Vaccination Number(ARIMAX)Unemployment Rate ~ Daily Vaccination Number + Confirmed COVID-19 Cases\n\n\nAuto.arima gave me ARIMA(3,1,5), while the manual model selection resulted in ARIMA(3,1,3).\n\n\nCode\nfit.reg &lt;- lm(value_sum ~ daily_vaccinations_per_million, data=cava_df.ts)\nres.fit1&lt;-ts(residuals(fit.reg),start = decimal_date(as.Date(\"2020-12-20\", format = \"%Y-%m-%d\")),frequency = 365.25)\n\nn = length(res.fit1) # 872\nk = 220\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain &lt;- res.fit1[1:(k-1)+i] #observations from 1 to 12\n  xtest &lt;- res.fit1[k+i] #13th observation as the test set\n  \n  fit1 &lt;- Arima(xtrain, order=c(3,1,5),include.drift=FALSE, method=\"ML\")\n  fcast1 &lt;- forecast(fit1, h=1)\n  \n  fit2 &lt;- Arima(xtrain, order=c(3,1,3),include.drift=FALSE, method=\"ML\")\n  fcast2 &lt;- forecast(fit2, h=1)\n\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  #print(i)\n}\n\nRMSE1 &lt;- sqrt(mean(err3, na.rm = TRUE))\nRMSE2 &lt;- sqrt(mean(err4, na.rm = TRUE))\n\ncat('The RMSE of Auto Arima is', RMSE1, '\\n')\n\n\nThe RMSE of Auto Arima is 951.6204 \n\n\nCode\ncat('The RMSE of Manually Selection is', RMSE2, '\\n')\n\n\nThe RMSE of Manually Selection is 1930.114 \n\n\nThe Auto Arima model had lower RMSE compared to the manual model. Therefore, the best model is ARIMA(3,1,5).\n\n\nAuto.arima gave me ARIMA(5,2,2), while the manual model selection resulted in ARIMA(3,1,3).\n\n\nCode\nfit.reg &lt;- lm(value_sum ~ daily_vaccinations_per_million, data=deva_df.ts)\nres.fit1&lt;-ts(residuals(fit.reg),start = decimal_date(as.Date(\"2020-12-20\", format = \"%Y-%m-%d\")),frequency = 365.25)\n\nn = length(res.fit1) # 872\nk = 220\n\ni=1\nerr1 = c()\nerr2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain &lt;- res.fit1[1:(k-1)+i] #observations from 1 to 12\n  xtest &lt;- res.fit1[k+i] #13th observation as the test set\n  \n  fit1 &lt;- Arima(xtrain, order=c(5,2,2),include.drift=FALSE, method=\"ML\")\n  fcast1 &lt;- forecast(fit1, h=1)\n  \n  fit2 &lt;- Arima(xtrain, order=c(3,1,3),include.drift=FALSE, method=\"ML\")\n  fcast2 &lt;- forecast(fit2, h=1)\n\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  #print(i)\n}\n\nRMSE1 &lt;- sqrt(mean(err3, na.rm = TRUE))\nRMSE2 &lt;- sqrt(mean(err4, na.rm = TRUE))\n\ncat('The RMSE of Auto Arima is', RMSE1, '\\n')\n\n\nThe RMSE of Auto Arima is 57.65793 \n\n\nCode\ncat('The RMSE of Manually Selection is', RMSE2, '\\n')\n\n\nThe RMSE of Manually Selection is 59.52566 \n\n\nThe manual model had lower RMSE compared to the Auto Arima model. Therefore, the best model is ARIMA(3,1,3).\n\n\nAuto.arima gave me ARIMA(0,0,0), while the manual model selection resulted in ARIMA(1,0,0). Since the data sample for this time series only contains 16 observations, we choose to use ARIMA(1,0,0) since ARIMA(0,0,0) contains more biases"
  },
  {
    "objectID": "MT.html#model-fitting",
    "href": "MT.html#model-fitting",
    "title": "Multivariate TS Models",
    "section": "3.5 Model Fitting",
    "text": "3.5 Model Fitting\nIn this section, we will use the best model to fit the model.\n\n(ARIMAX)Confirmed COVID-19 Cases ~ Daily Vaccination Number(ARIMAX)COVID-19 Death Cases ~ Daily Vaccination Number(ARIMAX)Unemployment Rate ~ Daily Vaccination Number + Confirmed COVID-19 Cases\n\n\nThe best ARIMAX model for this relationship is ARIMA(3,1,5).\n\n\nCode\nfit1 &lt;- Arima(cava_df.ts[,\"value_sum\"],order=c(3,1,5), xreg=cava_df.ts[,\"daily_vaccinations_per_million\"])\nsummary(fit1)\n\n\nSeries: cava_df.ts[, \"value_sum\"] \nRegression with ARIMA(3,1,5) errors \n\nCoefficients:\n          ar1    ar2     ar3     ma1      ma2      ma3     ma4     ma5     xreg\n      -0.0956  0.533  0.5298  0.2741  -0.6293  -0.6067  0.3318  0.5648  -0.0029\ns.e.   0.0594  0.043  0.0667  0.0513   0.0371   0.0584  0.0280  0.0308   0.1508\n\nsigma^2 = 4.609e+09:  log likelihood = -10924.09\nAIC=21868.18   AICc=21868.44   BIC=21915.88\n\nTraining set error measures:\n                   ME     RMSE      MAE         MPE       MAPE         MASE\nTraining set 2872.186 67499.31 36164.96 0.007343933 0.06440783 0.0008628181\n                   ACF1\nTraining set 0.02757954\n\n\nEquation:\n\\[ \\nabla Y_t = \\mu - 0.0956 \\nabla Y_{t-1} + 0.533 \\nabla Y_{t-2} + 0.5298 \\nabla Y_{t-3} + 0.2741 \\epsilon_{t-1} - 0.6293 \\epsilon_{t-2} - 0.6067 \\epsilon_{t-3} + 0.3318 \\epsilon_{t-4} + 0.5648 \\epsilon_{t-5} - 0.0029 X_t + \\epsilon_t  \\]\n\n\nThe best ARIMAX model for this relationship is ARIMA(3,1,3).\n\n\nCode\nfit1 &lt;- Arima(deva_df.ts[,\"value_sum\"],order=c(3,1,3), xreg=deva_df.ts[,\"daily_vaccinations_per_million\"])\nsummary(fit1)\n\n\nSeries: deva_df.ts[, \"value_sum\"] \nRegression with ARIMA(3,1,3) errors \n\nCoefficients:\n         ar1      ar2     ar3      ma1     ma2      ma3    xreg\n      0.5142  -0.4725  0.9553  -0.3085  0.4341  -0.8067  0.0018\ns.e.  0.0153   0.0215  0.0156   0.0260  0.0268   0.0195  0.0019\n\nsigma^2 = 378948:  log likelihood = -6828.51\nAIC=13673.02   AICc=13673.19   BIC=13711.18\n\nTraining set error measures:\n                    ME     RMSE      MAE           MPE       MAPE       MASE\nTraining set -10.57903 612.7571 411.9691 -0.0006127656 0.05651456 0.00125989\n                   ACF1\nTraining set 0.05821364\n\n\nEquation:\n\\[ \\nabla Y_t = \\mu + 0.5142 \\nabla Y_{t-1} - 0.4725 \\nabla Y_{t-2} + 0.9553 \\nabla Y_{t-3} - 0.3085 \\epsilon_{t-1} + 0.4341 \\epsilon_{t-2} - 0.8067 \\epsilon_{t-3} + 0.0018 X_t + \\epsilon_t  \\]\n\n\nThe best ARIMAX model for this relationship is ARIMA(1,0,0).\n\n\nCode\nfit1 &lt;- Arima(ddd_df.ts[,\"Unemployment\"],order=c(1,0,0), xreg=ddd_df.ts[,2:3])\nsummary(fit1)\n\n\nSeries: ddd_df.ts[, \"Unemployment\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n        ar1  intercept  daily_vaccinations_per_million  value_sum\n      0.897      0.066                               0          0\ns.e.    NaN        NaN                             NaN        NaN\n\nsigma^2 = 8.212e-06:  log likelihood = 72.46\nAIC=-134.92   AICc=-128.92   BIC=-131.06\n\nTraining set error measures:\n                        ME        RMSE         MAE      MPE     MAPE       MASE\nTraining set -0.0005780741 0.002481684 0.002158275 -1.36368 4.464085 0.08720304\n                    ACF1\nTraining set -0.08143333\n\n\nEquation:\n\\[ \\nabla Y_t = 0.066 + 0.897 \\nabla Y_{t-1} + \\epsilon_t  \\]"
  },
  {
    "objectID": "MT.html#forecast",
    "href": "MT.html#forecast",
    "title": "Multivariate TS Models",
    "section": "3.6 Forecast",
    "text": "3.6 Forecast\nThen, I use the best model for each relationship and forecast the values.\n\n(ARIMAX)Confirmed COVID-19 Cases ~ Daily Vaccination Number(ARIMAX)COVID-19 Death Cases ~ Daily Vaccination Number(ARIMAX)Unemployment Rate ~ Daily Vaccination Number + Confirmed COVID-19 Cases\n\n\n\n\nCode\nfit1 &lt;- Arima(cava_df.ts[,\"value_sum\"],order=c(3,1,5), xreg=cava_df.ts[,\"daily_vaccinations_per_million\"])\nforecast(fit1, h = 100, xreg=cava_df.ts[,\"daily_vaccinations_per_million\"]) %&gt;%\n  autoplot() + xlab(\"Date\")+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \n\n\n\n\n\nIn summary, this forecast suggests that, according to the ARIMA(3,1,5) model used, the daily confirmed cases are expected to remain at a consistent level in the near term, with the uncertainty of the forecast increasing as it projects further out into the future.\n\n\n\n\nCode\nfit1 &lt;- Arima(deva_df.ts[,\"value_sum\"],order=c(3,1,3), xreg=deva_df.ts[,\"daily_vaccinations_per_million\"])\nforecast(fit1, h = 100, xreg=deva_df.ts[,\"daily_vaccinations_per_million\"]) %&gt;%\n  autoplot() + xlab(\"Date\")+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \n\n\n\n\n\nIn summary, this forecast suggests that, according to the ARIMA(3,1,3) model used, the daily dead cases are expected to remain at a consistent level in the near term, with the uncertainty of the forecast increasing as it projects further out into the future.\n\n\n\n\nCode\nfit1 &lt;- Arima(ddd_df.ts[,\"Unemployment\"],order=c(1,0,0), xreg=ddd_df.ts[,2:3])\nforecast(fit1, h = 4, xreg=ddd_df.ts[,2:3]) %&gt;%\n  autoplot() + xlab(\"Month\")+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA)) \n\n\n\n\n\nIn summary, the model seems to predict that the decreasing trend in unemployment observed in the actual data will not persist in the same way, and that unemployment may stabilize at the levels seen at the end of the observed data."
  },
  {
    "objectID": "MT.html#variable-selection-1",
    "href": "MT.html#variable-selection-1",
    "title": "Multivariate TS Models",
    "section": "4.1 Variable Selection",
    "text": "4.1 Variable Selection\nWhen crafting a VAR model, it is pivotal to delineate each variable that will be considered within the system. Unlike ARIMAX/SARIMAX models, which focus on a primary response variable influenced by exogenous predictors, VAR models treat all included variables as endogenously interrelated. Each variable in a VAR model is explained by its own lagged values as well as the lagged values of all other variables in the system, allowing for a multidirectional analysis of influence and response over time.\nThe selection process for variables in a VAR model is underpinned by both theoretical frameworks and empirical data, necessitating a rigorous review to ascertain their interconnectedness and the absence of unit roots that may indicate non-stationarity. This ensures that each variable not only possesses inherent relevance but also contributes to a comprehensive understanding of the system’s internal dynamics. By employing a VAR model, one can dissect and comprehend the nuanced interplay between variables, enabling a holistic view of how each variable evolves in relation to others within the dataset.\n\n(VAR)Daily Vaccination Number ~ Independent Party Support Rate(VAR)Daily Vaccination Number ~ Democratic Party Support Rate(VAR)Daily Vaccination Number ~ Republican Party Support Rate\n\n\n\n\nCode\ninde_df.ts&lt;-ts(inde_df,star=decimal_date(as.Date(\"2020-12-22\",format = \"%Y-%m-%d\")),frequency = 52)\n\nautoplot(inde_df.ts[,c(2:3)], facets=TRUE, color=\"#5a3196\") +\n  xlab(\"Year\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Daily changes in new vaccination number and independent party support rate\")\n\n\n\n\n\n\n\n\n\nCode\ndemo_df.ts&lt;-ts(demo_df,star=decimal_date(as.Date(\"2020-12-22\",format = \"%Y-%m-%d\")),frequency = 52)\n\nautoplot(demo_df.ts[,c(2:3)], facets=TRUE, color=\"#5a3196\") +\n  xlab(\"Year\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Daily changes in new vaccination number and democratic party support rate\")\n\n\n\n\n\n\n\n\n\nCode\nrep_df.ts&lt;-ts(rep_df,star=decimal_date(as.Date(\"2020-12-22\",format = \"%Y-%m-%d\")),frequency = 52)\n\nautoplot(rep_df.ts[,c(2:3)], facets=TRUE, color=\"#5a3196\") +\n  xlab(\"Year\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Daily changes in new vaccination number and republican party support rate\")"
  },
  {
    "objectID": "MT.html#confirmed-covid-19-cases-as-a-function-of-daily-vaccination-number",
    "href": "MT.html#confirmed-covid-19-cases-as-a-function-of-daily-vaccination-number",
    "title": "Multivariate TS Models",
    "section": "2.1 Confirmed COVID-19 Cases as a Function of Daily Vaccination Number:",
    "text": "2.1 Confirmed COVID-19 Cases as a Function of Daily Vaccination Number:\n(ARIMAX)Confirmed COVID-19 Cases ~ Daily Vaccination Number\nThis model aims to capture the impact of Daily Vaccination Numbers on the trend of newly confirmed COVID-19 cases.\n\n\nCode\n# Read the dataset\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million')\nt &lt;- vac_df[, cols_show]\n\n# Group by 'date' and summarize columns, ignoring NA values\nt1 &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    daily_vaccinations_per_million = sum(daily_vaccinations_per_million, na.rm = TRUE)\n  )\n\n# Convert date column to Date format\nt1$date &lt;- as.Date(t1$date)\n\n# Newly confirmed cases\nwide_data &lt;- read_csv(\"Datasets/covid_confirmed_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ncon_case_df &lt;- long_data %&gt;%\n  group_by(date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Convert date column to Date format\ncon_case_df$date &lt;- as.Date(con_case_df$date)\n\n# Merge them together\ncava_df &lt;- merge(t1, con_case_df, by = \"date\", all = FALSE)\nhead(cava_df)\n\n\n        date daily_vaccinations_per_million value_sum\n1 2020-12-20                              0  17848765\n2 2020-12-21                            174  18010051\n3 2020-12-22                            384  18204174\n4 2020-12-23                            454  18416088\n5 2020-12-24                            575  18609135\n6 2020-12-25                            648  18704673\n\n\nCode\n# Shape of the df\ncat('The shape of this dataframe is', dim(cava_df))\n\n\nThe shape of this dataframe is 872 3"
  },
  {
    "objectID": "MT.html#covid-19-death-cases-as-a-function-of-daily-vaccination-number",
    "href": "MT.html#covid-19-death-cases-as-a-function-of-daily-vaccination-number",
    "title": "Multivariate TS Models",
    "section": "2.2 COVID-19 Death Cases as a Function of Daily Vaccination Number:",
    "text": "2.2 COVID-19 Death Cases as a Function of Daily Vaccination Number:\n(ARIMAX)COVID-19 Death Cases ~ Daily Vaccination Number\nThrough this model, we explore how variations in Daily Vaccination Numbers influence the count of COVID-19-related fatalities.\n\n\nCode\n# Read the dataset\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million')\nt &lt;- vac_df[, cols_show]\n\n# Group by 'date' and summarize columns, ignoring NA values\nt1 &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    daily_vaccinations_per_million = sum(daily_vaccinations_per_million, na.rm = TRUE)\n  )\n\n# Convert date column to Date format\nt1$date &lt;- as.Date(t1$date)\n\n\n# Death cases\nwide_data &lt;- read_csv(\"Datasets/covid_deaths_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ndead_case_df &lt;- long_data %&gt;%\n  group_by(date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Convert date column to Date format\ndead_case_df$date &lt;- as.Date(dead_case_df$date)\n\n# Merge them together\ndeva_df &lt;- merge(t1, dead_case_df, by = \"date\", all = FALSE)\nhead(deva_df)\n\n\n        date daily_vaccinations_per_million value_sum\n1 2020-12-20                              0    324336\n2 2020-12-21                            174    326365\n3 2020-12-22                            384    328950\n4 2020-12-23                            454    332017\n5 2020-12-24                            575    335328\n6 2020-12-25                            648    336496\n\n\nCode\n# Shape of the df\ncat('The shape of this dataframe is', dim(deva_df))\n\n\nThe shape of this dataframe is 872 3"
  },
  {
    "objectID": "MT.html#unemployment-rate-as-influenced-by-daily-vaccination-number-and-confirmed-cases",
    "href": "MT.html#unemployment-rate-as-influenced-by-daily-vaccination-number-and-confirmed-cases",
    "title": "Multivariate TS Models",
    "section": "2.3 Unemployment Rate as Influenced by Daily Vaccination Number and Confirmed Cases:",
    "text": "2.3 Unemployment Rate as Influenced by Daily Vaccination Number and Confirmed Cases:\n(ARIMAX)Unemployment Rate ~ Daily Vaccination Number + Confirmed COVID-19 Cases\nThis model investigates the effect of Daily Vaccination Numbers and COVID-19 case counts on the unemployment rate, providing insights into the pandemic’s broader economic implications.\n\n\nCode\n# Read the dataset\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million')\nt &lt;- vac_df[, cols_show]\n\n# Group by 'date' and summarize columns, ignoring NA values\nt1 &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    daily_vaccinations_per_million = sum(daily_vaccinations_per_million, na.rm = TRUE)\n  )\n\n# Convert date column to Date format\nt1$date &lt;- as.Date(t1$date)\n\n# Aggregate data to monthly level using mean for each column\nt1 &lt;- t1 %&gt;%\n  group_by(date = format(date, \"%Y-%m\")) %&gt;%\n  summarize(daily_vaccinations_per_million = mean(daily_vaccinations_per_million, na.rm = TRUE))\n\n# Transform the date column type with specified format\nt1$date &lt;- as.Date(paste0(t1$date, \"-01-01\"))\n\n# Newly confirmed cases\nwide_data &lt;- read_csv(\"Datasets/covid_confirmed_usafacts.csv\")\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"countyFIPS\", \"County Name\", \"State\", \"StateFIPS\")\nvalue_cols &lt;- setdiff(names(wide_data), key_cols)\n\n# Pivot the data from wide to long\nlong_data &lt;- pivot_longer(\n  wide_data,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"value\"\n)\n\n# Group by 'State' and 'date', and calculate the sum of Confirmed Cases\ncon_case_df &lt;- long_data %&gt;%\n  group_by(date) %&gt;%\n  summarize(value_sum = sum(value, na.rm = TRUE))\n\n# Convert date column to Date format\ncon_case_df$date &lt;- as.Date(con_case_df$date)\n\n# Aggregate data to monthly level using mean for each column\ncon_case_df &lt;- con_case_df %&gt;%\n  group_by(date = format(date, \"%Y-%m\")) %&gt;%\n  summarize(value_sum = mean(value_sum, na.rm = TRUE))\n\n# Transform the date column type with specified format\ncon_case_df$date &lt;- as.Date(paste0(con_case_df$date, \"-01-01\"))\n\n\nunemp &lt;- read_csv('Datasets/unemployment.csv')\nkey_cols &lt;- c(\"Location\")\nvalue_cols &lt;- setdiff(names(unemp), key_cols)\nunemp1 &lt;- pivot_longer(\n  unemp,\n  cols = value_cols,\n  names_to = \"Time\",\n  values_to = \"Unemployment\"\n)\n\n# Convert Time column to Date format\nunemp1$date &lt;- as.Date(paste0(unemp1$Time, \"-01\"))\n\n# Convert Unemployment column to numeric (floating-point) format\nunemp1$Unemployment &lt;- as.numeric(unemp1$Unemployment)\n\n# Focus on US\nunemp2 &lt;- unemp1[unemp1$Location =='United States',]\nunemp3 &lt;- unemp2[,c('date', 'Unemployment')]\n\n# Merge them together\ndd_df &lt;- merge(t1, con_case_df, by = \"date\", all = FALSE)\nddd_df &lt;- merge(dd_df, unemp3, by = \"date\", all = FALSE)\n\nhead(ddd_df)\n\n\n        date daily_vaccinations_per_million value_sum Unemployment\n1 2020-12-01                       544.9167  16927034        0.067\n2 2021-01-01                    123266.4194  23365835        0.063\n3 2021-02-01                    304911.1429  27271116        0.062\n4 2021-03-01                    429380.9032  29098434        0.060\n5 2021-04-01                    514652.3333  30975088        0.061\n6 2021-05-01                    319535.1290  32356593        0.058\n\n\nCode\n# Shape of the df\ncat('The shape of this dataframe is', dim(ddd_df))\n\n\nThe shape of this dataframe is 16 4"
  },
  {
    "objectID": "MT.html#interdependence-between-daily-vaccination-number-and-independent-party-support-rate",
    "href": "MT.html#interdependence-between-daily-vaccination-number-and-independent-party-support-rate",
    "title": "Multivariate TS Models",
    "section": "2.4 Interdependence between Daily Vaccination Number and Independent Party Support Rate:",
    "text": "2.4 Interdependence between Daily Vaccination Number and Independent Party Support Rate:\n(VAR)Daily Vaccination Number ~ Independent Party Support Rate\nThis model evaluates how vaccination efforts and support for independent political parties influence each other over time.\n\n\nCode\n# Read the dataset\nvac_df &lt;- read_csv(\"Datasets/us_state_vaccinations.csv\")\n\n# Select relevant columns\ncols_show &lt;- c('date', 'location', 'daily_vaccinations_per_million')\nt &lt;- vac_df[, cols_show]\n\n# Group by 'date' and summarize columns, ignoring NA values\nt1 &lt;- t %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    daily_vaccinations_per_million = sum(daily_vaccinations_per_million, na.rm = TRUE)\n  )\n\n# Convert date column to Date format\nt1$date &lt;- as.Date(t1$date)\n\ndemo &lt;- read_excel('Datasets/party.xlsx',sheet = 'Democrat')\ninde &lt;- read_excel('Datasets/party.xlsx',sheet = 'Independent')\nrep &lt;- read_excel('Datasets/party.xlsx',sheet = 'Republican')\n\n# Transform the wide dataframe into a long dataframe\nkey_cols &lt;- c(\"Attitude\")\nvalue_cols &lt;- setdiff(names(demo), key_cols)\ndemo1 &lt;- pivot_longer(\n  demo,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"democrat\"\n)\n\ninde1 &lt;- pivot_longer(\n  inde,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"independent\"\n)\n\nrep1 &lt;- pivot_longer(\n  rep,\n  cols = value_cols,\n  names_to = \"date\",\n  values_to = \"republican\"\n)\n\n# Combine these three datasets together\ncombined_data &lt;- full_join(demo1, inde1, by = c(\"date\", \"Attitude\")) %&gt;%\n  full_join(rep1, by = c(\"date\", \"Attitude\"))\ncombined_data1 &lt;- combined_data[combined_data$Attitude=='Favorable',]\n\n# Define the key and value columns for pivoting\nkey_cols &lt;- c(\"Attitude\", \"date\")\nvalue_cols &lt;- setdiff(names(combined_data1), key_cols)\n\n# Pivot the data from wide to long\ncombined_data2 &lt;- pivot_longer(\n  combined_data1,\n  cols = value_cols,\n  names_to = \"Party\",\n  values_to = \"value\"\n)\n\n# Convert date column to Date format\ncombined_data2$date &lt;- as.Date(combined_data2$date)\n\n# Subset to each party\ndemo_data &lt;- combined_data2[combined_data2$Party=='democrat',][,c(\"date\",\"value\")]\ninde_data &lt;- combined_data2[combined_data2$Party=='independent',][,c(\"date\",\"value\")]\nrep_data &lt;- combined_data2[combined_data2$Party=='republican',][,c(\"date\",\"value\")]\n\ninde_df &lt;- merge(t1, inde_data, by = \"date\", all = FALSE)\nhead(inde_df)\n\n\n        date daily_vaccinations_per_million value\n1 2020-12-22                            384  0.28\n2 2021-01-05                           1022  0.30\n3 2021-01-12                           1932  0.30\n4 2021-01-19                         174090  0.32\n5 2021-01-26                         214206  0.29\n6 2021-02-02                         238622  0.30\n\n\nCode\n# Shape of the df\ncat('The shape of this dataframe is', dim(inde_df))\n\n\nThe shape of this dataframe is 117 3"
  },
  {
    "objectID": "MT.html#daily-vaccination-number-and-democratic-party-support-rate-dynamics",
    "href": "MT.html#daily-vaccination-number-and-democratic-party-support-rate-dynamics",
    "title": "Multivariate TS Models",
    "section": "2.5 Daily Vaccination Number and Democratic Party Support Rate Dynamics:",
    "text": "2.5 Daily Vaccination Number and Democratic Party Support Rate Dynamics:\n(VAR)Daily Vaccination Number ~ Democratic Party Support Rate\nHere, we assess the mutual influences between COVID-19 Daily Vaccination Numbers and support levels for the Democratic Party.\n\n\nCode\ndemo_df &lt;- merge(t1, demo_data, by = \"date\", all = FALSE)\nhead(demo_df)\n\n\n        date daily_vaccinations_per_million value\n1 2020-12-22                            384  0.86\n2 2021-01-05                           1022  0.86\n3 2021-01-12                           1932  0.90\n4 2021-01-19                         174090  0.88\n5 2021-01-26                         214206  0.91\n6 2021-02-02                         238622  0.88\n\n\nCode\n# Shape of the df\ncat('The shape of this dataframe is', dim(demo_df))\n\n\nThe shape of this dataframe is 117 3"
  },
  {
    "objectID": "MT.html#the-relationship-between-daily-vaccination-number-and-republican-party-support-rate",
    "href": "MT.html#the-relationship-between-daily-vaccination-number-and-republican-party-support-rate",
    "title": "Multivariate TS Models",
    "section": "2.6 The Relationship between Daily Vaccination Number and Republican Party Support Rate:",
    "text": "2.6 The Relationship between Daily Vaccination Number and Republican Party Support Rate:\n(VAR)Daily Vaccination Number ~ Republican Party Support Rate\nThis model explores the bidirectional influence between vaccination initiatives and Republican Party support rates.\n\n\nCode\nrep_df &lt;- merge(t1, rep_data, by = \"date\", all = FALSE)\nhead(rep_df)\n\n\n        date daily_vaccinations_per_million value\n1 2020-12-22                            384  0.05\n2 2021-01-05                           1022  0.09\n3 2021-01-12                           1932  0.10\n4 2021-01-19                         174090  0.09\n5 2021-01-26                         214206  0.09\n6 2021-02-02                         238622  0.07\n\n\nCode\n# Shape of the df\ncat('The shape of this dataframe is', dim(rep_df))\n\n\nThe shape of this dataframe is 117 3"
  },
  {
    "objectID": "MT.html#variable-selection-2",
    "href": "MT.html#variable-selection-2",
    "title": "Multivariate TS Models",
    "section": "4.2 Variable Selection",
    "text": "4.2 Variable Selection\nIn the process of refining our VAR model, a critical step involves selecting the optimal number of lag periods, denoted as ( p ). This selection is pivotal as it significantly influences the model’s accuracy and effectiveness in capturing the dynamics between the time series variables. To determine the best ( p ), we utilize criteria such as the Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), and the Hannan-Quinn Information Criterion (HQIC), which help in balancing model complexity and goodness of fit.\n\n(VAR)Daily Vaccination Number ~ Independent Party Support Rate(VAR)Daily Vaccination Number ~ Democratic Party Support Rate(VAR)Daily Vaccination Number ~ Republican Party Support Rate\n\n\n\n\nCode\nVARselect(inde_df.ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     3      1      1      3 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 1.480094e+01 1.475230e+01 1.474972e+01 1.479836e+01 1.486156e+01\nHQ(n)  1.495284e+01 1.499533e+01 1.508389e+01 1.522367e+01 1.537801e+01\nSC(n)  1.517564e+01 1.535181e+01 1.557405e+01 1.584751e+01 1.613553e+01\nFPE(n) 2.679516e+06 2.553912e+06 2.550776e+06 2.684193e+06 2.869670e+06\n                  6            7            8            9           10\nAIC(n) 1.482814e+01 1.493031e+01 1.505620e+01 1.509082e+01 1.516958e+01\nHQ(n)  1.543572e+01 1.562903e+01 1.584606e+01 1.597182e+01 1.614172e+01\nSC(n)  1.632692e+01 1.665391e+01 1.700461e+01 1.726405e+01 1.756763e+01\nFPE(n) 2.789790e+06 3.111892e+06 3.562422e+06 3.732117e+06 4.098560e+06\n\n\nAccording to the selection criterion, we select the model with p=1 and 3.\n\n\nCode\nsummary(vars::VAR(inde_df.ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: date, daily_vaccinations_per_million, value \nDeterministic variables: both \nSample size: 116 \nLog Likelihood: -1356.754 \nRoots of the characteristic polynomial:\n0.914 0.8901 0.3618\nCall:\nvars::VAR(y = inde_df.ts, p = 1, type = \"both\")\n\n\nEstimation results for equation date: \n===================================== \ndate = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + const + trend \n\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                           9.030e-01  4.630e-02  19.505   &lt;2e-16 ***\ndaily_vaccinations_per_million.l1 1.724e-08  1.882e-06   0.009   0.9927    \nvalue.l1                          2.129e+00  5.309e+00   0.401   0.6891    \nconst                             1.811e+03  8.609e+02   2.103   0.0377 *  \ntrend                             7.207e-01  3.417e-01   2.109   0.0372 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.859 on 111 degrees of freedom\nMultiple R-Squared: 0.9999, Adjusted R-squared: 0.9999 \nF-statistic: 5.17e+05 on 4 and 111 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation daily_vaccinations_per_million: \n=============================================================== \ndaily_vaccinations_per_million = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                           -6.894e+02  8.357e+02  -0.825  0.41120    \ndaily_vaccinations_per_million.l1  8.705e-01  3.397e-02  25.624  &lt; 2e-16 ***\nvalue.l1                           2.724e+05  9.583e+04   2.842  0.00533 ** \nconst                              1.280e+07  1.554e+07   0.824  0.41196    \ntrend                              4.604e+03  6.167e+03   0.746  0.45698    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 33560 on 111 degrees of freedom\nMultiple R-Squared: 0.9339, Adjusted R-squared: 0.9315 \nF-statistic: 391.8 on 4 and 111 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation value: \n====================================== \nvalue = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            1.959e-03  7.669e-04   2.555   0.0120 *  \ndaily_vaccinations_per_million.l1  4.250e-08  3.117e-08   1.363   0.1755    \nvalue.l1                           3.924e-01  8.794e-02   4.462 1.95e-05 ***\nconst                             -3.631e+01  1.426e+01  -2.546   0.0123 *  \ntrend                             -1.427e-02  5.659e-03  -2.521   0.0131 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.03079 on 111 degrees of freedom\nMultiple R-Squared: 0.3111, Adjusted R-squared: 0.2863 \nF-statistic: 12.53 on 4 and 111 DF,  p-value: 1.895e-08 \n\n\n\nCovariance matrix of residuals:\n                                     date daily_vaccinations_per_million\ndate                            3.456e+00                     -1.884e+04\ndaily_vaccinations_per_million -1.884e+04                      1.126e+09\nvalue                           6.952e-03                     -3.151e+01\n                                    value\ndate                            6.952e-03\ndaily_vaccinations_per_million -3.151e+01\nvalue                           9.483e-04\n\nCorrelation matrix of residuals:\n                                  date daily_vaccinations_per_million    value\ndate                            1.0000                       -0.30194  0.12144\ndaily_vaccinations_per_million -0.3019                        1.00000 -0.03049\nvalue                           0.1214                       -0.03049  1.00000\n\n\n\n\nCode\nsummary(vars::VAR(inde_df.ts, p=3, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: date, daily_vaccinations_per_million, value \nDeterministic variables: both \nSample size: 114 \nLog Likelihood: -1301.287 \nRoots of the characteristic polynomial:\n0.9039 0.9039 0.773 0.4963 0.4963 0.4696 0.4696 0.4464 0.435\nCall:\nvars::VAR(y = inde_df.ts, p = 3, type = \"both\")\n\n\nEstimation results for equation date: \n===================================== \ndate = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + date.l2 + daily_vaccinations_per_million.l2 + value.l2 + date.l3 + daily_vaccinations_per_million.l3 + value.l3 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            7.913e-01  1.008e-01   7.853 4.05e-12 ***\ndaily_vaccinations_per_million.l1 -5.850e-06  5.182e-06  -1.129   0.2616    \nvalue.l1                          -3.134e+00  5.909e+00  -0.530   0.5970    \ndate.l2                            1.998e-01  1.256e-01   1.590   0.1149    \ndaily_vaccinations_per_million.l2  4.061e-06  7.061e-06   0.575   0.5665    \nvalue.l2                           8.315e+00  5.763e+00   1.443   0.1521    \ndate.l3                           -8.653e-02  9.656e-02  -0.896   0.3723    \ndaily_vaccinations_per_million.l3  4.690e-06  4.936e-06   0.950   0.3443    \nvalue.l3                          -1.275e+00  5.978e+00  -0.213   0.8316    \nconst                              1.781e+03  9.125e+02   1.952   0.0537 .  \ntrend                              7.192e-01  3.613e-01   1.990   0.0492 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.73 on 103 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 2.27e+05 on 10 and 103 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation daily_vaccinations_per_million: \n=============================================================== \ndaily_vaccinations_per_million = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + date.l2 + daily_vaccinations_per_million.l2 + value.l2 + date.l3 + daily_vaccinations_per_million.l3 + value.l3 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            9.748e+02  1.864e+03   0.523  0.60211    \ndaily_vaccinations_per_million.l1  9.188e-01  9.585e-02   9.585 6.18e-16 ***\nvalue.l1                           1.874e+05  1.093e+05   1.715  0.08943 .  \ndate.l2                            1.743e+03  2.324e+03   0.750  0.45488    \ndaily_vaccinations_per_million.l2  1.470e-01  1.306e-01   1.125  0.26303    \nvalue.l2                           7.362e+04  1.066e+05   0.691  0.49137    \ndate.l3                           -3.968e+03  1.786e+03  -2.222  0.02848 *  \ndaily_vaccinations_per_million.l3 -2.530e-01  9.131e-02  -2.771  0.00663 ** \nvalue.l3                           7.192e+04  1.106e+05   0.650  0.51690    \nconst                              2.319e+07  1.688e+07   1.374  0.17237    \ntrend                              8.562e+03  6.683e+03   1.281  0.20300    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 32000 on 103 degrees of freedom\nMultiple R-Squared: 0.943,  Adjusted R-squared: 0.9375 \nF-statistic: 170.4 on 10 and 103 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation value: \n====================================== \nvalue = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + date.l2 + daily_vaccinations_per_million.l2 + value.l2 + date.l3 + daily_vaccinations_per_million.l3 + value.l3 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)   \ndate.l1                            4.375e-03  1.641e-03   2.666  0.00892 **\ndaily_vaccinations_per_million.l1  1.207e-07  8.440e-08   1.431  0.15557   \nvalue.l1                           1.576e-01  9.624e-02   1.638  0.10448   \ndate.l2                           -1.759e-03  2.046e-03  -0.860  0.39201   \ndaily_vaccinations_per_million.l2 -2.634e-08  1.150e-07  -0.229  0.81928   \nvalue.l2                           3.130e-01  9.386e-02   3.335  0.00119 **\ndate.l3                           -1.981e-03  1.573e-03  -1.259  0.21072   \ndaily_vaccinations_per_million.l3 -7.508e-08  8.040e-08  -0.934  0.35255   \nvalue.l3                           1.861e-01  9.737e-02   1.911  0.05873 . \nconst                             -1.178e+01  1.486e+01  -0.793  0.42971   \ntrend                             -4.541e-03  5.885e-03  -0.772  0.44212   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.02818 on 103 degrees of freedom\nMultiple R-Squared: 0.4611, Adjusted R-squared: 0.4088 \nF-statistic: 8.814 on 10 and 103 DF,  p-value: 2.602e-10 \n\n\n\nCovariance matrix of residuals:\n                                     date daily_vaccinations_per_million\ndate                            2.993e+00                     -1.450e+04\ndaily_vaccinations_per_million -1.450e+04                      1.024e+09\nvalue                           3.949e-03                     -8.783e+01\n                                    value\ndate                            3.949e-03\ndaily_vaccinations_per_million -8.783e+01\nvalue                           7.938e-04\n\nCorrelation matrix of residuals:\n                                   date daily_vaccinations_per_million    value\ndate                            1.00000                       -0.26203  0.08103\ndaily_vaccinations_per_million -0.26203                        1.00000 -0.09742\nvalue                           0.08103                       -0.09742  1.00000\n\n\nExamining the VAR models with ( p = 1 ) and ( p = 3 ) lag orders, we notice the presence of some variables that do not significantly contribute to the model. Notably, the model with ( p = 3 ) lag periods tends to have fewer significant variables compared to the ( p = 1 ) model. However, opting for ( p = 1 ) may overly simplify the model, potentially failing to capture all the relevant and significant relationships among the variables. Thus, while ( p = 1 ) offers a more parsimonious model, it might not sufficiently account for the complexity of the dynamics within the data, suggesting that a balance must be struck between model simplicity and its ability to elucidate significant interactions.\n\n\n\n\nCode\nVARselect(demo_df.ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     6      1      1      6 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 1.438534e+01 1.440756e+01 1.431920e+01 1.433737e+01 1.427389e+01\nHQ(n)  1.453724e+01 1.465060e+01 1.465337e+01 1.476268e+01 1.479034e+01\nSC(n)  1.476004e+01 1.500708e+01 1.514353e+01 1.538652e+01 1.554786e+01\nFPE(n) 1.768330e+06 1.809214e+06 1.658430e+06 1.692801e+06 1.594443e+06\n                  6            7            8            9           10\nAIC(n) 1.422535e+01 1.426444e+01 1.438205e+01 1.445555e+01 1.448973e+01\nHQ(n)  1.483294e+01 1.496317e+01 1.517191e+01 1.533655e+01 1.546186e+01\nSC(n)  1.572414e+01 1.598804e+01 1.633046e+01 1.662879e+01 1.688778e+01\nFPE(n) 1.526816e+06 1.598976e+06 1.815374e+06 1.977256e+06 2.076702e+06\n\n\nAccording to the selection criterion, we select the model with p=1 and 6.\n\n\nCode\nsummary(vars::VAR(demo_df.ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: date, daily_vaccinations_per_million, value \nDeterministic variables: both \nSample size: 116 \nLog Likelihood: -1326.706 \nRoots of the characteristic polynomial:\n0.9116 0.868 0.4189\nCall:\nvars::VAR(y = demo_df.ts, p = 1, type = \"both\")\n\n\nEstimation results for equation date: \n===================================== \ndate = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            9.152e-01  4.362e-02  20.982   &lt;2e-16 ***\ndaily_vaccinations_per_million.l1  3.004e-07  1.856e-06   0.162   0.8717    \nvalue.l1                          -3.985e+00  6.743e+00  -0.591   0.5557    \nconst                              1.588e+03  8.105e+02   1.959   0.0526 .  \ntrend                              6.317e-01  3.224e-01   1.959   0.0526 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.857 on 111 degrees of freedom\nMultiple R-Squared: 0.9999, Adjusted R-squared: 0.9999 \nF-statistic: 5.178e+05 on 4 and 111 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation daily_vaccinations_per_million: \n=============================================================== \ndaily_vaccinations_per_million = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                           -2.499e+02  7.848e+02  -0.318  0.75077    \ndaily_vaccinations_per_million.l1  8.776e-01  3.340e-02  26.278  &lt; 2e-16 ***\nvalue.l1                           3.652e+05  1.213e+05   3.011  0.00323 ** \nconst                              4.376e+06  1.458e+07   0.300  0.76469    \ntrend                              1.412e+03  5.801e+03   0.243  0.80807    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 33420 on 111 degrees of freedom\nMultiple R-Squared: 0.9344, Adjusted R-squared: 0.932 \nF-statistic: 395.3 on 4 and 111 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation value: \n====================================== \nvalue = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            9.846e-04  5.604e-04   1.757   0.0817 .  \ndaily_vaccinations_per_million.l1 -6.079e-09  2.385e-08  -0.255   0.7993    \nvalue.l1                           4.058e-01  8.662e-02   4.684 8.02e-06 ***\nconst                             -1.780e+01  1.041e+01  -1.710   0.0901 .  \ntrend                             -7.294e-03  4.142e-03  -1.761   0.0810 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.02386 on 111 degrees of freedom\nMultiple R-Squared: 0.2103, Adjusted R-squared: 0.1819 \nF-statistic: 7.391 on 4 and 111 DF,  p-value: 2.576e-05 \n\n\n\nCovariance matrix of residuals:\n                                     date daily_vaccinations_per_million\ndate                            3.450e+00                     -1.720e+04\ndaily_vaccinations_per_million -1.720e+04                      1.117e+09\nvalue                          -8.675e-04                      1.326e+02\n                                    value\ndate                           -8.675e-04\ndaily_vaccinations_per_million  1.326e+02\nvalue                           5.695e-04\n\nCorrelation matrix of residuals:\n                                   date daily_vaccinations_per_million    value\ndate                            1.00000                        -0.2771 -0.01957\ndaily_vaccinations_per_million -0.27709                         1.0000  0.16632\nvalue                          -0.01957                         0.1663  1.00000\n\n\n\n\nCode\nsummary(vars::VAR(demo_df.ts, p=6, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: date, daily_vaccinations_per_million, value \nDeterministic variables: both \nSample size: 111 \nLog Likelihood: -1206.123 \nRoots of the characteristic polynomial:\n0.9774 0.9451 0.9451 0.8051 0.8051 0.7966 0.7966 0.7777 0.748 0.748 0.7465 0.7465 0.7393 0.7393 0.5313 0.5313 0.497 0.497\nCall:\nvars::VAR(y = demo_df.ts, p = 6, type = \"both\")\n\n\nEstimation results for equation date: \n===================================== \ndate = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + date.l2 + daily_vaccinations_per_million.l2 + value.l2 + date.l3 + daily_vaccinations_per_million.l3 + value.l3 + date.l4 + daily_vaccinations_per_million.l4 + value.l4 + date.l5 + daily_vaccinations_per_million.l5 + value.l5 + date.l6 + daily_vaccinations_per_million.l6 + value.l6 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            7.744e-01  1.067e-01   7.257 1.28e-10 ***\ndaily_vaccinations_per_million.l1 -9.409e-06  6.125e-06  -1.536  0.12795    \nvalue.l1                          -3.182e+00  8.333e+00  -0.382  0.70344    \ndate.l2                            2.109e-01  1.352e-01   1.559  0.12243    \ndaily_vaccinations_per_million.l2  6.106e-06  7.888e-06   0.774  0.44089    \nvalue.l2                           1.004e+00  7.945e+00   0.126  0.89969    \ndate.l3                           -1.116e-02  1.339e-01  -0.083  0.93376    \ndaily_vaccinations_per_million.l3  1.974e-05  7.768e-06   2.542  0.01272 *  \nvalue.l3                          -2.905e+00  7.871e+00  -0.369  0.71296    \ndate.l4                           -7.672e-02  1.313e-01  -0.584  0.56049    \ndaily_vaccinations_per_million.l4 -2.096e-05  7.377e-06  -2.841  0.00554 ** \nvalue.l4                           1.098e+01  7.882e+00   1.393  0.16699    \ndate.l5                           -9.411e-02  1.350e-01  -0.697  0.48759    \ndaily_vaccinations_per_million.l5  1.267e-06  7.433e-06   0.170  0.86501    \nvalue.l5                          -3.205e+00  8.133e+00  -0.394  0.69447    \ndate.l6                            8.972e-02  1.083e-01   0.829  0.40936    \ndaily_vaccinations_per_million.l6  7.508e-06  5.513e-06   1.362  0.17655    \nvalue.l6                           5.956e+00  8.429e+00   0.707  0.48166    \nconst                              1.989e+03  9.640e+02   2.063  0.04193 *  \ntrend                              8.111e-01  3.839e-01   2.113  0.03736 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.706 on 91 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared: 0.9999 \nF-statistic: 1.135e+05 on 19 and 91 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation daily_vaccinations_per_million: \n=============================================================== \ndaily_vaccinations_per_million = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + date.l2 + daily_vaccinations_per_million.l2 + value.l2 + date.l3 + daily_vaccinations_per_million.l3 + value.l3 + date.l4 + daily_vaccinations_per_million.l4 + value.l4 + date.l5 + daily_vaccinations_per_million.l5 + value.l5 + date.l6 + daily_vaccinations_per_million.l6 + value.l6 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            6.278e+02  1.822e+03   0.345  0.73123    \ndaily_vaccinations_per_million.l1  8.405e-01  1.046e-01   8.037 3.21e-12 ***\nvalue.l1                           4.909e+04  1.423e+05   0.345  0.73087    \ndate.l2                            1.029e+03  2.309e+03   0.446  0.65682    \ndaily_vaccinations_per_million.l2  1.742e-01  1.347e-01   1.293  0.19919    \nvalue.l2                           7.426e+04  1.357e+05   0.547  0.58546    \ndate.l3                           -1.618e+03  2.286e+03  -0.708  0.48096    \ndaily_vaccinations_per_million.l3 -1.780e-01  1.326e-01  -1.342  0.18292    \nvalue.l3                           1.953e+05  1.344e+05   1.453  0.14972    \ndate.l4                            3.658e+02  2.242e+03   0.163  0.87077    \ndaily_vaccinations_per_million.l4  1.818e-01  1.260e-01   1.443  0.15236    \nvalue.l4                          -1.597e+05  1.346e+05  -1.187  0.23843    \ndate.l5                            1.114e+03  2.306e+03   0.483  0.63005    \ndaily_vaccinations_per_million.l5  4.197e-02  1.269e-01   0.331  0.74161    \nvalue.l5                           2.157e+05  1.389e+05   1.554  0.12377    \ndate.l6                           -1.793e+03  1.848e+03  -0.970  0.33467    \ndaily_vaccinations_per_million.l6 -2.543e-01  9.413e-02  -2.701  0.00824 ** \nvalue.l6                          -4.281e+04  1.439e+05  -0.297  0.76681    \nconst                              4.815e+06  1.646e+07   0.293  0.77056    \ntrend                              1.463e+03  6.555e+03   0.223  0.82389    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 29140 on 91 degrees of freedom\nMultiple R-Squared: 0.9579, Adjusted R-squared: 0.9491 \nF-statistic:   109 on 19 and 91 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation value: \n====================================== \nvalue = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + date.l2 + daily_vaccinations_per_million.l2 + value.l2 + date.l3 + daily_vaccinations_per_million.l3 + value.l3 + date.l4 + daily_vaccinations_per_million.l4 + value.l4 + date.l5 + daily_vaccinations_per_million.l5 + value.l5 + date.l6 + daily_vaccinations_per_million.l6 + value.l6 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)  \ndate.l1                           -9.094e-04  1.304e-03  -0.697   0.4874  \ndaily_vaccinations_per_million.l1 -2.550e-08  7.485e-08  -0.341   0.7341  \nvalue.l1                           1.003e-01  1.018e-01   0.985   0.3272  \ndate.l2                            1.984e-03  1.653e-03   1.201   0.2330  \ndaily_vaccinations_per_million.l2 -5.861e-08  9.640e-08  -0.608   0.5447  \nvalue.l2                           7.876e-02  9.710e-02   0.811   0.4194  \ndate.l3                            2.452e-04  1.636e-03   0.150   0.8812  \ndaily_vaccinations_per_million.l3  1.404e-07  9.494e-08   1.478   0.1428  \nvalue.l3                           2.450e-01  9.620e-02   2.547   0.0125 *\ndate.l4                           -1.086e-05  1.605e-03  -0.007   0.9946  \ndaily_vaccinations_per_million.l4 -1.089e-08  9.016e-08  -0.121   0.9041  \nvalue.l4                           2.046e-01  9.632e-02   2.124   0.0364 *\ndate.l5                           -2.960e-03  1.650e-03  -1.794   0.0762 .\ndaily_vaccinations_per_million.l5 -2.073e-07  9.083e-08  -2.282   0.0248 *\nvalue.l5                           2.529e-01  9.939e-02   2.544   0.0126 *\ndate.l6                            2.679e-03  1.323e-03   2.025   0.0458 *\ndaily_vaccinations_per_million.l6  1.123e-07  6.737e-08   1.667   0.0990 .\nvalue.l6                          -9.519e-02  1.030e-01  -0.924   0.3579  \nconst                             -1.890e+01  1.178e+01  -1.604   0.1121  \ntrend                             -7.687e-03  4.691e-03  -1.639   0.1048  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.02085 on 91 degrees of freedom\nMultiple R-Squared: 0.4856, Adjusted R-squared: 0.3782 \nF-statistic: 4.521 on 19 and 91 DF,  p-value: 4.642e-07 \n\n\n\nCovariance matrix of residuals:\n                                     date daily_vaccinations_per_million\ndate                            2.912e+00                     -1.187e+04\ndaily_vaccinations_per_million -1.187e+04                      8.490e+08\nvalue                          -2.724e-03                      6.711e+01\n                                    value\ndate                           -0.0027238\ndaily_vaccinations_per_million 67.1054112\nvalue                           0.0004349\n\nCorrelation matrix of residuals:\n                                   date daily_vaccinations_per_million    value\ndate                            1.00000                        -0.2388 -0.07654\ndaily_vaccinations_per_million -0.23875                         1.0000  0.11043\nvalue                          -0.07654                         0.1104  1.00000\n\n\nFrom the perspective of the p-values associated with these models, each demonstrates a moderate number of significant variables. Although a model with ( p = 1 ) lag might appear overly simplistic and possibly inadequate for capturing the full dynamics of the dataset, a model with ( p = 6 ) lags could potentially offer a slight improvement. This enhancement in performance suggests that incorporating more lag terms allows the VAR model to better account for the temporal dependencies among the variables, thus providing a more accurate and insightful analysis.\n\n\n\n\nCode\nVARselect(rep_df.ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     1      1      1      1 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 1.418188e+01 1.424910e+01 1.420589e+01 1.423788e+01 1.420512e+01\nHQ(n)  1.433378e+01 1.449213e+01 1.454006e+01 1.466319e+01 1.472157e+01\nSC(n)  1.455658e+01 1.484861e+01 1.503022e+01 1.528702e+01 1.547909e+01\nFPE(n) 1.442787e+06 1.544077e+06 1.480779e+06 1.532486e+06 1.488478e+06\n                  6            7            8            9           10\nAIC(n) 1.423938e+01 1.427922e+01 1.435414e+01 1.446980e+01 1.441354e+01\nHQ(n)  1.484697e+01 1.497794e+01 1.514401e+01 1.535080e+01 1.538567e+01\nSC(n)  1.573816e+01 1.600282e+01 1.630256e+01 1.664303e+01 1.681159e+01\nFPE(n) 1.548383e+06 1.622777e+06 1.765421e+06 2.005630e+06 1.924354e+06\n\n\nAccording to the selection criterion, we select the model with p=1 and 5(second lowest AIC).\n\n\nCode\nsummary(vars::VAR(rep_df.ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: date, daily_vaccinations_per_million, value \nDeterministic variables: both \nSample size: 116 \nLog Likelihood: -1315.974 \nRoots of the characteristic polynomial:\n0.9166 0.8849 0.08589\nCall:\nvars::VAR(y = rep_df.ts, p = 1, type = \"both\")\n\n\nEstimation results for equation date: \n===================================== \ndate = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            8.730e-01  5.020e-02  17.389   &lt;2e-16 ***\ndaily_vaccinations_per_million.l1 -3.159e-07  1.863e-06  -0.170   0.8656    \nvalue.l1                           1.101e+01  7.954e+00   1.385   0.1689    \nconst                              2.369e+03  9.338e+02   2.537   0.0126 *  \ntrend                              9.399e-01  3.699e-01   2.541   0.0124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.845 on 111 degrees of freedom\nMultiple R-Squared: 0.9999, Adjusted R-squared: 0.9999 \nF-statistic: 5.251e+05 on 4 and 111 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation daily_vaccinations_per_million: \n=============================================================== \ndaily_vaccinations_per_million = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            1.362e+03  9.238e+02   1.475    0.143    \ndaily_vaccinations_per_million.l1  9.047e-01  3.428e-02  26.393   &lt;2e-16 ***\nvalue.l1                          -3.402e+05  1.464e+05  -2.324    0.022 *  \nconst                             -2.529e+07  1.718e+07  -1.472    0.144    \ntrend                             -1.038e+04  6.807e+03  -1.524    0.130    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 33940 on 111 degrees of freedom\nMultiple R-Squared: 0.9323, Adjusted R-squared: 0.9299 \nF-statistic: 382.4 on 4 and 111 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation value: \n====================================== \nvalue = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            3.171e-03  5.818e-04   5.450 3.08e-07 ***\ndaily_vaccinations_per_million.l1  4.565e-08  2.159e-08   2.115   0.0367 *  \nvalue.l1                           1.097e-01  9.218e-02   1.190   0.2367    \nconst                             -5.893e+01  1.082e+01  -5.446 3.13e-07 ***\ntrend                             -2.316e-02  4.287e-03  -5.402 3.80e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.02138 on 111 degrees of freedom\nMultiple R-Squared: 0.3924, Adjusted R-squared: 0.3705 \nF-statistic: 17.92 on 4 and 111 DF,  p-value: 2.238e-11 \n\n\n\nCovariance matrix of residuals:\n                                     date daily_vaccinations_per_million\ndate                            3.402e+00                     -1.638e+04\ndaily_vaccinations_per_million -1.638e+04                      1.152e+09\nvalue                           3.770e-03                     -8.916e+01\n                                    value\ndate                            3.770e-03\ndaily_vaccinations_per_million -8.916e+01\nvalue                           4.569e-04\n\nCorrelation matrix of residuals:\n                                   date daily_vaccinations_per_million    value\ndate                            1.00000                        -0.2617  0.09562\ndaily_vaccinations_per_million -0.26165                         1.0000 -0.12289\nvalue                           0.09562                        -0.1229  1.00000\n\n\n\n\nCode\nsummary(vars::VAR(rep_df.ts, p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: date, daily_vaccinations_per_million, value \nDeterministic variables: both \nSample size: 112 \nLog Likelihood: -1219.025 \nRoots of the characteristic polynomial:\n0.9039 0.8932 0.8932 0.7899 0.7899 0.775 0.7678 0.7678 0.6684 0.6684 0.5979 0.5796 0.5796 0.4668 0.4668\nCall:\nvars::VAR(y = rep_df.ts, p = 5, type = \"both\")\n\n\nEstimation results for equation date: \n===================================== \ndate = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + date.l2 + daily_vaccinations_per_million.l2 + value.l2 + date.l3 + daily_vaccinations_per_million.l3 + value.l3 + date.l4 + daily_vaccinations_per_million.l4 + value.l4 + date.l5 + daily_vaccinations_per_million.l5 + value.l5 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            8.097e-01  1.058e-01   7.653 1.61e-11 ***\ndaily_vaccinations_per_million.l1 -6.276e-06  5.840e-06  -1.075   0.2853    \nvalue.l1                           6.927e+00  8.050e+00   0.860   0.3917    \ndate.l2                            2.009e-01  1.326e-01   1.515   0.1330    \ndaily_vaccinations_per_million.l2  8.339e-06  7.906e-06   1.055   0.2942    \nvalue.l2                           8.777e+00  8.204e+00   1.070   0.2874    \ndate.l3                           -9.356e-02  1.342e-01  -0.697   0.4873    \ndaily_vaccinations_per_million.l3  1.013e-05  7.516e-06   1.347   0.1810    \nvalue.l3                           2.046e+00  8.248e+00   0.248   0.8046    \ndate.l4                           -3.519e-02  1.313e-01  -0.268   0.7892    \ndaily_vaccinations_per_million.l4 -1.772e-05  7.104e-06  -2.495   0.0143 *  \nvalue.l4                          -1.658e+01  8.278e+00  -2.003   0.0480 *  \ndate.l5                            8.711e-03  1.042e-01   0.084   0.9335    \ndaily_vaccinations_per_million.l5  9.113e-06  5.232e-06   1.742   0.0848 .  \nvalue.l5                           3.697e+00  8.121e+00   0.455   0.6499    \nconst                              2.041e+03  1.326e+03   1.540   0.1270    \ntrend                              8.247e-01  5.242e-01   1.573   0.1190    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.67 on 95 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.445e+05 on 16 and 95 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation daily_vaccinations_per_million: \n=============================================================== \ndaily_vaccinations_per_million = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + date.l2 + daily_vaccinations_per_million.l2 + value.l2 + date.l3 + daily_vaccinations_per_million.l3 + value.l3 + date.l4 + daily_vaccinations_per_million.l4 + value.l4 + date.l5 + daily_vaccinations_per_million.l5 + value.l5 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \ndate.l1                            1.871e+02  1.872e+03   0.100   0.9206    \ndaily_vaccinations_per_million.l1  9.325e-01  1.033e-01   9.025 2.01e-14 ***\nvalue.l1                          -3.737e+05  1.424e+05  -2.623   0.0101 *  \ndate.l2                            3.565e+03  2.346e+03   1.520   0.1319    \ndaily_vaccinations_per_million.l2  2.095e-01  1.399e-01   1.498   0.1376    \nvalue.l2                           2.966e+05  1.451e+05   2.043   0.0438 *  \ndate.l3                           -2.426e+03  2.374e+03  -1.022   0.3095    \ndaily_vaccinations_per_million.l3 -1.810e-01  1.330e-01  -1.361   0.1768    \nvalue.l3                          -3.014e+04  1.459e+05  -0.207   0.8368    \ndate.l4                           -2.803e+03  2.322e+03  -1.207   0.2304    \ndaily_vaccinations_per_million.l4  1.316e-01  1.257e-01   1.047   0.2977    \nvalue.l4                           3.679e+04  1.465e+05   0.251   0.8022    \ndate.l5                            2.090e+03  1.843e+03   1.134   0.2597    \ndaily_vaccinations_per_million.l5 -2.150e-01  9.256e-02  -2.323   0.0223 *  \nvalue.l5                          -1.888e+04  1.437e+05  -0.131   0.8957    \nconst                             -1.137e+07  2.345e+07  -0.485   0.6289    \ntrend                             -4.856e+03  9.274e+03  -0.524   0.6018    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 29550 on 95 degrees of freedom\nMultiple R-Squared: 0.955,  Adjusted R-squared: 0.9474 \nF-statistic:   126 on 16 and 95 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation value: \n====================================== \nvalue = date.l1 + daily_vaccinations_per_million.l1 + value.l1 + date.l2 + daily_vaccinations_per_million.l2 + value.l2 + date.l3 + daily_vaccinations_per_million.l3 + value.l3 + date.l4 + daily_vaccinations_per_million.l4 + value.l4 + date.l5 + daily_vaccinations_per_million.l5 + value.l5 + const + trend \n\n                                    Estimate Std. Error t value Pr(&gt;|t|)   \ndate.l1                            4.080e-03  1.295e-03   3.151  0.00218 **\ndaily_vaccinations_per_million.l1  1.271e-07  7.148e-08   1.778  0.07858 . \nvalue.l1                           1.007e-01  9.853e-02   1.022  0.30922   \ndate.l2                            8.453e-04  1.623e-03   0.521  0.60364   \ndaily_vaccinations_per_million.l2 -1.079e-07  9.676e-08  -1.115  0.26778   \nvalue.l2                          -1.087e-05  1.004e-01   0.000  0.99991   \ndate.l3                           -3.989e-03  1.642e-03  -2.429  0.01700 * \ndaily_vaccinations_per_million.l3  4.500e-09  9.199e-08   0.049  0.96109   \nvalue.l3                          -1.007e-01  1.009e-01  -0.998  0.32103   \ndate.l4                            2.680e-03  1.607e-03   1.668  0.09858 . \ndaily_vaccinations_per_million.l4 -3.637e-08  8.695e-08  -0.418  0.67668   \nvalue.l4                           1.543e-01  1.013e-01   1.523  0.13113   \ndate.l5                           -1.523e-03  1.275e-03  -1.194  0.23531   \ndaily_vaccinations_per_million.l5  9.103e-08  6.403e-08   1.422  0.15841   \nvalue.l5                           1.812e-01  9.939e-02   1.823  0.07141 . \nconst                             -3.895e+01  1.622e+01  -2.401  0.01832 * \ntrend                             -1.513e-02  6.415e-03  -2.359  0.02038 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.02044 on 95 degrees of freedom\nMultiple R-Squared: 0.5238, Adjusted R-squared: 0.4436 \nF-statistic:  6.53 on 16 and 95 DF,  p-value: 1.168e-09 \n\n\n\nCovariance matrix of residuals:\n                                     date daily_vaccinations_per_million\ndate                            2.790e+00                     -1.391e+04\ndaily_vaccinations_per_million -1.391e+04                      8.734e+08\nvalue                          -8.582e-05                     -2.959e+01\n                                    value\ndate                           -8.582e-05\ndaily_vaccinations_per_million -2.959e+01\nvalue                           4.180e-04\n\nCorrelation matrix of residuals:\n                                    date daily_vaccinations_per_million\ndate                            1.000000                       -0.28181\ndaily_vaccinations_per_million -0.281806                        1.00000\nvalue                          -0.002513                       -0.04898\n                                   value\ndate                           -0.002513\ndaily_vaccinations_per_million -0.048975\nvalue                           1.000000\n\n\nExamining the VAR models with ( p = 1 ) and ( p = 5 ) lag orders, we notice the presence of some variables that do not significantly contribute to the model. Notably, the model with ( p = 5 ) lag periods tends to have fewer significant variables compared to the ( p = 1 ) model. However, opting for ( p = 1 ) may overly simplify the model, potentially failing to capture all the relevant and significant relationships among the variables. Thus, while ( p = 1 ) offers a more parsimonious model, it might not sufficiently account for the complexity of the dynamics within the data, suggesting that a balance must be struck between model simplicity and its ability to elucidate significant interactions."
  },
  {
    "objectID": "MT.html#cross-validation-1",
    "href": "MT.html#cross-validation-1",
    "title": "Multivariate TS Models",
    "section": "4.3 Cross Validation",
    "text": "4.3 Cross Validation\nTo validate our hypothesis and ensure the robustness of our model selection, we employ cross-validation to compare the Root Mean Squared Error (RMSE) of the two chosen models. By assessing the RMSE, we can quantitatively determine which model provides a more accurate fit to the data.\n\n(VAR)Daily Vaccination Number ~ Independent Party Support Rate(VAR)Daily Vaccination Number ~ Democratic Party Support Rate(VAR)Daily Vaccination Number ~ Republican Party Support Rate\n\n\n\n\nCode\n#n=length(inde_df.ts) 351\n#n-k=91; 260/52=5;\nn=length(inde_df.ts)\nk=91\n\nrmse1 &lt;- matrix(NA, 52,5)\nrmse2 &lt;- matrix(NA, 52,5)\nweek&lt;-c()\n\n# Convert data frame to time series object\nst &lt;- tsp(inde_df.ts )[1]+(k-1)/52\n\n\nfor(i in 1:1)\n{\n  \n  xtrain &lt;- window(inde_df.ts, end=st + i-1)\n  xtest &lt;- window(inde_df.ts, start=st + (i-1) + 1/52, end=st + i)\n  \n  ######## first Model ############\n  fit &lt;- VAR(inde_df.ts, p=3, type='both')\n  fcast &lt;- predict(fit, n.ahead = 52)\n  \n  fvacc&lt;-fcast$fcst$daily_vaccinations_per_million\n  fsupp&lt;-fcast$fcst$value\n\n  ff&lt;-data.frame(fvacc[,1],fsupp[,1]) #collecting the forecasts for 3 variables\n  \n  week&lt;-st + (i-1) + 1/52 \n  \n  ff&lt;-ts(ff,start=c(week,1),frequency = 52)\n  \n  a = 52*i-51 \n  b = 52*i \n  \n  ##### collecting errors ######\n  rmse1[c(a:b),]  &lt;-sqrt((ff-xtest[,2:3])^2)\n  \n  ######## Second Model ############\n  fit2 &lt;- vars::VAR(inde_df.ts, p=1, type='both')\n  fcast2 &lt;- predict(fit2, n.ahead = 52)\n  \n  fvacc&lt;-fcast2$fcst$daily_vaccinations_per_million\n  fsupp&lt;-fcast2$fcst$value\n\n  ff2&lt;-data.frame(fvacc[,1],fsupp[,1])\n  \n  week&lt;-st + (i-1) + 1/52\n  \n  ff2&lt;-ts(ff2,start=c(week,1),frequency = 52)\n  \n  a = 52*i-51 \n  b = 52*i\n  rmse2[c(a:b),]  &lt;-sqrt((ff2-xtest[,2:3])^2)\n}\n\n\n\n\nCode\nplot(1:52, rowMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"Week\", ylab=\"RMSE\")\nlines(1:52, rowMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"VAR 3\",\"VAR 1\"),col=2:4,lty=1)\n\n\n\n\n\nVAR(1) appears to have lower RMSE than VAR(3), therefore VAR(1) is a better fit.\n\n\n\n\nCode\n#n=length(inde_df.ts) 351\n#n-k=91; 260/52=5;\nn=length(demo_df.ts)\nk=91\n\nrmse1 &lt;- matrix(NA, 52,5)\nrmse2 &lt;- matrix(NA, 52,5)\nweek&lt;-c()\n\n# Convert data frame to time series object\nst &lt;- tsp(demo_df.ts )[1]+(k-1)/52\n\n\nfor(i in 1:1)\n{\n  \n  xtrain &lt;- window(demo_df.ts, end=st + i-1)\n  xtest &lt;- window(demo_df.ts, start=st + (i-1) + 1/52, end=st + i)\n  \n  ######## first Model ############\n  fit &lt;- VAR(demo_df.ts, p=6, type='both')\n  fcast &lt;- predict(fit, n.ahead = 52)\n  \n  fvacc&lt;-fcast$fcst$daily_vaccinations_per_million\n  fsupp&lt;-fcast$fcst$value\n\n  ff&lt;-data.frame(fvacc[,1],fsupp[,1]) #collecting the forecasts for 3 variables\n  \n  week&lt;-st + (i-1) + 1/52 \n  \n  ff&lt;-ts(ff,start=c(week,1),frequency = 52)\n  \n  a = 52*i-51 \n  b = 52*i \n  \n  ##### collecting errors ######\n  rmse1[c(a:b),]  &lt;-sqrt((ff-xtest[,2:3])^2)\n  \n  ######## Second Model ############\n  fit2 &lt;- vars::VAR(demo_df.ts, p=1, type='both')\n  fcast2 &lt;- predict(fit2, n.ahead = 52)\n  \n  fvacc&lt;-fcast2$fcst$daily_vaccinations_per_million\n  fsupp&lt;-fcast2$fcst$value\n\n  ff2&lt;-data.frame(fvacc[,1],fsupp[,1])\n  \n  week&lt;-st + (i-1) + 1/52\n  \n  ff2&lt;-ts(ff2,start=c(week,1),frequency = 52)\n  \n  a = 52*i-51 \n  b = 52*i\n  rmse2[c(a:b),]  &lt;-sqrt((ff2-xtest[,2:3])^2)\n}\n\n\n\n\nCode\nplot(1:52, rowMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"Week\", ylab=\"RMSE\")\nlines(1:52, rowMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"VAR 6\",\"VAR 1\"),col=2:4,lty=1)\n\n\n\n\n\nVAR(6) appears to have lower RMSE than VAR(1), therefore VAR(6) is a better fit.\n\n\n\n\nCode\n#n=length(inde_df.ts) 351\n#n-k=91; 260/52=5;\nn=length(rep_df.ts)\nk=91\n\nrmse1 &lt;- matrix(NA, 52,5)\nrmse2 &lt;- matrix(NA, 52,5)\nweek&lt;-c()\n\n# Convert data frame to time series object\nst &lt;- tsp(rep_df.ts )[1]+(k-1)/52\n\n\nfor(i in 1:1)\n{\n  \n  xtrain &lt;- window(rep_df.ts, end=st + i-1)\n  xtest &lt;- window(rep_df.ts, start=st + (i-1) + 1/52, end=st + i)\n  \n  ######## first Model ############\n  fit &lt;- VAR(rep_df.ts, p=5, type='both')\n  fcast &lt;- predict(fit, n.ahead = 52)\n  \n  fvacc&lt;-fcast$fcst$daily_vaccinations_per_million\n  fsupp&lt;-fcast$fcst$value\n\n  ff&lt;-data.frame(fvacc[,1],fsupp[,1]) #collecting the forecasts for 3 variables\n  \n  week&lt;-st + (i-1) + 1/52 \n  \n  ff&lt;-ts(ff,start=c(week,1),frequency = 52)\n  \n  a = 52*i-51 \n  b = 52*i \n  \n  ##### collecting errors ######\n  rmse1[c(a:b),]  &lt;-sqrt((ff-xtest[,2:3])^2)\n  \n  ######## Second Model ############\n  fit2 &lt;- vars::VAR(rep_df.ts, p=1, type='both')\n  fcast2 &lt;- predict(fit2, n.ahead = 52)\n  \n  fvacc&lt;-fcast2$fcst$daily_vaccinations_per_million\n  fsupp&lt;-fcast2$fcst$value\n\n  ff2&lt;-data.frame(fvacc[,1],fsupp[,1])\n  \n  week&lt;-st + (i-1) + 1/52\n  \n  ff2&lt;-ts(ff2,start=c(week,1),frequency = 52)\n  \n  a = 52*i-51 \n  b = 52*i\n  rmse2[c(a:b),]  &lt;-sqrt((ff2-xtest[,2:3])^2)\n}\n\n\n\n\nCode\nplot(1:52, rowMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"Week\", ylab=\"RMSE\")\nlines(1:52, rowMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"VAR 5\",\"VAR 1\"),col=2:4,lty=1)\n\n\n\n\n\nVAR(5) appears to have lower RMSE than VAR(1), therefore VAR(5) is a better fit."
  },
  {
    "objectID": "MT.html#forecast-1",
    "href": "MT.html#forecast-1",
    "title": "Multivariate TS Models",
    "section": "4.4 Forecast",
    "text": "4.4 Forecast\nIn this section, we used the model we selected from the results of cross validation to forecast the future trend.\n\n(VAR)Daily Vaccination Number ~ Independent Party Support Rate(VAR)Daily Vaccination Number ~ Democratic Party Support Rate(VAR)Daily Vaccination Number ~ Republican Party Support Rate\n\n\nThe best model for this relationship is VAR(1).\n\n\nCode\nvar1 &lt;- VAR(inde_df.ts, p=1, type=\"const\")\nforecast(var1) %&gt;%\n  autoplot() + xlab(\"Week\")+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\nThe forecast results indicate a projected decline in daily vaccinations per million in the upcoming period, which aligns with our current understanding of the trends. Additionally, the support rate for the independent party is expected to decrease overall. These trends suggest significant shifts in public health dynamics and political landscapes, which warrant close monitoring and analysis to better understand the underlying factors driving these changes.\n\n\nThe best model for this relationship is VAR(6).\n\n\nCode\nvar1 &lt;- VAR(demo_df.ts, p=6, type=\"const\")\nforecast(var1) %&gt;%\n  autoplot() + xlab(\"Week\")+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\nThe forecast results indicate a projected decline in daily vaccinations per million in the upcoming period, which aligns with our current understanding of the trends. Additionally, the support rate for the democratic party is expected to be stable with some fluctuations overall. These trends suggest significant shifts in public health dynamics and political landscapes, which warrant close monitoring and analysis to better understand the underlying factors driving these changes.\n\n\nThe best model for this relationship is VAR(5).\n\n\nCode\nvar1 &lt;- VAR(rep_df.ts, p=5, type=\"const\")\nforecast(var1) %&gt;%\n  autoplot() + xlab(\"Week\")+ theme_bw()+\n      theme(plot.background = element_rect(fill = \"#D9E3F1\", color = NA),\n            panel.background = element_rect(fill = \"#D9E3F1\", color = NA))\n\n\n\n\n\nThe forecast results indicate a projected decline in daily vaccinations per million in the upcoming period, which aligns with our current understanding of the trends. Additionally, the support rate for the republican party is expected to be stable with some fluctuations overall. These trends suggest significant shifts in public health dynamics and political landscapes, which warrant close monitoring and analysis to better understand the underlying factors driving these changes."
  },
  {
    "objectID": "other.html#gdp-per-capita",
    "href": "other.html#gdp-per-capita",
    "title": "Other: Interrupted TS/ARFIMA/Spectral Analysis",
    "section": "",
    "text": "Code\ngdp &lt;- read_csv('Datasets/gdp.csv')\n\n# Convert DATE column from m/d/yy format to Date object and reformat to \"Year\" only for simplicity\ngdp$DATE &lt;- format(mdy(gdp$DATE), \"%Y/%m/%d\")\n\n# Convert GDP column to numeric (floating-point) format if not already\ngdp$GDP &lt;- as.numeric(gdp$GDP)\n\n# Visualize the plot\nstart_year &lt;- year(min(gdp$DATE))\nstart_month &lt;- month(min(gdp$DATE))\nend_year &lt;- year(max(gdp$DATE))\nend_month &lt;- month(max(gdp$DATE))\n\n# Calculate the number of observations from start to end\nnum_obs &lt;- (end_year - start_year) * 4 + ceiling((end_month - start_month) / 3)\n\n\n# data glimpse\nhead(gdp, 3)\n\n\n# A tibble: 3 × 2\n  DATE          GDP\n  &lt;chr&gt;       &lt;dbl&gt;\n1 2017/01/01 19280.\n2 2017/04/01 19439.\n3 2017/07/01 19693.\n\n\nCode\nfig &lt;- plot_ly(gdp, x = ~DATE, y = ~GDP,name = 'GDP Per Capita', type = 'scatter', mode = 'lines')\n\nfig\n\n\n\n\n\n\nCode\n# Create the time series object\n#gdp_ts &lt;- ts(gdp$GDP, start = c(start_year, start_month), frequency = 4, deltat = 0.25)\n\ndataTS &lt;-data.frame(\"Y\"=gdp$GDP)\ndataTS$Ts&lt;-seq(1:length(gdp$GDP))\ndataTS$D &lt;- ifelse(gdp$DATE &lt; as.Date(\"2020-02-20\"), 0, 1)\ndataTS$P&lt;- seq_along(gdp$DATE)\ndataTS$P[gdp$DATE &lt; as.Date(\"2020-02-20\")] &lt;- 0\n\nhead(dataTS)\n\n\n         Y Ts D P\n1 19280.08  1 0 0\n2 19438.64  2 0 0\n3 19692.60  3 0 0\n4 20037.09  4 0 0\n5 20328.55  5 0 0\n6 20580.91  6 0 0\n\n\nCode\nindex_of_2020_02_20 &lt;- which(gdp$DATE == as.Date(\"2020-04-01\"))\n#index_of_2020_02_20 #14\n\nplot( dataTS$Ts, dataTS$Y,\n      bty=\"n\", pch=19, col=\"gray\",\n      xlab = \"Time\", \n      ylab = \"GDP Per Capita\" )\n\n# Line marking the interruption\nabline( v=13, col=\"firebrick\", lty=2 )\ntext( 9, 24000, \"Start of COVID\", col=\"firebrick\", cex=1.3, pos=4 )\n\n# Add the regression line\nts &lt;- lm( Y ~ Ts + D + P, data=dataTS )\nlines( dataTS$Ts, ts$fitted.values, col=\"steelblue\", lwd=2 )\n\n\n\n\n\n\nPre-COVID Trend: Prior to the dashed red line labeled “Start of COVID,” there is a positive trend in GDP per capita, indicating economic growth. This is represented by the blue line, which is the fitted line from a regression model, showing the upward trajectory of GDP per capita over time.\nCOVID-19 Impact: The dashed red line signifies the point at which the COVID-19 pandemic began. Following this line, there’s a noticeable dip in GDP per capita, reflecting the immediate economic impact of the pandemic. This downturn signifies a break from the previous growth trend.\nPost-COVID Recovery: After the initial dip, the blue fitted line indicates that GDP per capita began to recover, continuing on an upward trajectory, although starting from a lower point than where the pre-COVID trend would have predicted.\nRecovery Trend: The post-COVID section of the blue line is steeper than the pre-COVID section, suggesting that the rate of growth in GDP per capita after the initial pandemic shock may be faster than the growth rate before the pandemic.\n\n\n\nCode\nstargazer( ts, \n           type = \"text\", \n           dep.var.labels = (\"GDP Per Capita\"),\n           column.labels = (\"Model results\"),\n           covariate.labels = c(\"Time\", \"Treatment\", \"Time Since Treatment\"),\n           omit.stat = \"all\", \n           digits = 2 )\n\n\n\n================================================\n                         Dependent variable:    \n                     ---------------------------\n                           GDP Per Capita       \n                            Model results       \n------------------------------------------------\nTime                          224.26***         \n                               (23.05)          \n                                                \nTreatment                   -5,566.75***        \n                              (438.41)          \n                                                \nTime Since Treatment          306.49***         \n                               (29.61)          \n                                                \nConstant                    19,112.23***        \n                              (182.95)          \n                                                \n================================================\n================================================\nNote:                *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nThe interrupted regression result presented describes the impact of the onset of the COVID-19 pandemic on GDP per capita over time. Here’s a detailed breakdown of the model’s components and the statistical outputs:\n\nTime (Ts): The coefficient of 224.26 (with a standard error of 23.05) and significance indicated by three asterisks (***), suggesting a p-value less than 0.01, indicates that for every unit increase in time, the GDP per capita increases by approximately 224.26 units, assuming no treatment and not considering the passage of time since the treatment. This shows a general trend of GDP growth over time under normal circumstances.\nTreatment (D): The treatment effect is -5,566.75 (with a standard error of 438.41), also highly significant (***). This suggests that the introduction of the treatment (the onset of the pandemic) leads to an immediate drop in GDP per capita by approximately 5,566.75 units. This captures the initial shock of the event on the economy.\nTime Since Treatment (P): The coefficient of 306.49 (with a standard error of 29.61) and significance (***), indicates that for each unit of time after the treatment, GDP per capita increases by about 306.49 units. This represents a recovery trajectory where, after the initial shock, GDP per capita begins to recover at this rate.\n\nIn summary, the model indicates that while the treatment had a significant negative impact on GDP per capita, there has been a subsequent positive rate of recovery over time following the initial shock. This provides a quantified insight into the economic impact of the event and its aftermath."
  },
  {
    "objectID": "other.html#stock-price",
    "href": "other.html#stock-price",
    "title": "Other: Interrupted TS/ARFIMA/Spectral Analysis",
    "section": "",
    "text": "Code\n# Set options to suppress warnings\noptions(\"getSymbols.warning4.0\" = FALSE)\noptions(\"getSymbols.yahoo.warning\" = FALSE)\n\n# Define the tickers\ntickers &lt;- c(\"PFE\")\n\n# Loop through tickers to get stock data\nfor (ticker in tickers) {\n  getSymbols(ticker,\n             from = \"2016-01-01\",\n             to = \"2024-01-01\")\n}\n\n# Create a data frame with adjusted closing prices\nstock &lt;- data.frame(date = index(PFE), value = Ad(PFE))\n\n# data glimpse\nhead(stock, 3)\n\n\n                 date PFE.Adjusted\n2016-01-04 2016-01-04     22.07899\n2016-01-05 2016-01-05     22.23794\n2016-01-06 2016-01-06     21.84404\n\n\nCode\nfig &lt;- plot_ly(stock, x = ~date, y = ~PFE.Adjusted,name = 'Pfizer Stock Price', type = 'scatter', mode = 'lines')\n\nfig\n\n\n\n\n\n\nCode\ndataTS &lt;-data.frame(\"Y\"=stock$PFE.Adjusted)\ndataTS$Ts&lt;-seq(1:length(stock$PFE.Adjusted))\ndataTS$D &lt;- ifelse(stock$date &lt; as.Date(\"2020-02-20\"), 0, 1)\ndataTS$P&lt;- seq_along(stock$date)\ndataTS$P[stock$date &lt; as.Date(\"2020-02-20\")] &lt;- 0\n\nhead(dataTS)\n\n\n         Y Ts D P\n1 22.07899  1 0 0\n2 22.23794  2 0 0\n3 21.84404  3 0 0\n4 21.69892  4 0 0\n5 21.42250  5 0 0\n6 21.47087  6 0 0\n\n\nCode\nindex_of_2020_02_20 &lt;- which(stock$date == as.Date(\"2020-02-20\"))\n#index_of_2020_02_20 #1040\n\nplot( dataTS$Ts, dataTS$Y,\n      bty=\"n\", pch=19, col=\"gray\",\n      xlab = \"Time (days)\", \n      ylab = \"Pfizer Stock Price\" )\n\n# Line marking the interruption\nabline( v=1039, col=\"firebrick\", lty=2 )\ntext( 700, 40, \"Start of COVID\", col=\"firebrick\", cex=1.3, pos=4 )\n\n# Add the regression line\nts &lt;- lm( Y ~ Ts + D + P, data=dataTS )\nlines( dataTS$Ts, ts$fitted.values, col=\"steelblue\", lwd=2 )\n\n\n\n\n\n\nTrend Before COVID-19: Before the dashed red vertical line that denotes the start of COVID-19, there is a blue line indicating a general upward trend in Pfizer’s stock price. This suggests that Pfizer’s stock was gradually increasing in value over time before the pandemic began.\nStart of COVID-19: The dashed red vertical line indicates the point at which the COVID-19 pandemic started. This is a crucial time marker for the analysis.\nTrend After COVID-19: After the start of COVID-19, the plot shows significant volatility in Pfizer’s stock price. The stock price initially follows the pre-pandemic trend and continues to increase but then shows some fluctuation before sharply rising. This sharp rise could be due to various factors, potentially including positive news about vaccine development or other pharmaceutical advances related to COVID-19 made by Pfizer.\nOverall Pattern: Towards the right end of the plot, after a peak, there is a noticeable decline in the stock price, which may indicate a period of correction or response to other market or global factors.\n\n\n\nCode\nstargazer( ts, \n           type = \"text\", \n           dep.var.labels = (\"Pfizer Stock Price\"),\n           column.labels = (\"Model results\"),\n           covariate.labels = c(\"Time\", \"Treatment\", \"Time Since Treatment\"),\n           omit.stat = \"all\", \n           digits = 2 )\n\n\n\n================================================\n                         Dependent variable:    \n                     ---------------------------\n                         Pfizer Stock Price     \n                            Model results       \n------------------------------------------------\nTime                           0.01***          \n                               (0.001)          \n                                                \nTreatment                      2.29**           \n                               (0.92)           \n                                                \nTime Since Treatment          -0.002***         \n                               (0.001)          \n                                                \nConstant                      21.56***          \n                               (0.31)           \n                                                \n================================================\n================================================\nNote:                *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nThe regression results presented show impact of the onset of the COVID-19 pandemic on the stock price of Pfizer. The regression model includes variables for time, treatment, and the time since treatment was introduced. Here’s a breakdown of each component of the output:\n\nTime (Ts): The coefficient for time is 0.01 with a p-value of less than 0.01 (indicated by ***), suggesting that it is highly statistically significant. This implies that, on average, the stock price of Pfizer has been increasing by 0.01 units each time unit regardless of other factors in the model.\nTreatment (D): The treatment variable has a coefficient of 2.29 with a p-value of less than 0.05 (**), indicating a statistically significant positive effect on the Pfizer stock price. This suggests that the onset of the COVID-19 pandemic led to an average increase in Pfizer’s stock price by 2.29 units at the time of the treatment’s implementation.\nTime Since Treatment (P): The coefficient for time since treatment is -0.002 with a p-value of less than 0.01 (***), which is highly significant. This indicates that following the treatment, the stock price decreases by an average of 0.002 units per time unit. This could suggest that the initial positive impact of the treatment diminishes over time.\n\nThe model suggests that the Pfizer stock price has a general upward trend over time. There was a significant increase in the stock price at the time of treatment, which means the investment of COVID-19 vaccinates could be helpful for the stock price during some periods of COVID-19. However, the effect of COVID-19 diminishes slightly over time, indicating that while the intervention had a positive immediate impact, its effect lessens as time progresses."
  },
  {
    "objectID": "FTS.html#returns",
    "href": "FTS.html#returns",
    "title": "FTS Models",
    "section": "4.1 Returns",
    "text": "4.1 Returns\n\nPfizerBioNTech SE\n\n\n\n\nCode\nggAcf(returns_pfe,40) + theme_bw()\n\n\n\n\n\nCode\nggPacf(returns_pfe,40) + theme_bw()\n\n\n\n\n\nWe can see for both ACF anf PACF plots, the Pfizer data is very weakly stationary. The possible q and p values would both be q and p = 3,9,10.\n\n\n\n\nCode\nggAcf(returns_bntx,40) + theme_bw()\n\n\n\n\n\nCode\nggPacf(returns_bntx,40) + theme_bw()\n\n\n\n\n\nWe can see for both ACF anf PACF plots, the BioNTech SE data is very weakly stationary. The possible q and p values would both be q and p = 9,32. Most of these are again too high, so we would have to likely consider p and q 0,9."
  },
  {
    "objectID": "FTS.html#absolute-returns",
    "href": "FTS.html#absolute-returns",
    "title": "FTS Models",
    "section": "4.2 Absolute Returns",
    "text": "4.2 Absolute Returns\n\nPfizerBioNTech SE\n\n\n\n\nCode\nggAcf(abs(returns_pfe),40) + theme_bw()\n\n\n\n\n\nCode\nggPacf(abs(returns_pfe),40) + theme_bw()\n\n\n\n\n\nThe analysis of absolute returns for Pfizer stock reveals a stable pattern in the ACF plot, with the first 40 lags exhibiting significant autocorrelation. This suggests a strong and consistent relationship in the data’s movements over these intervals. Conversely, the PACF plot indicates a more contained range of significant lags. Here, only the first 8, along with the 11th and 15th lags, show significant partial autocorrelation.\n\n\n\n\nCode\nggAcf(abs(returns_bntx),40) + theme_bw()\n\n\n\n\n\nCode\nggPacf(abs(returns_bntx),40) + theme_bw()\n\n\n\n\n\nThe analysis of absolute returns for BioNTech SE stock reveals a stable pattern in the ACF plot, with the first 40 lags exhibiting significant autocorrelation. This suggests a strong and consistent relationship in the data’s movements over these intervals. Conversely, the PACF plot indicates a more contained range of significant lags. Here, only the first 3, along with the 5th, 8th and 9th lags, show significant partial autocorrelation."
  },
  {
    "objectID": "FTS.html#squared-returns",
    "href": "FTS.html#squared-returns",
    "title": "FTS Models",
    "section": "4.3 Squared Returns",
    "text": "4.3 Squared Returns\n\nPfizerBioNTech SE\n\n\n\n\nCode\nggAcf(returns_pfe^2,40) + theme_bw()\n\n\n\n\n\nCode\nggPacf(returns_pfe^2,40) + theme_bw()\n\n\n\n\n\nSquared returns are less stationary than absolute returns but more stationary than only returns. For Pfizer ACF plot, significant lags are 1 through 21 and PACF plot had significant lags are 1 to 5, 8, 11, and 14.\n\n\n\n\nCode\nggAcf(returns_bntx^2,40) + theme_bw()\n\n\n\n\n\nCode\nggPacf(returns_bntx^2,40) + theme_bw()\n\n\n\n\n\nSquared returns are less stationary than absolute returns but more stationary than only returns. For BioNTech SE ACF plot, significant lags are 1 through 7 and PACF plot had significant lags are 1 through 6."
  },
  {
    "objectID": "conclusions.html#looking-back-to-big-picture",
    "href": "conclusions.html#looking-back-to-big-picture",
    "title": "Conclusions",
    "section": "",
    "text": "In the pursuit of a comprehensive understanding, our research embarks on a meticulous examination of public data sources, seeking to address pivotal questions that encapsulate the intricate interplay between COVID-19 vaccination rates and multifaceted dimensions. Through the lens of data-driven analysis, our endeavor unfolds as a systematic exploration into the evolving dynamics of vaccination and its cascading impact across diverse realms.\nOne of the central inquiries guiding this investigation is the temporal evolution of COVID-19 vaccination rates and their influence on various sectors. Delving into the data, we seek to discern patterns and shifts in vaccination rates over time, unraveling the nuanced story of how the collective endeavor to immunize populations unfolds. We examine the effectiveness of the vaccine from a quantitative perspective, scrutinizing its role in mitigating the spread of the virus and potentially altering the trajectory of the pandemic.\nHospitalization rates stand as a critical metric in gauging the efficacy of vaccination efforts. We probe into whether the vaccine, by conferring immunity, contributes to alleviating the burden on healthcare systems. Through rigorous data analysis, we aim to illuminate the extent to which vaccination rates correlate with fluctuations in hospitalization numbers, providing valuable insights into the broader public health landscape.\nThe economic ramifications of COVID-19 vaccination constitute another facet of our exploration. By scrutinizing the data, we seek to ascertain whether higher vaccination rates correspond to economic recovery. Unpacking the intricate relationship between vaccination efforts and economic indicators, our analysis endeavors to shed light on the potential role of vaccination campaigns in fostering economic resilience.\nIn the financial realm, we examine the impact of vaccination on medical corporations, particularly in the context of stock prices. This inquiry navigates the nexus between vaccination investments and market dynamics, unraveling the intricate dance between public health imperatives and corporate performance.\nBeyond these dimensions, our research extends its gaze into the political arena, exploring the relationship between vaccination rates and party support. By dissecting the data, we aim to uncover whether vaccination rates influence political sentiments, offering a nuanced understanding of how public health measures intertwine with political dynamics."
  },
  {
    "objectID": "conclusions.html#conclusion",
    "href": "conclusions.html#conclusion",
    "title": "Conclusions",
    "section": "Conclusion",
    "text": "Conclusion\nThe analysis of various metrics through the ARIMA model yields several insights into post-pandemic trends. For vaccination rates, the model predicts stability, aligning with expectations given the transition into a post-pandemic era. Similarly, the numbers for confirmed cases and deaths are also expected to stabilize at lower levels than during the pandemic peak, suggesting a sustained reduction in COVID-19 impact.\nIn contrast, hospitalization figures might show a slight increase, but the general trend remains largely unchanged. Economic indicators such as the unemployment rate are projected to continue their decline from the highs of April 2020, reflecting recovery and growth. This improvement is echoed in the GDP per capita projections, which indicate a rebound from the pandemic-induced dip in 2019-2020.\nHowever, political support rates within the US, according to ARIMA, appear highly volatile and unpredictable, continuing to fluctuate around the mean. To delve deeper, ARIMAX and VAR models are employed to explore relationships between variables, particularly how vaccination impacts case numbers and unemployment. It’s clear that increased vaccination rates correlate with lower confirmed and death case numbers. Surprisingly, vaccination rates also seem to influence unemployment rates, underscoring a complex interplay between health interventions and economic factors.\nVAR models further examine the impact of vaccination on political party support rates, which, similar to ARIMA predictions, show persistent volatility. In financial markets, analysis of Pfizer and BioNTech SE stock prices illustrates an initial surge during the early stages of the pandemic, followed by stabilization or decline in the post-pandemic period.\nLastly, when comparing traditional models to Deep Learning (DL) methodologies, the latter shows superior performance due to its advanced, intricate algorithms, offering more nuanced analysis and predictions across the studied metrics. This highlights DL’s potential in providing more accurate forecasts and deeper insights into complex data relationships.\nThis comprehensive analysis not only charts the path of recovery and normalization but also equips policymakers and businesses with the foresight needed to navigate the post-pandemic world effectively."
  }
]